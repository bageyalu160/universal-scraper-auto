---
description: 
globs: 
alwaysApply: true
---
# Universal Scraper 开发规范与未来规划

## 一、项目概述

Universal Scraper 是一个功能强大、高度可配置的网页数据采集和 AI 分析框架，专为研究和数据分析项目设计。项目当前版本为 v1.1.0，主要提供以下核心能力：

- 高度可配置的爬虫框架，支持多种爬取引擎
- AI 驱动的数据分析能力
- 结构化输出支持
- 自动化工作流
- 多渠道通知
- 代理池和反爬虫机制

## 二、代码组织规范

### 1. 目录结构规范

```
universal-scraper/
├── config/                   # 配置文件目录
│   ├── sites/                # 站点配置文件
│   ├── analysis/             # AI分析配置
│   ├── workflow/             # 工作流模板
├── scripts/                  # 脚本目录
├── src/                      # 源代码目录
│   ├── scrapers/             # 爬虫实现
│   ├── analyzers/            # 分析器实现
│   ├── parsers/              # 解析器实现
│   ├── notifiers/            # 通知器实现
│   ├── storage/              # 存储实现
│   ├── utils/                # 工具函数
├── data/                     # 数据存储目录
├── analysis/                 # 分析结果目录
├── docs/                     # 文档目录
├── status/                   # 状态文件目录
└── .github/                  # GitHub相关文件
    ├── workflows/            # GitHub Actions工作流
```

### 2. 命名规范

- **文件命名**：使用小写字母，单词间用下划线连接，例如 `heimao_scraper.py`
- **类命名**：使用驼峰命名法，例如 `AIAnalyzer`
- **函数和变量命名**：使用小写字母，单词间用下划线连接，例如 `load_config`
- **常量命名**：使用大写字母，单词间用下划线连接，例如 `DEFAULT_TIMEOUT`
- **配置文件命名**：使用小写字母，与相应的网站 ID 一致，例如 `heimao.yaml`

### 3. 编码规范

- 遵循 PEP 8 编码规范
- 缩进使用 4 个空格
- 行长度不超过 100 字符
- 使用 UTF-8 编码
- 为所有公共 API 添加文档字符串

## 三、模块化与组件设计规范

### 1. 爬虫引擎设计

爬虫引擎应遵循以下接口规范：

```python
def scrape(config: dict, output_dir: str) -> dict:
    """
    执行爬虫任务
    
    Args:
        config: 配置字典
        output_dir: 输出目录路径
        
    Returns:
        包含状态和统计信息的字典，必须包含 "status" 和 "count" 字段
    """
    # 实现代码
    return {
        "status": "success",  # 或 "failure"
        "count": data_count,  # 获取的数据条数
        "message": "任务完成"  # 可选的消息
    }
```

### 2. 分析器设计

分析器应遵循统一的接口规范，支持多种 AI 提供商：

```python
class Analyzer:
    def __init__(self, config: dict):
        """初始化分析器"""
        pass
        
    def analyze(self, data: list) -> dict:
        """
        分析数据
        
        Args:
            data: 要分析的数据列表
            
        Returns:
            分析结果字典
        """
        pass
```

### 3. 配置设计

项目配置应遵循层次化设计：

- **全局设置**：`config/settings.yaml`
- **站点配置**：`config/sites/{site_id}.yaml`
- **分析提示词**：`config/analysis/prompts/{site_id}_prompt.txt`

所有配置应使用 YAML 格式，支持环境变量引用和模板替换。

## 四、技术选型与版本控制

### 1. 核心依赖

- Python >= 3.8
- 爬虫引擎：requests、Playwright、Firecrawl
- AI 分析：OpenAI、Google Gemini
- 数据处理：pandas、numpy
- 通知：自定义通知实现

### 2. 版本控制

- 使用语义化版本号：主版本.次版本.修订版本（例如 1.1.0）
- 主版本：不兼容的 API 变更
- 次版本：向后兼容的功能新增
- 修订版本：向后兼容的问题修复

### 3. 分支策略

- `main`：稳定版本，随时可部署
- `develop`：开发分支，用于合并功能分支
- `feature/{feature-name}`：功能分支
- `fix/{issue-id}`：修复分支
- `release/{version}`：发布分支

## 五、测试与质量控制

### 1. 测试策略

- **单元测试**：使用 pytest 进行关键组件的单元测试
- **集成测试**：确保组件之间的交互正常
- **端到端测试**：使用 Playwright 进行浏览器自动化测试

### 2. 代码质量控制

- 使用 pylint 进行静态代码分析
- 使用 black 进行代码格式化
- 引入代码审查机制，确保代码质量
- 建立 CI/CD 工作流，自动化测试和部署

## 六、文档规范

### 1. 代码文档

- 所有公共 API 必须有文档字符串
- 遵循 Google 风格的文档字符串格式
- 复杂逻辑应添加详细注释

### 2. 用户文档

- 为每个主要功能编写使用指南
- 提供详细的配置选项文档
- 包含示例和最佳实践

### 3. 开发者文档

- 系统架构文档
- 组件设计与交互文档
- 开发环境设置指南
- 贡献指南

## 七、错误处理与日志规范

### 1. 错误处理

- 使用异常处理捕获预期错误
- 对于公共 API，提供明确的错误信息
- 使用自定义异常类表示特定错误类型

### 2. 日志规范

- 使用 Python 标准库的 logging 模块
- 定义统一的日志格式和级别
- 关键操作必须记录日志
- 敏感信息不应在日志中明文显示

## 八、安全性考虑

### 1. 数据安全

- 敏感配置通过环境变量或加密存储
- API 密钥不应硬编码在源代码中
- 确保数据存储的安全性

### 2. 爬虫伦理

- 遵守网站的 robots.txt 规则
- 合理控制爬取频率，避免对目标站点造成负担
- 尊重网站的访问政策和条款

## 九、CI/CD 与自动化

### 1. GitHub Actions 工作流

- 代码检查与测试工作流
- 爬虫和分析任务自动化工作流
- 文档自动生成与部署工作流

### 2. 自动化脚本

- 工作流生成器
- 配置验证器
- 状态监控系统

## 十、未来规划

### 1. 近期目标（1-3个月）

1. **完善代码结构**：
   - 重构代码以遵循上述规范
   - 提高模块化程度，降低组件间耦合
   - 建立统一的错误处理机制

2. **增强监控系统**：
   - 完成基于 GitHub Pages 的监控仪表盘开发
   - 添加实时统计和可视化功能
   - 完善任务状态和错误报告

3. **扩展爬虫引擎**：
   - 增强 Firecrawl 集成能力
   - 添加更多反爬机制支持
   - 优化代理池管理系统

### 2. 中期目标（3-6个月）

1. **AI 分析增强**：
   - 支持更多 AI 模型和提供商
   - 添加分析结果验证机制
   - 开发领域特定分析模板

2. **数据管理系统**：
   - 设计和实现数据版本控制
   - 添加数据查询和检索接口
   - 开发数据质量评估工具

3. **Web 界面开发**：
   - 设计爬虫和分析任务的 Web 管理界面
   - 开发数据可视化仪表盘
   - 添加用户权限和多用户支持

### 3. 长期目标（6个月以上）

1. **分布式架构**：
   - 设计和实现分布式爬虫架构
   - 开发任务调度和资源分配系统
   - 支持大规模爬虫任务

2. **机器学习集成**：
   - 集成自动网页分类和提取模型
   - 开发异常检测和数据质量控制模型
   - 支持自动优化爬取策略

3. **多语言支持**：
   - 增强多语言网站爬取能力
   - 开发多语言内容分析功能
   - 支持跨语言数据比较和分析

4. **SDK 和 API 开发**：
   - 设计和实现 RESTful API
   - 开发多语言 SDK
   - 提供云服务集成能力

## 十一、性能优化规范

### 1. 爬虫性能

- 合理使用异步和并发
- 优化网络请求策略
- 实现智能重试和失败处理

### 2. 资源消耗控制

- 控制内存使用
- 优化 CPU 密集型操作
- 实现资源自适应调整

### 3. 数据处理性能

- 优化大数据集处理
- 实现增量处理机制
- 考虑数据流处理模式

## 十二、项目管理与协作

### 1. 任务管理

- 使用 GitHub Issues 进行任务追踪
- 实施敏捷开发方法
- 建立明确的优先级和里程碑

### 2. 协作规范

- 遵循 GitHub Flow 工作流
- 明确的 Pull Request 审查流程
- 定期代码审查和改进

### 3. 版本发布流程

- 明确的版本发布计划
- 完善的版本测试和验证
- 详细的更新日志维护
