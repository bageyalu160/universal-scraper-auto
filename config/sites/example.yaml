# 示例网站爬虫配置模板
# 用户可以基于此模板创建新的网站配置

# 基本站点信息
site:
  name: "ExampleSite" # 网站名称
  description: "网站描述" # 简短的网站描述
  base_url: "https://example.com/" # 网站基础URL，以/结尾
  output_filename: "example_data.tsv" # 输出文件名
  encoding: "utf-8" # 网站编码，常见值: utf-8, gbk, iso-8859-1等

# 爬取规则
scraping:
  # 爬取的目标列表 (可以是板块ID、分类URL等，根据网站结构定义)
  targets:
    - id: "category1" # 目标ID/名称
      name: "分类1" # 显示名称
      category: "主分类" # 所属大分类
      url: "https://example.com/category1" # 可选：特定URL
    - id: "category2"
      name: "分类2"
      category: "主分类"
    # 可以添加更多目标...

  # 时间范围设置
  time_range:
    days_limit: 7 # 抓取最近几天的内容
    start_date: null # 可选：指定开始日期，格式为YYYY-MM-DD
    end_date: null # 可选：指定结束日期，格式为YYYY-MM-DD

  # 分页设置
  pagination:
    pages_per_target: 5 # 每个目标抓取的页数
    items_per_page: 20 # 每页项目数（用于估算或验证）
    max_items: 100 # 可选：最大抓取项目数

  # URL格式
  url_format:
    # 使用Python字符串格式化语法，可用变量取决于特定爬虫实现
    target_page: "{base_url}{target_id}/page/{page_num}"
    item_detail: "{base_url}item/{item_id}"

# 网络请求配置
network:
  # 重试设置
  retry:
    max_retries: 3 # 最大重试次数
    backoff_factor: 0.5 # 重试退避因子
    status_forcelist: [408, 429, 500, 502, 503, 504] # 触发重试的HTTP状态码

  # 超时设置
  timeout: 30 # 请求超时时间（秒）

  # 延迟设置
  delay:
    page_delay:
      min: 2 # 页面间最小延迟（秒）
      max: 5 # 页面间最大延迟（秒）
    target_delay:
      min: 5 # 目标间最小延迟（秒）
      max: 10 # 目标间最大延迟（秒）

  # User-Agent配置
  user_agents:
    - "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    - "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15"

  # 代理设置（可选）
  proxy:
    enabled: false # 是否启用代理
    proxy_url: null # 代理服务器URL，格式：http://user:pass@host:port
    proxy_rotation: false # 是否轮换代理

  # 请求头设置（可选）
  headers:
    Accept: "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
    Accept-Language: "zh-CN,zh;q=0.9,en;q=0.8"
    # 可添加更多请求头...

# 解析配置
parsing:
  # 列表项选择器
  item_selector: "div.item" # CSS选择器，用于定位列表中的每个项目

  # 字段选择器
  field_selectors:
    title: # 字段名
      selector: "h2.title a" # CSS选择器
      attribute: "text" # 要获取的属性：text, href, src, html等
      regex: null # 可选：应用于属性值的正则表达式
      default: "无标题" # 可选：默认值
    item_id:
      selector: "a.item-link"
      attribute: "href"
      regex: "item/(\\d+)" # 使用捕获组提取数字ID
    author:
      selector: "span.author"
      attribute: "text"
    date:
      selector: "time.published"
      attribute: "datetime"
      # 可选：日期格式化
      date_format: "%Y-%m-%dT%H:%M:%S" # Python日期格式字符串
    # 可以添加更多字段选择器...

  # 页面导航选择器（可选）
  pagination_selector: "a.next-page"

  # 详情页配置（可选，用于需要访问详情页的情况）
  detail_page:
    enabled: false # 是否需要访问详情页
    fields: # 从详情页提取的字段
      content:
        selector: "div.content"
        attribute: "html"
      # 可以添加更多字段...

# 输出配置
output:
  # 输出文件字段
  fields:
    ["target_id", "target_name", "page", "item_id", "title", "author", "date"]

  # 日志配置
  logging:
    filename: "example_scraper.log"
    level: "INFO" # DEBUG, INFO, WARNING, ERROR, CRITICAL
    format: "%(asctime)s - %(levelname)s - %(message)s"

  # 数据存储配置（可选）
  storage:
    type: "file" # file, database, s3, etc.
    # 数据库配置（当type为database时）
    database:
      url: "sqlite:///data.db" # 数据库连接URL
      table: "scraped_data" # 表名
    # S3配置（当type为s3时）
    s3:
      bucket: "example-bucket"
      prefix: "scraped-data/"

# 过滤器配置（可选）
filters:
  # 关键词过滤
  keywords:
    include: [] # 必须包含的关键词列表
    exclude: [] # 不能包含的关键词列表

  # 正则表达式过滤
  regex:
    title: null # 标题必须匹配的正则表达式
    content: null # 内容必须匹配的正则表达式
