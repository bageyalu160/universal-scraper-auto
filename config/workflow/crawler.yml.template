name: {{ site_name }} 爬虫任务

# 定义何时触发此工作流程
on:
  # 允许手动触发
  workflow_dispatch:
  # 定时触发
  schedule:
    - cron: "{{ cron_schedule }}"

# 全局环境变量
env:
  PYTHON_VERSION: "{{ python_version }}"
  # 使用shell命令定义日期
  RUN_DATE: {% raw %}${{ github.event.inputs.date || '' }}{% endraw %}
  # 添加黑猫投诉Cookie环境变量
  HEIMAO_COOKIE: {% raw %}${{ secrets.HEIMAO_COOKIE }}{% endraw %}

# 定义工作流的权限
permissions:
  # 只需要内容写入权限
  contents: write
  # 添加工作流权限
  actions: write

# 定义工作流中的作业
jobs:
  # 爬虫作业
  scrape-website:
    name: 运行网站爬虫
    runs-on: ubuntu-latest

    steps:
      # 步骤1: 检出代码
      - name: 检出仓库代码
        uses: actions/checkout@v4
        with:
          # 完整克隆以便进行Git操作
          fetch-depth: 0

      # 步骤2: 设置日期环境变量
      - name: 设置日期环境变量
        run: |
          echo "RUN_DATE=$(date -u +"%Y-%m-%d")" >> $GITHUB_ENV

      # 步骤3: 设置Python环境
      - name: 设置Python环境
        uses: actions/setup-python@v5
        with:
          python-version: {% raw %}${{ env.PYTHON_VERSION }}{% endraw %}
          # 启用依赖缓存
          cache: "pip"

      # 步骤4: 安装依赖
      - name: 安装依赖
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            echo "安装必要的依赖..."
            pip install {{ scraper_dependencies }}
          fi

      # 步骤5: 创建数据目录
      - name: 创建数据目录
        run: |
          mkdir -p {{ data_dir }}/daily
          echo "创建日期目录: {{ data_dir }}/daily/{% raw %}${{ env.RUN_DATE }}{% endraw %}"
          mkdir -p {{ data_dir }}/daily/{% raw %}${{ env.RUN_DATE }}{% endraw %}

      # 步骤6: 运行爬虫脚本
      - name: 运行爬虫脚本
        id: run-scraper
        continue-on-error: true # 允许失败后继续执行工作流
        env:
{% for env_var in env_vars %}
          {{ env_var.name }}: {% raw %}${{ secrets.{% endraw %}{{ env_var.secret }}{% raw %} }}{% endraw %}
{% endfor %}
          # 可选环境变量，使用GitHub Variables
          HEIMAO_KEYWORDS: {% raw %}${{ vars.HEIMAO_KEYWORDS || '' }}{% endraw %}
          ENABLE_NOTIFICATION: {% raw %}${{ vars.ENABLE_NOTIFICATION || 'false' }}{% endraw %}
          NOTIFICATION_TYPE: {% raw %}${{ vars.NOTIFICATION_TYPE || 'none' }}{% endraw %}
          NOTIFICATION_WEBHOOK: {% raw %}${{ vars.NOTIFICATION_WEBHOOK || '' }}{% endraw %}
        run: |
          echo "开始运行爬虫..."
          python {{ scraper_script }} --site {{ site_id }} --config config/sites/{{ site_id }}.yaml

          # 检查生成的文件
          if [ -f "{{ output_filename }}" ]; then
            echo "爬虫成功完成，发现结果文件"
            echo "file_exists=true" >> $GITHUB_OUTPUT
            echo "file_size=$(stat -c%s {{ output_filename }})" >> $GITHUB_OUTPUT
            
            # 复制到日期目录
            cp {{ output_filename }} {{ data_dir }}/daily/{% raw %}${{ env.RUN_DATE }}{% endraw %}/
            echo "数据文件已复制到日期目录"
          else
            echo "警告：未找到结果文件"
            echo "file_exists=false" >> $GITHUB_OUTPUT
            echo "file_size=0" >> $GITHUB_OUTPUT
          fi

      # 步骤7: 创建运行状态文件
      - name: 创建爬虫状态文件
        run: |
          mkdir -p {{ status_dir }}
          # 创建状态文件
          if [ "{% raw %}${{ steps.run-scraper.outcome }}{% endraw %}" == "success" ] && [ "{% raw %}${{ steps.run-scraper.outputs.file_exists }}{% endraw %}" == "true" ]; then
            echo '{
              "status": "success",
              "date": "{% raw %}${{ env.RUN_DATE }}{% endraw %}",
              "timestamp": "'$(date -u +"%Y-%m-%dT%H:%M:%SZ")'",
              "file_path": "{{ data_dir }}/daily/{% raw %}${{ env.RUN_DATE }}{% endraw %}/{{ output_filename }}",
              "file_size": "{% raw %}${{ steps.run-scraper.outputs.file_size }}{% endraw %}",
              "message": "爬虫运行成功，已生成数据文件"
            }' > {{ status_dir }}/crawler_status.json
          else
            echo '{
              "status": "failed",
              "date": "{% raw %}${{ env.RUN_DATE }}{% endraw %}",
              "timestamp": "'$(date -u +"%Y-%m-%dT%H:%M:%SZ")'",
              "message": "爬虫运行失败或未生成文件"
            }' > {{ status_dir }}/crawler_status.json
          fi
          echo "已创建爬虫状态文件"

      # 步骤8: 提交结果和状态到仓库
      - name: 提交爬虫结果和状态
        run: |
          echo "正在提交爬虫结果和状态..."
          # 设置git配置
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          # 添加需要提交的文件
          git add {{ data_dir }}/daily/{% raw %}${{ env.RUN_DATE }}{% endraw %}/ || echo "没有数据目录变更"
          git add {{ output_filename }} || echo "没有主数据文件"
          git add {{ status_dir }}/crawler_status.json

          # 检查是否有变更需要提交
          if git diff --staged --quiet; then
            echo "没有变更需要提交"
          else
            # 创建提交
            git commit -m "自动更新：{{ site_name }}爬虫数据 {% raw %}${{ env.RUN_DATE }}{% endraw %}"
            # 推送到仓库
            git push
            echo "成功提交并推送爬虫结果和状态"
          fi

      {% if run_analysis %}
      # 步骤9: 触发分析工作流
      - name: 触发分析工作流
        if: {% raw %}steps.run-scraper.outputs.file_exists == 'true'{% endraw %}
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: analyzer_{{ site_id }}.yml
          token: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}
          inputs: '{"data_date": "{% raw %}${{ env.RUN_DATE }}{% endraw %}", "data_file": "{{ data_dir }}/daily/{% raw %}${{ env.RUN_DATE }}{% endraw %}/{{ output_filename }}", "site_id": "{{ site_id }}"}'
      {% endif %} 

      # 步骤10: 触发仪表盘更新工作流
      - name: 触发仪表盘更新工作流
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: update_dashboard.yml
          token: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}
          inputs: '{"site_id": "{{ site_id }}"}'
        if: {% raw %}steps.run-scraper.outputs.file_exists == 'true'{% endraw %} 