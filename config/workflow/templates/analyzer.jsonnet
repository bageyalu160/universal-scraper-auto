// åˆ†æå·¥ä½œæµæ¨¡æ¿ - Jsonnetç‰ˆæœ¬ (å¢å¼ºç‰ˆ)

local params = import 'params.libsonnet';
local utils = import 'utils.libsonnet';

// å¤–éƒ¨å‚æ•°
local site_id = std.extVar('site_id');
local site_config = std.parseJson(std.extVar('site_config'));
local global_config = std.parseJson(std.extVar('global_config'));

// ç«™ç‚¹ä¿¡æ¯
local site_name = utils.getSiteName(site_config, site_id);

// åˆ†æé…ç½®
local analysis_config = utils.getConfigSection(site_config, 'analysis', {});

// ç¡®å®šAIæä¾›å•†
local ai_provider = utils.getConfigValue(analysis_config, 'provider', 'openai');

// ç¡®å®šä¾èµ–é¡¹
local dependencies = utils.getAnalyzerDependencies(ai_provider, params.dependencies);

// ç¼“å­˜é…ç½®
local cache_config = utils.generateCacheConfig('analyzer', site_id, ai_provider);

// è¶…æ—¶è®¾ç½®
local analyze_timeout = utils.getJobTimeout('analyze', global_config);

// é”™è¯¯å¤„ç†ç­–ç•¥
local analyze_error_strategy = utils.getErrorHandlingStrategy('analyze', global_config);

// ç¡®å®šæç¤ºè¯æ¨¡æ¿
local prompt_template = utils.getConfigValue(analysis_config, 'prompt_template', 'default');

// ç¯å¢ƒå˜é‡
local workflow_env = utils.generateWorkflowEnv('analyzer', global_config);

{
  name: site_name + ' æ•°æ®åˆ†æ',
  'run-name': 'ğŸ§  ' + site_name + ' æ•°æ®åˆ†æ #${{ github.run_number }} (${{ github.actor }})',
  
  // å®šä¹‰å·¥ä½œæµçš„æƒé™
  permissions: {
    contents: 'write',  // å…è®¸æ¨é€åˆ°ä»“åº“
    actions: 'write'    // å…è®¸è§¦å‘å…¶ä»–å·¥ä½œæµ
  },
  
  on: utils.generateWorkflowDispatchTrigger({
    data_date: {
      description: 'æ•°æ®æ—¥æœŸ',
      required: true,
      type: 'string'
    },
    data_file: {
      description: 'æ•°æ®æ–‡ä»¶è·¯å¾„',
      required: true,
      type: 'string'
    },
    site_id: {
      description: 'ç«™ç‚¹ID',
      required: true,
      type: 'string',
      default: site_id
    }
  }),
  
  // å…¨å±€ç¯å¢ƒå˜é‡
  env: workflow_env,
  
  jobs: {
    analyze: {
      name: 'åˆ†ææ•°æ®',
      'runs-on': params.runtime.runner,
      'timeout-minutes': analyze_timeout,
      'continue-on-error': analyze_error_strategy['continue-on-error'],
      steps: [
        utils.generateCheckoutStep(),
        utils.generatePythonSetupStep(params.runtime.python_version, true),
        utils.generateCacheStep(cache_config, '**/requirements.txt, config/sites/' + site_id + '.yaml'),
        {
          name: 'å®‰è£…ä¾èµ–',
          run: 'pip install ' + std.join(' ', dependencies)
        },
        utils.generateDirectorySetupStep(['analysis/' + site_id, 'analysis/' + site_id + '/cache']),
        {
          name: 'è¿è¡Œåˆ†æ',
          env: {
            OPENAI_API_KEY: '${{ secrets.OPENAI_API_KEY }}',
            GEMINI_API_KEY: '${{ secrets.GEMINI_API_KEY }}',
            ANTHROPIC_API_KEY: '${{ secrets.ANTHROPIC_API_KEY }}'
          },
          run: |||
            python scripts/ai_analyzer.py \
              --site "${{ github.event.inputs.site_id }}" \
              --date "${{ github.event.inputs.data_date }}" \
              --input "${{ github.event.inputs.data_file }}" \
              --output "analysis/${{ github.event.inputs.site_id }}/${{ github.event.inputs.site_id }}_${{ github.event.inputs.data_date }}_analysis.json" \
              --prompt-template "%(prompt_template)s" \
              --provider "%(ai_provider)s"
          ||| % {
            prompt_template: prompt_template,
            ai_provider: ai_provider
          }
        },
        {
          name: 'ä¸Šä¼ åˆ†æç»“æœ',
          uses: 'actions/upload-artifact@v4',
          with: {
            name: '${{ github.event.inputs.site_id }}-analysis-${{ github.event.inputs.data_date }}',
            path: 'analysis/${{ github.event.inputs.site_id }}/${{ github.event.inputs.site_id }}_${{ github.event.inputs.data_date }}_analysis.json',
            'retention-days': 7
          }
        },
        utils.generateGitCommitStep(
          ["analysis/" + site_id + "/"],
          "ğŸ§  è‡ªåŠ¨æ›´æ–°: " + site_name + "åˆ†æç»“æœ (${{ github.event.inputs.data_date }})"
        )
      ]
    }
  }
}
