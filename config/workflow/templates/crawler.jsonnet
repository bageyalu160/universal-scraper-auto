// çˆ¬è™«å·¥ä½œæµæ¨¡æ¿ - Jsonnetç‰ˆæœ¬ (å¢å¼ºç‰ˆ)

local params = import 'params.libsonnet';
local utils = import 'utils.libsonnet';

// å¤–éƒ¨å‚æ•°
local site_id = std.extVar('site_id');
local site_config = std.parseJson(std.extVar('site_config'));
local global_config = std.parseJson(std.extVar('global_config'));

// ç«™ç‚¹ä¿¡æ¯
local site_name = utils.getSiteName(site_config, site_id);

// çˆ¬å–é…ç½®
local scraping_config = utils.getConfigSection(site_config, 'scraping', {});

// å¼•æ“ç±»å‹
local engine = utils.getConfigValue(scraping_config, 'engine', 'custom');

// ç¡®å®šä¾èµ–é¡¹
local dependencies = utils.getCrawlerDependencies(engine, params.dependencies);

// ç¼“å­˜é…ç½®
local cache_config = utils.generateCacheConfig('crawler', site_id, engine);

// è¶…æ—¶è®¾ç½®
local crawl_timeout = utils.getJobTimeout('crawl', global_config);
local setup_timeout = utils.getJobTimeout('setup', global_config);

// é”™è¯¯å¤„ç†ç­–ç•¥
local crawl_error_strategy = utils.getErrorHandlingStrategy('crawl', global_config);

// ç¯å¢ƒå˜é‡
local workflow_env = utils.generateWorkflowEnv('crawler', global_config) + {
  SITE_ID: site_id,
  ENGINE_TYPE: engine
};

// ç¡®å®šcronè¡¨è¾¾å¼
local schedule = if std.objectHas(scraping_config, 'schedule') then
  scraping_config.schedule
else
  params.schedules.master;

// ç¡®å®šè¾“å‡ºæ–‡ä»¶å
local output_filename = utils.getConfigValue(site_config, 'output.filename', 
                         utils.getConfigValue(site_config, 'site_info.output_filename', 
                         site_id + '_data.json'));

// æ£€æŸ¥æ˜¯å¦éœ€è¦è¿è¡Œåˆ†æ
local run_analysis = utils.getConfigValue(site_config, 'analysis.enabled', true);

// æ£€æŸ¥ä»£ç†é…ç½®
local proxy_config = utils.getConfigSection(scraping_config, 'proxy', {});
local use_proxy = utils.getConfigValue(proxy_config, 'enabled', false);

// ç¯å¢ƒå˜é‡
local env_vars = if std.objectHas(scraping_config, 'api') && std.objectHas(scraping_config.api, 'key_env') then
  [{
    name: scraping_config.api.key_env,
    secret: scraping_config.api.key_env
  }]
else
  [];

{
  name: site_name + ' (' + site_id + ') çˆ¬è™«',
  'run-name': 'ğŸ•·ï¸ ' + site_name + ' (' + site_id + ') çˆ¬è™« #${{ github.run_number }} (${{ github.actor }})',
  
  // å®šä¹‰å·¥ä½œæµçš„æƒé™
  permissions: {
    contents: 'write',  // å…è®¸æ¨é€åˆ°ä»“åº“
    actions: 'write'    // å…è®¸è§¦å‘å…¶ä»–å·¥ä½œæµ
  },
  
  on: utils.generateWorkflowDispatchTrigger({
    date: {
      description: 'æ•°æ®æ—¥æœŸ (ç•™ç©ºåˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ)',
      required: false,
      type: 'string'
    },
    use_proxy: {
      description: 'æ˜¯å¦ä½¿ç”¨ä»£ç†',
      required: false,
      type: 'boolean',
      default: use_proxy
    },
    parent_workflow_id: {
      description: 'çˆ¶å·¥ä½œæµID(ç”±ä¸»å·¥ä½œæµè§¦å‘æ—¶ä½¿ç”¨)',
      required: false,
      type: 'string'
    },
    retry: {
      description: 'æ˜¯å¦ä¸ºé‡è¯•æ‰§è¡Œ',
      required: false,
      type: 'boolean',
      default: false
    }
  }) + utils.generateScheduleTrigger(schedule),
  
  // å¹¶å‘æ§åˆ¶ - é¿å…ç›¸åŒç«™ç‚¹çš„ä»»åŠ¡å¹¶è¡Œè¿è¡Œ
  concurrency: utils.generateConcurrencyConfig('crawler', site_id),
  
  // å…¨å±€ç¯å¢ƒå˜é‡
  env: workflow_env,
  
  jobs: {
    // é¢„æ£€æŸ¥ä½œä¸š
    'pre-check': {
      name: 'ç¯å¢ƒä¸é…ç½®æ£€æŸ¥',
      'runs-on': params.runtime.runner,
      'timeout-minutes': setup_timeout,
      outputs: {
        run_date: '${{ steps.prepare_env.outputs.date }}',
        cache_key: '${{ steps.prepare_env.outputs.cache_key }}',
        site_config_valid: '${{ steps.validate_config.outputs.valid }}',
        use_proxy: '${{ steps.prepare_env.outputs.use_proxy }}'
      },
      steps: [
        utils.generateCheckoutStep(),
        {
          name: 'å‡†å¤‡ç¯å¢ƒå˜é‡',
          id: 'prepare_env',
          run: |||
            # è®¾ç½®è¿è¡Œæ—¥æœŸ
            if [ -n "${{ github.event.inputs.date }}" ]; then
              echo "ä½¿ç”¨æŒ‡å®šæ—¥æœŸ: ${{ github.event.inputs.date }}"
              echo "date=${{ github.event.inputs.date }}" >> $GITHUB_OUTPUT
            else
              echo "ä½¿ç”¨å½“å‰æ—¥æœŸ"
              echo "date=$(date +%%Y-%%m-%%d)" >> $GITHUB_OUTPUT
            fi
            
            # ç”Ÿæˆç¼“å­˜é”®
            if [ -f "requirements.txt" ]; then
              HASH=$(sha256sum requirements.txt | cut -d ' ' -f 1 | head -c 8)
              echo "cache_key=deps-%(site_id)s-$HASH-v1" >> $GITHUB_OUTPUT
            else
              echo "cache_key=deps-%(site_id)s-default-v1" >> $GITHUB_OUTPUT
            fi
            
            # è®¾ç½®æ˜¯å¦ä½¿ç”¨ä»£ç†
            USE_PROXY="${{ github.event.inputs.use_proxy || '%(use_proxy)s' }}"
            echo "use_proxy=$USE_PROXY" >> $GITHUB_OUTPUT
            echo "ä»£ç†ä½¿ç”¨è®¾ç½®: $USE_PROXY"
          ||| % {site_id: site_id, use_proxy: use_proxy}
        },
        {
          name: 'éªŒè¯ç«™ç‚¹é…ç½®',
          id: 'validate_config',
          run: |||
            if [ -f "config/sites/%(site_id)s.yaml" ]; then
              echo "âœ… ç«™ç‚¹é…ç½®æœ‰æ•ˆ"
              echo "valid=true" >> $GITHUB_OUTPUT
            else
              echo "âŒ ç«™ç‚¹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"
              echo "valid=false" >> $GITHUB_OUTPUT
              exit 1
            fi
          ||| % {site_id: site_id}
        }
      ]
    },
    
    // æ£€æŸ¥ä»£ç†å¯ç”¨æ€§ï¼ˆå¦‚æœéœ€è¦ï¼‰
    'check-proxy': {
      name: 'æ£€æŸ¥ä»£ç†å¯ç”¨æ€§',
      needs: ['pre-check'],
      'if': "needs.pre-check.outputs.use_proxy == 'true'",
      'runs-on': params.runtime.runner,
      steps: [
        utils.generateCheckoutStep(0),
        utils.generatePythonSetupStep(params.runtime.python_version, true),
        {
          name: 'å®‰è£…ä¾èµ–',
          run: |||
            python -m pip install --upgrade pip
            if [ -f requirements.txt ]; then
              pip install -r requirements.txt
            else
              echo "å®‰è£…å¿…è¦çš„ä¾èµ–..."
              pip install requests pyyaml
            fi
          |||
        },
        utils.generateDirectorySetupStep(['data/proxies', 'status/proxies', 'logs']),
        {
          name: 'æ£€æŸ¥ä»£ç†æ± çŠ¶æ€',
          id: 'check_proxy_pool',
          run: |||
            # æ£€æŸ¥ä»£ç†çŠ¶æ€æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if [ -f "status/proxies/pool_status.json" ]; then
              echo "æ‰¾åˆ°ä»£ç†æ± çŠ¶æ€æ–‡ä»¶"
              
              # æ£€æŸ¥å¯ç”¨ä»£ç†æ•°é‡
              VALID_COUNT=$(jq '.valid_count' status/proxies/pool_status.json)
              THRESHOLD=5
              
              echo "å½“å‰æœ‰æ•ˆä»£ç†æ•°: $VALID_COUNT"
              echo "æœ€ä½éœ€æ±‚é˜ˆå€¼: $THRESHOLD"
              
              if [ "$VALID_COUNT" -ge "$THRESHOLD" ]; then
                echo "âœ… ä»£ç†æ± çŠ¶æ€è‰¯å¥½ï¼Œæœ‰è¶³å¤Ÿçš„ä»£ç†"
                echo "sufficient=true" >> $GITHUB_OUTPUT
              else
                echo "âš ï¸ ä»£ç†æ± ä¸­çš„æœ‰æ•ˆä»£ç†ä¸è¶³ï¼Œéœ€è¦æ›´æ–°"
                echo "sufficient=false" >> $GITHUB_OUTPUT
              fi
            else
              echo "âš ï¸ æœªæ‰¾åˆ°ä»£ç†æ± çŠ¶æ€æ–‡ä»¶ï¼Œéœ€è¦åˆå§‹åŒ–ä»£ç†æ± "
              echo "sufficient=false" >> $GITHUB_OUTPUT
            fi
          |||
        },
        {
          name: 'æ›´æ–°ä»£ç†æ± ',
          'if': "steps.check_proxy_pool.outputs.sufficient == 'false'",
          run: |||
            echo "å¼€å§‹æ›´æ–°ä»£ç†æ± ..."
            
            # å°è¯•æ‰§è¡Œæ›´æ–°ï¼Œå¦‚æœå¤±è´¥åˆ™å°è¯•æ¢å¤
            python scripts/proxy_manager.py update --min-count 10 --timeout 10
            
            # æ£€æŸ¥æ›´æ–°åçš„çŠ¶æ€
            if [ -f "status/proxies/pool_status.json" ]; then
              valid_count=$(jq '.valid_count' status/proxies/pool_status.json)
              echo "æ›´æ–°åçš„æœ‰æ•ˆä»£ç†æ•°: $valid_count"
              
              # å¦‚æœæ›´æ–°åä»£ç†ä»ç„¶ä¸è¶³ï¼Œå°è¯•æ¢å¤å¤‡ä»½
              if [ "$valid_count" -lt "5" ] && [ -f "data/proxies/proxy_pool_backup.json" ]; then
                echo "âš ï¸ æ›´æ–°åä»£ç†ä»ç„¶ä¸è¶³ï¼Œå°è¯•æ¢å¤å¤‡ä»½..."
                cp data/proxies/proxy_pool_backup.json data/proxies/proxy_pool.json
                python scripts/proxy_manager.py validate --timeout 5
                
                if [ -f "status/proxies/pool_status.json" ]; then
                  valid_count=$(jq '.valid_count' status/proxies/pool_status.json)
                  echo "æ¢å¤å¤‡ä»½åçš„æœ‰æ•ˆä»£ç†æ•°: $valid_count"
                fi
              fi
              
              echo "æ›´æ–°åçš„æœ‰æ•ˆä»£ç†æ•°: $valid_count"
            else
              echo "âš ï¸ æ›´æ–°åä»æœªæ‰¾åˆ°ä»£ç†æ± çŠ¶æ€æ–‡ä»¶"
            fi
          |||
        },
        utils.generateGitCommitStep(
          ["data/proxies/", "status/proxies/"],
          "ğŸ”„ çˆ¬è™«ä»»åŠ¡å‰çš„ä»£ç†æ± æ›´æ–° (ç«™ç‚¹: " + site_id + ")"
        )
      ]
    },
    
    // çˆ¬è™«ä½œä¸š
    crawl: {
      name: 'è¿è¡Œçˆ¬è™«',
      needs: ['pre-check', 'check-proxy'],
      'if': "always() && needs.pre-check.outputs.site_config_valid == 'true' && (needs.check-proxy.result == 'success' || needs.pre-check.outputs.use_proxy != 'true')",
      'runs-on': params.runtime.runner,
      'timeout-minutes': crawl_timeout,
      strategy: {
        'fail-fast': crawl_error_strategy['fail-fast']
      },
      env: {
        RUN_DATE: '${{ needs.pre-check.outputs.run_date }}',
        USE_PROXY: '${{ needs.pre-check.outputs.use_proxy }}'
      },
      steps: [
        utils.generateCheckoutStep(),
        utils.generatePythonSetupStep(params.runtime.python_version, true),
        utils.generateCacheStep(cache_config, 'requirements.txt'),
        {
          name: 'å®‰è£…ä¾èµ–',
          run: |||
            python -m pip install --upgrade pip
            if [ -f requirements.txt ]; then
              pip install -r requirements.txt
            else
              echo "å®‰è£…å¿…è¦çš„ä¾èµ–..."
              pip install %(dependencies)s
            fi
          ||| % {dependencies: dependencies}
        },
        utils.generateDirectorySetupStep(['data/' + site_id, 'status/' + site_id, 'logs']),
        {
          name: 'è¿è¡Œçˆ¬è™«',
          id: 'run_scraper',
          'continue-on-error': true,
          env: {
            [env_var.name]: '${{ secrets.' + env_var.secret + ' }}'
            for env_var in env_vars
          },
          run: |||
            echo "ğŸ•·ï¸ å¼€å§‹çˆ¬å–æ•°æ®: %(site_id)s"
            echo "ğŸ“… è¿è¡Œæ—¥æœŸ: $RUN_DATE"
            echo "ğŸ”„ ä½¿ç”¨ä»£ç†: $USE_PROXY"
            
            # æ„å»ºå‘½ä»¤å‚æ•°
            PROXY_ARG=""
            if [ "$USE_PROXY" = "true" ] && [ -f "data/proxies/proxy_pool.json" ]; then
              PROXY_ARG="--proxy-file data/proxies/proxy_pool.json"
              echo "ğŸ“‹ ä½¿ç”¨ä»£ç†æ± æ–‡ä»¶"
            fi
            
            # æ‰§è¡Œçˆ¬è™«
            python scripts/scraper.py \
              --site %(site_id)s \
              --date "$RUN_DATE" \
              --output data/%(site_id)s/%(output_filename)s \
              --status status/%(site_id)s/status.json \
              --log-file logs/%(site_id)s_$RUN_DATE.log \
              $PROXY_ARG
            
            # æ£€æŸ¥ç»“æœ
            if [ -f "data/%(site_id)s/%(output_filename)s" ]; then
              echo "scraper_success=true" >> $GITHUB_OUTPUT
              echo "âœ… çˆ¬è™«æ‰§è¡ŒæˆåŠŸ"
            else
              echo "scraper_success=false" >> $GITHUB_OUTPUT
              echo "âŒ çˆ¬è™«æ‰§è¡Œå¤±è´¥æˆ–æ— æ•°æ®"
            fi
          ||| % {
            site_id: site_id,
            output_filename: output_filename
          }
        },
        {
          name: 'ä¸Šä¼ æ•°æ®æ–‡ä»¶',
          'if': "steps.run_scraper.outputs.scraper_success == 'true'",
          uses: 'actions/upload-artifact@v4',
          with: {
            name: site_id + '-data-${{ needs.pre-check.outputs.run_date }}',
            path: 'data/' + site_id + '/' + output_filename,
            'retention-days': 7,
            'if-no-files-found': 'warn'
          }
        },
        {
          name: 'ä¸Šä¼ çŠ¶æ€æ–‡ä»¶',
          uses: 'actions/upload-artifact@v4',
          with: {
            name: site_id + '-status-${{ needs.pre-check.outputs.run_date }}',
            path: 'status/' + site_id + '/status.json',
            'retention-days': 7,
            'if-no-files-found': 'warn'
          }
        },
        {
          name: 'ä¸Šä¼ æ—¥å¿—æ–‡ä»¶',
          uses: 'actions/upload-artifact@v4',
          with: {
            name: site_id + '-logs-${{ needs.pre-check.outputs.run_date }}',
            path: 'logs/' + site_id + '_${{ needs.pre-check.outputs.run_date }}.log',
            'retention-days': 3,
            'if-no-files-found': 'warn'
          }
        },
        {
          name: 'é…ç½®Gitå¹¶æäº¤æ›´æ”¹',
          run: |
            # é…ç½®Git
            git config user.name "github-actions[bot]"
            git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

            # æ·»åŠ æ–‡ä»¶
            git add data/%(site_id)s/
            git add status/%(site_id)s/

            # æ‹‰å–è¿œç¨‹æ›´æ”¹ï¼Œé¿å…æ¨é€å†²çª
            git pull --rebase origin main || echo "æ‹‰å–è¿œç¨‹ä»“åº“å¤±è´¥ï¼Œå°è¯•ç»§ç»­æäº¤"

            # æäº¤æ›´æ”¹
            if git diff --staged --quiet; then
              echo "æ²¡æœ‰å˜æ›´éœ€è¦æäº¤"
            else
              git commit -m "ğŸ¤– è‡ªåŠ¨æ›´æ–°: %(site_name)sçˆ¬è™«ç»“æœ ($RUN_DATE)"
              git push
              echo "âœ… æˆåŠŸæäº¤å¹¶æ¨é€çˆ¬è™«ç»“æœ"
            fi
          ||| % {site_id: site_id, site_name: site_name}
        },
        // æ·»åŠ å·¥ä½œæµçŠ¶æ€æŠ¥å‘Š
        utils.generateWorkflowStatusStep('crawler', site_id)
      ]
    }
  }
}
