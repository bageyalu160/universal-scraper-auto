// çˆ¬è™«å·¥ä½œæµæ¨¡æ¿ - Jsonnetç‰ˆæœ¬ (å¢å¼ºç‰ˆ)

local params = import 'params.libsonnet';
local utils = import 'utils.libsonnet';

// å¤–éƒ¨å‚æ•°
local site_id = std.extVar('site_id');
local site_config = std.parseJson(std.extVar('site_config'));
local global_config = std.parseJson(std.extVar('global_config'));

// ç«™ç‚¹ä¿¡æ¯
local site_name = if std.objectHas(site_config, 'site_info') && std.objectHas(site_config.site_info, 'name') then
  site_config.site_info.name
else
  site_id + ' ç«™ç‚¹';

// çˆ¬å–é…ç½®
local scraping_config = if std.objectHas(site_config, 'scraping') then
  site_config.scraping
else
  {};

// å¼•æ“ç±»å‹
local engine = if std.objectHas(scraping_config, 'engine') then
  scraping_config.engine
else
  'custom';

// ç¡®å®šä¾èµ–é¡¹
local dependencies = if engine == 'firecrawl' then
  params.crawler.dependencies.firecrawl
else if engine == 'playwright' then
  params.crawler.dependencies.playwright
else
  params.crawler.dependencies.default;

// ç¡®å®šcronè¡¨è¾¾å¼
local schedule = if std.objectHas(scraping_config, 'schedule') then
  scraping_config.schedule
else
  params.global.default_cron;

// ç¡®å®šè¾“å‡ºæ–‡ä»¶å
local output_filename = if std.objectHas(site_config, 'output') && std.objectHas(site_config.output, 'filename') then
  site_config.output.filename
else if std.objectHas(site_config, 'site_info') && std.objectHas(site_config.site_info, 'output_filename') then
  site_config.site_info.output_filename
else
  site_id + '_data.json';

// æ£€æŸ¥æ˜¯å¦éœ€è¦è¿è¡Œåˆ†æ
local run_analysis = if std.objectHas(site_config, 'analysis') && std.objectHas(site_config.analysis, 'enabled') then
  site_config.analysis.enabled
else
  true;

// æ£€æŸ¥ä»£ç†é…ç½®
local proxy_config = if std.objectHas(scraping_config, 'proxy') then
  scraping_config.proxy
else
  {};
local use_proxy = if std.objectHas(proxy_config, 'enabled') then
  proxy_config.enabled
else
  false;

// ç¯å¢ƒå˜é‡
local env_vars = if std.objectHas(scraping_config, 'api') && std.objectHas(scraping_config.api, 'key_env') then
  [{
    name: scraping_config.api.key_env,
    secret: scraping_config.api.key_env
  }]
else
  [];

{
  name: site_name + ' çˆ¬è™«',
  'run-name': 'ğŸ•·ï¸ ' + site_name + ' çˆ¬è™« #${{ github.run_number }} (${{ github.actor }})',
  
  on: {
    workflow_dispatch: {
      inputs: {
        date: {
          description: 'æ•°æ®æ—¥æœŸ (ç•™ç©ºåˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ)',
          required: false,
          type: 'string'
        },
        use_proxy: {
          description: 'æ˜¯å¦ä½¿ç”¨ä»£ç†',
          required: false,
          type: 'boolean',
          default: use_proxy
        }
      }
    },
    schedule: [
      {cron: schedule}
    ]
  },
  
  // å¹¶å‘æ§åˆ¶ - é¿å…ç›¸åŒç«™ç‚¹çš„ä»»åŠ¡å¹¶è¡Œè¿è¡Œ
  concurrency: {
    group: 'crawler-' + site_id + '-${{ github.ref }}',
    'cancel-in-progress': true
  },
  
  jobs: {
    // é¢„æ£€æŸ¥ä½œä¸š
    'pre-check': {
      name: 'ç¯å¢ƒä¸é…ç½®æ£€æŸ¥',
      'runs-on': params.global.runner,
      outputs: {
        run_date: '${{ steps.prepare_env.outputs.date }}',
        cache_key: '${{ steps.prepare_env.outputs.cache_key }}',
        site_config_valid: '${{ steps.validate_config.outputs.valid }}',
        use_proxy: '${{ steps.prepare_env.outputs.use_proxy }}'
      },
      steps: [
        {
          name: 'æ£€å‡ºä»£ç ',
          uses: 'actions/checkout@v4'
        },
        {
          name: 'å‡†å¤‡ç¯å¢ƒå˜é‡',
          id: 'prepare_env',
          run: |||
            # è®¾ç½®è¿è¡Œæ—¥æœŸ
            if [ -n "${{ github.event.inputs.date }}" ]; then
              echo "ä½¿ç”¨æŒ‡å®šæ—¥æœŸ: ${{ github.event.inputs.date }}"
              echo "date=${{ github.event.inputs.date }}" >> $GITHUB_OUTPUT
            else
              echo "ä½¿ç”¨å½“å‰æ—¥æœŸ"
              echo "date=$(date +%%Y-%%m-%%d)" >> $GITHUB_OUTPUT
            fi
            
            # ç”Ÿæˆç¼“å­˜é”®
            if [ -f "requirements.txt" ]; then
              HASH=$(sha256sum requirements.txt | cut -d ' ' -f 1 | head -c 8)
              echo "cache_key=deps-%(site_id)s-$HASH-v1" >> $GITHUB_OUTPUT
            else
              echo "cache_key=deps-%(site_id)s-default-v1" >> $GITHUB_OUTPUT
            fi
            
            # è®¾ç½®æ˜¯å¦ä½¿ç”¨ä»£ç†
            USE_PROXY="${{ github.event.inputs.use_proxy || '%(use_proxy)s' }}"
            echo "use_proxy=$USE_PROXY" >> $GITHUB_OUTPUT
            echo "ä»£ç†ä½¿ç”¨è®¾ç½®: $USE_PROXY"
          ||| % {site_id: site_id, use_proxy: use_proxy}
        },
        {
          name: 'éªŒè¯ç«™ç‚¹é…ç½®',
          id: 'validate_config',
          run: |||
            if [ -f "config/sites/%(site_id)s.yaml" ]; then
              echo "âœ… ç«™ç‚¹é…ç½®æœ‰æ•ˆ"
              echo "valid=true" >> $GITHUB_OUTPUT
            else
              echo "âŒ ç«™ç‚¹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"
              echo "valid=false" >> $GITHUB_OUTPUT
              exit 1
            fi
          ||| % {site_id: site_id}
        }
      ]
    },
    
    // æ£€æŸ¥ä»£ç†å¯ç”¨æ€§ï¼ˆå¦‚æœéœ€è¦ï¼‰
    'check-proxy': {
      name: 'æ£€æŸ¥ä»£ç†å¯ç”¨æ€§',
      needs: ['pre-check'],
      'if': "needs.pre-check.outputs.use_proxy == 'true'",
      'runs-on': params.global.runner,
      steps: [
        {
          name: 'æ£€å‡ºä»£ç ',
          uses: 'actions/checkout@v4',
          with: {
            'fetch-depth': 0
          }
        },
        {
          name: 'è®¾ç½®Pythonç¯å¢ƒ',
          uses: 'actions/setup-python@v5',
          with: {
            'python-version': params.global.python_version,
            cache: 'pip'
          }
        },
        {
          name: 'å®‰è£…ä¾èµ–',
          run: |||
            python -m pip install --upgrade pip
            if [ -f requirements.txt ]; then
              pip install -r requirements.txt
            else
              echo "å®‰è£…å¿…è¦çš„ä¾èµ–..."
              pip install requests pyyaml
            fi
          |||
        },
        {
          name: 'åˆ›å»ºå¿…è¦ç›®å½•',
          run: |||
            mkdir -p data/proxies
            mkdir -p status/proxies
            mkdir -p logs
          |||
        },
        {
          name: 'æ£€æŸ¥ä»£ç†æ± çŠ¶æ€',
          id: 'check_proxy_pool',
          run: |||
            # æ£€æŸ¥ä»£ç†çŠ¶æ€æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if [ -f "status/proxies/pool_status.json" ]; then
              echo "å‘ç°ä»£ç†æ± çŠ¶æ€æ–‡ä»¶"
              
              # è·å–ä»£ç†ç»Ÿè®¡
              if [ -x "$(command -v jq)" ]; then
                valid_count=$(cat status/proxies/pool_status.json | jq '.stats.valid_count')
              else
                valid_count=$(grep -o '"valid_count":[0-9]*' status/proxies/pool_status.json | grep -o '[0-9]*')
              fi
              
              echo "å½“å‰æœ‰æ•ˆä»£ç†æ•°: $valid_count"
              
              # æ£€æŸ¥ä»£ç†æ•°é‡æ˜¯å¦è¶³å¤Ÿ
              if [ "$valid_count" -lt "5" ]; then
                echo "âš ï¸ ä»£ç†æ•°é‡ä¸è¶³ ($valid_count < 5)ï¼Œéœ€è¦æ›´æ–°ä»£ç†æ± "
                echo "sufficient=false" >> $GITHUB_OUTPUT
              else
                echo "âœ… ä»£ç†æ•°é‡å……è¶³ ($valid_count >= 5)"
                echo "sufficient=true" >> $GITHUB_OUTPUT
              fi
            else
              echo "âš ï¸ æœªæ‰¾åˆ°ä»£ç†æ± çŠ¶æ€æ–‡ä»¶ï¼Œéœ€è¦åˆå§‹åŒ–ä»£ç†æ± "
              echo "sufficient=false" >> $GITHUB_OUTPUT
            fi
          |||
        },
        {
          name: 'æ›´æ–°ä»£ç†æ± ',
          'if': "steps.check_proxy_pool.outputs.sufficient == 'false'",
          run: |||
            echo "å¼€å§‹æ›´æ–°ä»£ç†æ± ..."
            
            # å°è¯•æ‰§è¡Œæ›´æ–°ï¼Œå¦‚æœå¤±è´¥åˆ™å°è¯•æ¢å¤
            if ! python scripts/proxy_manager.py --action update --source all; then
              echo "æ›´æ–°å¤±è´¥ï¼Œå°è¯•æ¢å¤..."
              if ! python scripts/proxy_manager.py --action recover; then
                echo "æ¢å¤å¤±è´¥ï¼Œå°è¯•é‡å»º..."
                python scripts/proxy_manager.py --action rebuild --source all
              fi
            fi
            
            # æ£€æŸ¥æ›´æ–°åçš„çŠ¶æ€
            if [ -f "status/proxies/pool_status.json" ]; then
              if [ -x "$(command -v jq)" ]; then
                valid_count=$(cat status/proxies/pool_status.json | jq '.stats.valid_count')
              else
                valid_count=$(grep -o '"valid_count":[0-9]*' status/proxies/pool_status.json | grep -o '[0-9]*')
              fi
              
              echo "æ›´æ–°åçš„æœ‰æ•ˆä»£ç†æ•°: $valid_count"
            else
              echo "âš ï¸ æ›´æ–°åä»æœªæ‰¾åˆ°ä»£ç†æ± çŠ¶æ€æ–‡ä»¶"
            fi
          |||
        },
        {
          name: 'æäº¤ä»£ç†æ± çŠ¶æ€',
          'if': "steps.check_proxy_pool.outputs.sufficient == 'false'",
          run: |||
            # é…ç½®Git
            git config user.name "github-actions[bot]"
            git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
            
            # æ·»åŠ çŠ¶æ€æ–‡ä»¶
            git add status/proxies/pool_status.json
            git add data/proxies/
            
            # æäº¤æ›´æ”¹
            if git diff --staged --quiet; then
              echo "æ²¡æœ‰ä»£ç†æ± çŠ¶æ€å˜æ›´ï¼Œæ— éœ€æäº¤"
            else
              git commit -m "ğŸ”„ çˆ¬è™«ä»»åŠ¡å‰çš„ä»£ç†æ± æ›´æ–° (ç«™ç‚¹: %(site_id)s)"
              git push
              echo "âœ… æˆåŠŸæäº¤ä»£ç†æ± çŠ¶æ€æ›´æ–°"
            fi
          ||| % {site_id: site_id}
        }
      ]
    },
    
    // çˆ¬è™«ä½œä¸š
    crawl: {
      name: 'è¿è¡Œçˆ¬è™«',
      needs: ['pre-check', 'check-proxy'],
      'if': "always() && needs.pre-check.outputs.site_config_valid == 'true' && (needs.check-proxy.result == 'success' || needs.pre-check.outputs.use_proxy != 'true')",
      'runs-on': params.global.runner,
      env: {
        RUN_DATE: '${{ needs.pre-check.outputs.run_date }}',
        USE_PROXY: '${{ needs.pre-check.outputs.use_proxy }}'
      },
      steps: [
        {
          name: 'æ£€å‡ºä»£ç ',
          uses: 'actions/checkout@v4'
        },
        {
          name: 'è®¾ç½®Python',
          uses: 'actions/setup-python@v5',
          with: {
            'python-version': params.global.python_version,
            cache: 'pip',
            'cache-dependency-path': '**/requirements.txt'
          }
        },
        {
          name: 'å®‰è£…ä¾èµ–',
          run: |||
            python -m pip install --upgrade pip
            if [ -f requirements.txt ]; then
              pip install -r requirements.txt
            else
              echo "å®‰è£…å¿…è¦çš„ä¾èµ–..."
              pip install %(dependencies)s
            fi
          ||| % {dependencies: dependencies}
        },
        {
          name: 'åˆ›å»ºè¾“å‡ºç›®å½•',
          run: |||
            mkdir -p data/%(site_id)s
            mkdir -p status/%(site_id)s
            mkdir -p logs
          ||| % {site_id: site_id}
        },
        {
          name: 'è¿è¡Œçˆ¬è™«',
          id: 'run_scraper',
          'continue-on-error': true,
          env: {
            [env_var.name]: '${{ secrets.' + env_var.secret + ' }}'
            for env_var in env_vars
          },
          run: |||
            echo "ğŸ•·ï¸ å¼€å§‹çˆ¬å–æ•°æ®: %(site_id)s"
            echo "ğŸ“… è¿è¡Œæ—¥æœŸ: $RUN_DATE"
            echo "ğŸ”„ ä½¿ç”¨ä»£ç†: $USE_PROXY"
            
            # æ„å»ºå‘½ä»¤å‚æ•°
            PROXY_ARG=""
            if [ "$USE_PROXY" = "true" ] && [ -f "data/proxies/proxy_pool.json" ]; then
              PROXY_ARG="--proxy-file data/proxies/proxy_pool.json"
              echo "ğŸ“‹ ä½¿ç”¨ä»£ç†æ± æ–‡ä»¶"
            fi
            
            # æ‰§è¡Œçˆ¬è™«
            python scripts/scraper.py \
              --site %(site_id)s \
              --date "$RUN_DATE" \
              --output "data/%(site_id)s/%(output_filename)s" \
              --status "status/%(site_id)s/status.json" \
              --log-file "logs/%(site_id)s_$RUN_DATE.log" \
              $PROXY_ARG
            
            # æ£€æŸ¥ç»“æœ
            if [ -f "data/%(site_id)s/%(output_filename)s" ]; then
              echo "scraper_success=true" >> $GITHUB_OUTPUT
              echo "âœ… çˆ¬è™«æ‰§è¡ŒæˆåŠŸ"
            else
              echo "scraper_success=false" >> $GITHUB_OUTPUT
              echo "âŒ çˆ¬è™«æ‰§è¡Œå¤±è´¥æˆ–æ— æ•°æ®"
            fi
          ||| % {
            site_id: site_id,
            output_filename: output_filename
          }
        },
        {
          name: 'ä¸Šä¼ æ•°æ®æ–‡ä»¶',
          'if': "steps.run_scraper.outputs.scraper_success == 'true'",
          uses: 'actions/upload-artifact@v4',
          with: {
            name: site_id + '-data-${{ needs.pre-check.outputs.run_date }}',
            path: 'data/' + site_id + '/' + output_filename,
            'retention-days': 7
          }
        },
        {
          name: 'ä¸Šä¼ çŠ¶æ€æ–‡ä»¶',
          uses: 'actions/upload-artifact@v4',
          with: {
            name: site_id + '-status-${{ needs.pre-check.outputs.run_date }}',
            path: 'status/' + site_id + '/status.json',
            'retention-days': 7
          }
        },
        {
          name: 'ä¸Šä¼ æ—¥å¿—æ–‡ä»¶',
          uses: 'actions/upload-artifact@v4',
          with: {
            name: site_id + '-logs-${{ needs.pre-check.outputs.run_date }}',
            path: 'logs/' + site_id + '_${{ needs.pre-check.outputs.run_date }}.log',
            'retention-days': 3
          }
        },
        {
          name: 'æäº¤ç»“æœå’ŒçŠ¶æ€',
          run: |||
            # é…ç½®Git
            git config user.name "github-actions[bot]"
            git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
            
            # æ·»åŠ æ–‡ä»¶
            git add data/%(site_id)s/
            git add status/%(site_id)s/
            
            # æäº¤æ›´æ”¹
            if git diff --staged --quiet; then
              echo "æ²¡æœ‰å˜æ›´éœ€è¦æäº¤"
            else
              git commit -m "ğŸ¤– è‡ªåŠ¨æ›´æ–°: %(site_name)sçˆ¬è™«ç»“æœ ($RUN_DATE)"
              git push
              echo "âœ… æˆåŠŸæäº¤çˆ¬è™«ç»“æœ"
            fi
          ||| % {site_id: site_id, site_name: site_name}
        }
      ]
    }
  }
}
