name: {{ site_name }} 爬虫任务

# 工作流运行名称（动态生成）
run-name: "🕸️ {{ site_name }}爬虫 #${{ github.run_number }} (${{ github.actor }} 触发)"

# 多种触发方式组合
on:
  # 允许手动触发，带参数
  workflow_dispatch:
    inputs:
      date:
        description: '数据日期 (留空则使用当前日期)'
        required: false
        type: string
      debug:
        description: '是否启用调试模式'
        required: false
        default: false
        type: boolean
      keywords:
        description: '搜索关键词 (以逗号分隔)'
        required: false
        type: string
  
  # 定时触发
  schedule:
    - cron: "{{ cron_schedule }}"
  
  # 代码变更触发 - 仅当配置文件或脚本改变时
  push:
    branches: [ main ]
    paths:
      - 'config/sites/{{ site_id }}.yaml'
      - 'src/scrapers/**'
      - '.github/workflows/crawler_{{ site_id }}.yml'

# 全局环境变量
env:
  PYTHON_VERSION: "{{ python_version }}"
  SITE_ID: "{{ site_id }}"
  TZ: "Asia/Shanghai"
  # 使用shell命令定义日期
  RUN_DATE: {% raw %}${{ github.event.inputs.date || '' }}{% endraw %}
  # 代理池和反爬机制配置
  USE_PROXY: {% raw %}${{ vars.USE_PROXY || 'true' }}{% endraw %}
  ROTATE_PROXY: {% raw %}${{ vars.ROTATE_PROXY || 'true' }}{% endraw %}
  ANTI_DETECT_ENABLED: {% raw %}${{ vars.ANTI_DETECT_ENABLED || 'true' }}{% endraw %}
  # 添加黑猫投诉Cookie环境变量
  HEIMAO_COOKIE: {% raw %}${{ secrets.HEIMAO_COOKIE }}{% endraw %}

# 定义工作流的权限
permissions:
  contents: write  # 允许推送到仓库
  actions: write   # 允许触发其他工作流

# 全局默认配置
defaults:
  run:
    shell: bash

# 并发控制 - 避免相同站点的任务并行运行
concurrency:
  group: crawler-{{ site_id }}-{% raw %}${{ github.ref }}{% endraw %}
  cancel-in-progress: true

jobs:
  # 预检查作业
  pre-check:
    name: 环境与配置检查
    runs-on: ubuntu-latest
    outputs:
      run_date: {% raw %}${{ steps.prepare_env.outputs.date }}{% endraw %}
      cache_key: {% raw %}${{ steps.prepare_env.outputs.cache_key }}{% endraw %}
      site_config_valid: {% raw %}${{ steps.validate_config.outputs.valid }}{% endraw %}
    
    steps:
      - name: 检出代码
        uses: actions/checkout@v4
      
      - name: 准备环境变量
        id: prepare_env
        run: |
          # 设置运行日期
          if [ -n "$RUN_DATE" ]; then
            echo "使用指定日期: $RUN_DATE"
            echo "date=$RUN_DATE" >> $GITHUB_OUTPUT
          else
            echo "使用当前日期"
            echo "date=$(date +%Y-%m-%d)" >> $GITHUB_OUTPUT
          fi
          
          # 生成缓存键
          if [ -f "requirements.txt" ]; then
            HASH=$(sha256sum requirements.txt | cut -d ' ' -f 1 | head -c 8)
            echo "cache_key=deps-${{ site_id }}-$HASH-v1" >> $GITHUB_OUTPUT
          else
            echo "cache_key=deps-${{ site_id }}-default-v1" >> $GITHUB_OUTPUT
          fi
      
      - name: 验证站点配置
        id: validate_config
        run: |
          if [ -f "config/sites/$SITE_ID.yaml" ]; then
            echo "✅ 站点配置有效"
            echo "valid=true" >> $GITHUB_OUTPUT
          else
            echo "❌ 站点配置文件不存在"
            echo "valid=false" >> $GITHUB_OUTPUT
            exit 1
          fi

  # 爬虫作业 - 使用矩阵策略支持多引擎
  crawl:
    name: 运行爬虫
    needs: pre-check
    runs-on: ubuntu-latest
    env:
      RUN_DATE: {% raw %}${{ needs.pre-check.outputs.run_date }}{% endraw %}
    
    steps:
      - name: 检出代码
        uses: actions/checkout@v4
      
      - name: 设置Python环境
        uses: actions/setup-python@v5
        with:
          python-version: {% raw %}${{ env.PYTHON_VERSION }}{% endraw %}
          cache: "pip"
          cache-dependency-path: "**/requirements.txt"
      
      - name: 安装依赖
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            echo "安装必要的依赖..."
            pip install {{ scraper_dependencies }}
          fi

      # 新增步骤：检查代理池状态
      - name: 检查代理池状态
        id: check_proxy_pool
        if: env.USE_PROXY == 'true'
        run: |
          if [ -f "status/proxies/pool_status.json" ]; then
            echo "代理池状态文件存在"
            # 获取有效代理数量
            valid_count=$(cat status/proxies/pool_status.json | jq '.valid_proxies | length')
            echo "有效代理数量: $valid_count"
            
            if [ "$valid_count" -lt "5" ]; then
              echo "⚠️ 有效代理数量不足，尝试更新代理池"
              echo "needs_update=true" >> $GITHUB_OUTPUT
            else
              echo "✅ 代理池状态正常"
              echo "needs_update=false" >> $GITHUB_OUTPUT
            fi
            echo "pool_exists=true" >> $GITHUB_OUTPUT
          else
            echo "⚠️ 代理池状态文件不存在，需要初始化"
            echo "pool_exists=false" >> $GITHUB_OUTPUT
            echo "needs_update=true" >> $GITHUB_OUTPUT
          fi
      
      # 新增步骤：更新代理池(如果需要)
      - name: 更新代理池
        if: env.USE_PROXY == 'true' && steps.check_proxy_pool.outputs.needs_update == 'true'
        env:
          PROXY_API_KEY: {% raw %}${{ secrets.PROXY_API_KEY }}{% endraw %}
          PROXY_SOURCE_URL: {% raw %}${{ secrets.PROXY_SOURCE_URL }}{% endraw %}
        run: |
          echo "执行代理池更新..."
          
          if [ "{% raw %}${{ steps.check_proxy_pool.outputs.pool_exists }}{% endraw %}" == "true" ]; then
            # 更新现有代理池
            python scripts/proxy_manager.py --action update --source all
          else
            # 初始化代理池
            python scripts/proxy_manager.py --action rebuild --source all
          fi
          
          # 检查更新结果
          if [ -f "status/proxies/pool_status.json" ]; then
            valid_count=$(cat status/proxies/pool_status.json | jq '.valid_proxies | length')
            echo "更新后有效代理: $valid_count 个"
            
            if [ "$valid_count" -eq "0" ]; then
              echo "⚠️ 警告：代理池更新后无可用代理"
            fi
          else
            echo "❌ 代理池更新失败，状态文件不存在"
          fi
      
      # 安装Playwright (如需要)
      - name: 安装Playwright (如需要)
        id: check_engine
        run: |
          # 读取站点配置文件中的engine值
          ENGINE=$(grep -E "engine:" config/sites/${{ env.SITE_ID }}.yaml | head -1 | awk '{print $2}' | tr -d '"' | tr -d "'")
          echo "engine=$ENGINE" >> $GITHUB_OUTPUT
          
          # 如果是playwright或browser类型，则安装playwright
          if [[ "$ENGINE" == "playwright" || "$ENGINE" == "browser" ]]; then
            echo "需要安装Playwright"
            pip install playwright
            playwright install --with-deps chromium
          else
            echo "不需要安装Playwright"
          fi
      
      - name: 创建数据目录
        run: |
          mkdir -p {{ data_dir }}/daily/$RUN_DATE
      
      - name: 运行爬虫
        id: run_scraper
        continue-on-error: true
        env:
{% for env_var in env_vars %}
          {{ env_var.name }}: {% raw %}${{ secrets.{% endraw %}{{ env_var.secret }}{% raw %} }}{% endraw %}
{% endfor %}
          # 代理池相关
          PROXY_API_KEY: {% raw %}${{ secrets.PROXY_API_KEY }}{% endraw %}
          PROXY_SOURCE_URL: {% raw %}${{ secrets.PROXY_SOURCE_URL }}{% endraw %}
          USE_PROXY: {% raw %}${{ env.USE_PROXY }}{% endraw %}
          ROTATE_PROXY: {% raw %}${{ env.ROTATE_PROXY }}{% endraw %}
          
          # 反爬机制相关
          ANTI_DETECT_ENABLED: {% raw %}${{ env.ANTI_DETECT_ENABLED }}{% endraw %}
          CAPTCHA_API_KEY: {% raw %}${{ secrets.CAPTCHA_API_KEY }}{% endraw %}
          CAPTCHA_SERVICE: {% raw %}${{ vars.CAPTCHA_SERVICE || '2captcha' }}{% endraw %}
          BROWSER_FINGERPRINT: {% raw %}${{ vars.BROWSER_FINGERPRINT || 'true' }}{% endraw %}
          
          # 原有环境变量
          HEIMAO_COOKIE: {% raw %}${{ secrets.HEIMAO_COOKIE }}{% endraw %}
          HEIMAO_KEYWORDS: {% raw %}${{ github.event.inputs.keywords || vars.HEIMAO_KEYWORDS || '' }}{% endraw %}
          ENABLE_NOTIFICATION: {% raw %}${{ vars.ENABLE_NOTIFICATION || 'false' }}{% endraw %}
          NOTIFICATION_TYPE: {% raw %}${{ vars.NOTIFICATION_TYPE || 'none' }}{% endraw %}
          NOTIFICATION_WEBHOOK: {% raw %}${{ vars.NOTIFICATION_WEBHOOK || '' }}{% endraw %}
          DEBUG_MODE: {% raw %}${{ github.event.inputs.debug || 'false' }}{% endraw %}
        run: |
          echo "📌 开始运行爬虫 (站点: $SITE_ID, 日期: $RUN_DATE)"
          
          if [ "$DEBUG_MODE" = "true" ]; then
            echo "🔍 调试模式已启用"
            export LOG_LEVEL=DEBUG
          fi
          
          # 记录代理和反爬设置
          echo "🔒 代理设置: USE_PROXY=$USE_PROXY, ROTATE_PROXY=$ROTATE_PROXY"
          echo "🛡️ 反爬设置: ANTI_DETECT_ENABLED=$ANTI_DETECT_ENABLED, BROWSER_FINGERPRINT=$BROWSER_FINGERPRINT"
          
          # 运行爬虫脚本
          python {{ scraper_script }} --site $SITE_ID --config config/sites/$SITE_ID.yaml --output {{ data_dir }}/daily/$RUN_DATE/
          
          # 检查是否成功
          exit_code=$?
          if [ $exit_code -eq 0 ]; then
            if [ -f "{{ output_filename }}" ]; then
              echo "✅ 爬虫成功完成，已生成数据文件"
              echo "file_exists=true" >> $GITHUB_OUTPUT
              echo "file_size=$(stat -c%s {{ output_filename }})" >> $GITHUB_OUTPUT
              echo "file_path={{ output_filename }}" >> $GITHUB_OUTPUT
              
              # 复制到日期目录
              cp {{ output_filename }} {{ data_dir }}/daily/$RUN_DATE/
              echo "数据文件已复制到日期目录"
            else
              echo "⚠️ 爬虫进程成功但未找到输出文件"
              echo "file_exists=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "❌ 爬虫进程失败 (退出代码: $exit_code)"
            echo "file_exists=false" >> $GITHUB_OUTPUT
          fi
      
      - name: 创建爬虫状态文件
        run: |
          mkdir -p {{ status_dir }}
          if [ "{% raw %}${{ steps.run_scraper.outputs.file_exists }}{% endraw %}" == "true" ]; then
            cat > {{ status_dir }}/crawler_$SITE_ID.json << EOF
            {
              "status": "success",
              "site_id": "$SITE_ID",
              "date": "$RUN_DATE",
              "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "file_path": "{{ data_dir }}/daily/$RUN_DATE/{{ output_filename }}",
              "file_size": "{% raw %}${{ steps.run_scraper.outputs.file_size }}{% endraw %}",
              "run_id": "{% raw %}${{ github.run_id }}{% endraw %}",
              "run_url": "{% raw %}${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}{% endraw %}",
              "message": "爬虫运行成功"
            }
            EOF
          else
            cat > {{ status_dir }}/crawler_$SITE_ID.json << EOF
            {
              "status": "failed",
              "site_id": "$SITE_ID",
              "date": "$RUN_DATE",
              "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "run_id": "{% raw %}${{ github.run_id }}{% endraw %}",
              "run_url": "{% raw %}${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}{% endraw %}",
              "message": "爬虫运行失败或未生成文件"
            }
            EOF
          fi
          echo "已创建爬虫状态文件"
      
      - name: 提交结果到仓库
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          
          git add {{ data_dir }}/daily/$RUN_DATE/ || echo "没有数据目录变更"
          git add {{ output_filename }} || echo "没有主数据文件"
          git add {{ status_dir }}/crawler_$SITE_ID.json
          
          if git diff --staged --quiet; then
            echo "没有变更需要提交"
          else
            git commit -m "🤖 自动更新: {{ site_name }}爬虫数据 ($RUN_DATE)"
            git push
            echo "✅ 成功提交并推送爬虫结果"
          fi
      
      - name: 上传爬取数据 (作为工件)
        if: {% raw %}steps.run_scraper.outputs.file_exists == 'true'{% endraw %}
        uses: actions/upload-artifact@v4
        with:
          name: {{ site_id }}-data-${% raw %}{{ needs.pre-check.outputs.run_date }}{% endraw %}
          path: |
            {{ data_dir }}/daily/${% raw %}{{ needs.pre-check.outputs.run_date }}{% endraw %}/
            {{ output_filename }}
          retention-days: 5

      {% if run_analysis %}
      # 触发分析工作流
      - name: 触发分析工作流
        if: {% raw %}steps.run_scraper.outputs.file_exists == 'true'{% endraw %}
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: analyzer_{{ site_id }}.yml
          token: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}
          inputs: '{"data_date": "{% raw %}${{ needs.pre-check.outputs.run_date }}{% endraw %}", "data_file": "{{ data_dir }}/daily/{% raw %}${{ needs.pre-check.outputs.run_date }}{% endraw %}/{{ output_filename }}", "site_id": "{{ site_id }}"}'
      {% endif %}

  # 通知作业
  notify:
    name: 发送通知
    needs: [pre-check, crawl]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: 检出代码
        uses: actions/checkout@v4
      
      - name: 设置Python环境
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      
      - name: 安装依赖
        run: |
          pip install -r requirements.txt
      
      - name: 下载爬取数据
        if: {% raw %}needs.crawl.result == 'success'{% endraw %}
        uses: actions/download-artifact@v4
        with:
          name: {{ site_id }}-data-${% raw %}{{ needs.pre-check.outputs.run_date }}{% endraw %}
          path: temp-crawler/
        continue-on-error: true
      
      - name: 准备通知内容并发送
        env:
          DINGTALK_WEBHOOK_URL: {% raw %}${{ secrets.DINGTALK_WEBHOOK }}{% endraw %}
          FEISHU_WEBHOOK_URL: {% raw %}${{ secrets.FEISHU_WEBHOOK }}{% endraw %}
          WECHAT_WORK_WEBHOOK_URL: {% raw %}${{ secrets.WECHAT_WEBHOOK }}{% endraw %}
        run: |
          # 查找爬取结果文件
          CRAWLER_FILE=""
          if [ -d "temp-crawler" ] && [ "$(ls -A temp-crawler)" ]; then
            CRAWLER_FILE=$(find temp-crawler -name "*.json" -o -name "*.csv" -o -name "*.tsv" | head -1)
            echo "找到爬取文件: $CRAWLER_FILE"
          fi
          
          # 如果没有爬取文件，创建一个临时的状态文件
          if [ -z "$CRAWLER_FILE" ] || [ ! -f "$CRAWLER_FILE" ]; then
            mkdir -p temp-crawler
            cat > temp-crawler/crawler_status.json << EOF
          {
            "crawler_status": {
              "pre_check": "{% raw %}${{ needs.pre-check.result }}{% endraw %}",
              "crawl": "{% raw %}${{ needs.crawl.result }}{% endraw %}",
              "site_id": "{{ site_id }}",
              "date": "{% raw %}${{ needs.pre-check.outputs.run_date }}{% endraw %}",
              "run_id": "{% raw %}${{ github.run_id }}{% endraw %}",
              "run_url": "{% raw %}${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}{% endraw %}"
            }
          }
          EOF
            CRAWLER_FILE="temp-crawler/crawler_status.json"
            echo "创建状态文件: $CRAWLER_FILE"
          fi
          
          # 发送通知
          echo "📢 发送爬虫任务通知..."
          python scripts/notify.py --file "$CRAWLER_FILE" --site "{{ site_name }}" 