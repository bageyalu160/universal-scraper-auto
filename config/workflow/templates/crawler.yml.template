name: {{ site_name }} çˆ¬è™«ä»»åŠ¡

# å·¥ä½œæµè¿è¡Œåç§°ï¼ˆåŠ¨æ€ç”Ÿæˆï¼‰
run-name: "ðŸ•¸ï¸ {{ site_name }}çˆ¬è™« #${{ github.run_number }} (${{ github.actor }} è§¦å‘)"

# å¤šç§è§¦å‘æ–¹å¼ç»„åˆ
on:
  # å…è®¸æ‰‹åŠ¨è§¦å‘ï¼Œå¸¦å‚æ•°
  workflow_dispatch:
    inputs:
      date:
        description: 'æ•°æ®æ—¥æœŸ (ç•™ç©ºåˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ)'
        required: false
        type: string
      debug:
        description: 'æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼'
        required: false
        default: false
        type: boolean
      keywords:
        description: 'æœç´¢å…³é”®è¯ (ä»¥é€—å·åˆ†éš”)'
        required: false
        type: string
  
  # å®šæ—¶è§¦å‘
  schedule:
    - cron: "{{ cron_schedule }}"
  
  # ä»£ç å˜æ›´è§¦å‘ - ä»…å½“é…ç½®æ–‡ä»¶æˆ–è„šæœ¬æ”¹å˜æ—¶
  push:
    branches: [ main ]
    paths:
      - 'config/sites/{{ site_id }}.yaml'
      - 'src/scrapers/**'
      - '.github/workflows/crawler_{{ site_id }}.yml'

# å…¨å±€çŽ¯å¢ƒå˜é‡
env:
  PYTHON_VERSION: "{{ python_version }}"
  SITE_ID: "{{ site_id }}"
  TZ: "Asia/Shanghai"
  # ä½¿ç”¨shellå‘½ä»¤å®šä¹‰æ—¥æœŸ
  RUN_DATE: {% raw %}${{ github.event.inputs.date || '' }}{% endraw %}
  # ä»£ç†æ± å’Œåçˆ¬æœºåˆ¶é…ç½®
  USE_PROXY: {% raw %}${{ vars.USE_PROXY || 'true' }}{% endraw %}
  ROTATE_PROXY: {% raw %}${{ vars.ROTATE_PROXY || 'true' }}{% endraw %}
  ANTI_DETECT_ENABLED: {% raw %}${{ vars.ANTI_DETECT_ENABLED || 'true' }}{% endraw %}
  # æ·»åŠ é»‘çŒ«æŠ•è¯‰CookieçŽ¯å¢ƒå˜é‡
  HEIMAO_COOKIE: {% raw %}${{ secrets.HEIMAO_COOKIE }}{% endraw %}

# å®šä¹‰å·¥ä½œæµçš„æƒé™
permissions:
  contents: write  # å…è®¸æŽ¨é€åˆ°ä»“åº“
  actions: write   # å…è®¸è§¦å‘å…¶ä»–å·¥ä½œæµ

# å…¨å±€é»˜è®¤é…ç½®
defaults:
  run:
    shell: bash

# å¹¶å‘æŽ§åˆ¶ - é¿å…ç›¸åŒç«™ç‚¹çš„ä»»åŠ¡å¹¶è¡Œè¿è¡Œ
concurrency:
  group: crawler-{{ site_id }}-{% raw %}${{ github.ref }}{% endraw %}
  cancel-in-progress: true

jobs:
  # é¢„æ£€æŸ¥ä½œä¸š
  pre-check:
    name: çŽ¯å¢ƒä¸Žé…ç½®æ£€æŸ¥
    runs-on: ubuntu-latest
    outputs:
      run_date: {% raw %}${{ steps.prepare_env.outputs.date }}{% endraw %}
      cache_key: {% raw %}${{ steps.prepare_env.outputs.cache_key }}{% endraw %}
      site_config_valid: {% raw %}${{ steps.validate_config.outputs.valid }}{% endraw %}
    
    steps:
      - name: æ£€å‡ºä»£ç 
        uses: actions/checkout@v4
      
      - name: å‡†å¤‡çŽ¯å¢ƒå˜é‡
        id: prepare_env
        run: |
          # è®¾ç½®è¿è¡Œæ—¥æœŸ
          if [ -n "$RUN_DATE" ]; then
            echo "ä½¿ç”¨æŒ‡å®šæ—¥æœŸ: $RUN_DATE"
            echo "date=$RUN_DATE" >> $GITHUB_OUTPUT
          else
            echo "ä½¿ç”¨å½“å‰æ—¥æœŸ"
            echo "date=$(date +%Y-%m-%d)" >> $GITHUB_OUTPUT
          fi
          
          # ç”Ÿæˆç¼“å­˜é”®
          if [ -f "requirements.txt" ]; then
            HASH=$(sha256sum requirements.txt | cut -d ' ' -f 1 | head -c 8)
            echo "cache_key=deps-${{ site_id }}-$HASH-v1" >> $GITHUB_OUTPUT
          else
            echo "cache_key=deps-${{ site_id }}-default-v1" >> $GITHUB_OUTPUT
          fi
      
      - name: éªŒè¯ç«™ç‚¹é…ç½®
        id: validate_config
        run: |
          if [ -f "config/sites/$SITE_ID.yaml" ]; then
            echo "âœ… ç«™ç‚¹é…ç½®æœ‰æ•ˆ"
            echo "valid=true" >> $GITHUB_OUTPUT
          else
            echo "âŒ ç«™ç‚¹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"
            echo "valid=false" >> $GITHUB_OUTPUT
            exit 1
          fi

  # çˆ¬è™«ä½œä¸š - ä½¿ç”¨çŸ©é˜µç­–ç•¥æ”¯æŒå¤šå¼•æ“Ž
  crawl:
    name: è¿è¡Œçˆ¬è™«
    needs: pre-check
    runs-on: ubuntu-latest
    env:
      RUN_DATE: {% raw %}${{ needs.pre-check.outputs.run_date }}{% endraw %}
    
    steps:
      - name: æ£€å‡ºä»£ç 
        uses: actions/checkout@v4
      
      - name: è®¾ç½®PythonçŽ¯å¢ƒ
        uses: actions/setup-python@v5
        with:
          python-version: {% raw %}${{ env.PYTHON_VERSION }}{% endraw %}
          cache: "pip"
          cache-dependency-path: "**/requirements.txt"
      
      - name: å®‰è£…ä¾èµ–
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            echo "å®‰è£…å¿…è¦çš„ä¾èµ–..."
            pip install {{ scraper_dependencies }}
          fi

      # æ–°å¢žæ­¥éª¤ï¼šæ£€æŸ¥ä»£ç†æ± çŠ¶æ€
      - name: åˆ›å»ºä»£ç†æ± ç›®å½•å’Œæ£€æŸ¥çŠ¶æ€
        id: check_proxy_pool
        if: env.USE_PROXY == 'true'
        run: |
          # ç¡®ä¿ä»£ç†æ± ç›®å½•å­˜åœ¨
          mkdir -p data/proxies
          mkdir -p status/proxies
          
          # å¦‚æžœä»£ç†æ± æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„åˆå§‹æ–‡ä»¶
          if [ ! -f "data/proxies/proxy_pool.json" ]; then
            echo '{"proxies": []}' > data/proxies/proxy_pool.json
            echo "åˆ›å»ºäº†åˆå§‹çš„ä»£ç†æ± æ–‡ä»¶"
          fi
          
          if [ -f "status/proxies/pool_status.json" ]; then
            echo "ä»£ç†æ± çŠ¶æ€æ–‡ä»¶å­˜åœ¨"
            # è¯»å–æœ‰æ•ˆä»£ç†æ•°é‡
            valid_count=$(cat status/proxies/pool_status.json | grep -o '"valid_count":[0-9]\+' | cut -d ':' -f 2)
            total_count=$(cat status/proxies/pool_status.json | grep -o '"total_count":[0-9]\+' | cut -d ':' -f 2)
            echo "å½“å‰ä»£ç†æ± çŠ¶æ€: æ€»æ•° $total_count, æœ‰æ•ˆ $valid_count"
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°
            if [ -z "$valid_count" ] || [ "$valid_count" -lt 5 ]; then
              echo "æœ‰æ•ˆä»£ç†ä¸è¶³ï¼Œéœ€è¦æ›´æ–°"
              echo "pool_exists=true" >> $GITHUB_OUTPUT
              echo "needs_update=true" >> $GITHUB_OUTPUT
            else
              echo "ä»£ç†æ± çŠ¶æ€è‰¯å¥½ï¼Œæ— éœ€æ›´æ–°"
              echo "pool_exists=true" >> $GITHUB_OUTPUT
              echo "needs_update=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "ä»£ç†æ± çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦åˆå§‹åŒ–"
            echo "pool_exists=false" >> $GITHUB_OUTPUT
            echo "needs_update=true" >> $GITHUB_OUTPUT
          fi
      
      # æ–°å¢žæ­¥éª¤ï¼šæ›´æ–°ä»£ç†æ± (å¦‚æžœéœ€è¦)
      - name: æ›´æ–°ä»£ç†æ± 
        if: env.USE_PROXY == 'true' && steps.check_proxy_pool.outputs.needs_update == 'true'
        env:
          PROXY_API_KEY: {% raw %}${{ secrets.PROXY_API_KEY }}{% endraw %}
        run: |
          echo "å¼€å§‹æ›´æ–°ä»£ç†æ± ..."
          
          # å°è¯•æ‰§è¡Œæ›´æ–°ï¼Œå¦‚æžœå¤±è´¥åˆ™å°è¯•æ¢å¤å¤‡ä»½
          python scripts/proxy_manager.py update --min-count 10 --timeout 10 || echo "ä»£ç†æ›´æ–°è„šæœ¬æ‰§è¡Œå¤±è´¥"
          
          # æ£€æŸ¥æ›´æ–°ç»“æžœ
          if [ -f "status/proxies/pool_status.json" ]; then
            valid_count=$(cat status/proxies/pool_status.json | grep -o '"valid_count":[0-9]\+' | cut -d ':' -f 2)
            echo "æ›´æ–°åŽçš„æœ‰æ•ˆä»£ç†æ•°: $valid_count"
            
            if [ -n "$valid_count" ] && [ "$valid_count" -gt 0 ]; then
              echo "âœ… ä»£ç†æ± æ›´æ–°æˆåŠŸ"
              # å¤‡ä»½æˆåŠŸçš„ä»£ç†æ± 
              cp data/proxies/proxy_pool.json data/proxies/proxy_pool_backup.json
            else
              echo "âš ï¸ è­¦å‘Šï¼šä»£ç†æ± æ›´æ–°åŽæ— å¯ç”¨ä»£ç†"
              # å¦‚æžœæœ‰å¤‡ä»½ï¼Œå°è¯•æ¢å¤
              if [ -f "data/proxies/proxy_pool_backup.json" ]; then
                echo "å°è¯•æ¢å¤ä»£ç†æ± å¤‡ä»½..."
                cp data/proxies/proxy_pool_backup.json data/proxies/proxy_pool.json
                python scripts/proxy_manager.py validate --timeout 5 || echo "ä»£ç†éªŒè¯å¤±è´¥"
              fi
            fi
          else
            echo "âŒ ä»£ç†æ± æ›´æ–°å¤±è´¥ï¼ŒçŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨"
          fi
      
      # å®‰è£…Playwright (å¦‚éœ€è¦)
      - name: å®‰è£…Playwright (å¦‚éœ€è¦)
        id: check_engine
        run: |
          # è¯»å–ç«™ç‚¹é…ç½®æ–‡ä»¶ä¸­çš„engineå€¼
          ENGINE=$(grep -E "engine:" config/sites/${{ env.SITE_ID }}.yaml | head -1 | awk '{print $2}' | tr -d '"' | tr -d "'")
          echo "engine=$ENGINE" >> $GITHUB_OUTPUT
          
          # å¦‚æžœæ˜¯playwrightæˆ–browserç±»åž‹ï¼Œåˆ™å®‰è£…playwright
          if [[ "$ENGINE" == "playwright" || "$ENGINE" == "browser" ]]; then
            echo "éœ€è¦å®‰è£…Playwright"
            pip install playwright
            playwright install --with-deps chromium
          else
            echo "ä¸éœ€è¦å®‰è£…Playwright"
          fi
      
      - name: åˆ›å»ºæ•°æ®ç›®å½•
        run: |
          mkdir -p {{ data_dir }}/daily/$RUN_DATE
      
      - name: è¿è¡Œçˆ¬è™«
        id: run_scraper
        continue-on-error: true
        env:
{% for env_var in env_vars %}
          {{ env_var.name }}: {% raw %}${{ secrets.{% endraw %}{{ env_var.secret }}{% raw %} }}{% endraw %}
{% endfor %}
          # ä»£ç†æ± ç›¸å…³
          PROXY_API_KEY: {% raw %}${{ secrets.PROXY_API_KEY }}{% endraw %}
          PROXY_SOURCE_URL: {% raw %}${{ secrets.PROXY_SOURCE_URL }}{% endraw %}
          USE_PROXY: {% raw %}${{ env.USE_PROXY }}{% endraw %}
          ROTATE_PROXY: {% raw %}${{ env.ROTATE_PROXY }}{% endraw %}
          
          # åçˆ¬æœºåˆ¶ç›¸å…³
          ANTI_DETECT_ENABLED: {% raw %}${{ env.ANTI_DETECT_ENABLED }}{% endraw %}
          CAPTCHA_API_KEY: {% raw %}${{ secrets.CAPTCHA_API_KEY }}{% endraw %}
          CAPTCHA_SERVICE: {% raw %}${{ vars.CAPTCHA_SERVICE || '2captcha' }}{% endraw %}
          BROWSER_FINGERPRINT: {% raw %}${{ vars.BROWSER_FINGERPRINT || 'true' }}{% endraw %}
          
          # åŽŸæœ‰çŽ¯å¢ƒå˜é‡
          HEIMAO_COOKIE: {% raw %}${{ secrets.HEIMAO_COOKIE }}{% endraw %}
          HEIMAO_KEYWORDS: {% raw %}${{ github.event.inputs.keywords || vars.HEIMAO_KEYWORDS || '' }}{% endraw %}
          ENABLE_NOTIFICATION: {% raw %}${{ vars.ENABLE_NOTIFICATION || 'false' }}{% endraw %}
          NOTIFICATION_TYPE: {% raw %}${{ vars.NOTIFICATION_TYPE || 'none' }}{% endraw %}
          NOTIFICATION_WEBHOOK: {% raw %}${{ vars.NOTIFICATION_WEBHOOK || '' }}{% endraw %}
          DEBUG_MODE: {% raw %}${{ github.event.inputs.debug || 'false' }}{% endraw %}
        run: |
          echo "ðŸ“Œ å¼€å§‹è¿è¡Œçˆ¬è™« (ç«™ç‚¹: $SITE_ID, æ—¥æœŸ: $RUN_DATE)"
          
          if [ "$DEBUG_MODE" = "true" ]; then
            echo "ðŸ” è°ƒè¯•æ¨¡å¼å·²å¯ç”¨"
            export LOG_LEVEL=DEBUG
          fi
          
          # è®°å½•ä»£ç†å’Œåçˆ¬è®¾ç½®
          echo "ðŸ”’ ä»£ç†è®¾ç½®: USE_PROXY=$USE_PROXY, ROTATE_PROXY=$ROTATE_PROXY"
          echo "ðŸ›¡ï¸ åçˆ¬è®¾ç½®: ANTI_DETECT_ENABLED=$ANTI_DETECT_ENABLED, BROWSER_FINGERPRINT=$BROWSER_FINGERPRINT"
          
          # è¿è¡Œçˆ¬è™«è„šæœ¬
          python {{ scraper_script }} --site $SITE_ID --config config/sites/$SITE_ID.yaml --output {{ data_dir }}/daily/$RUN_DATE/
          
          # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
          exit_code=$?
          if [ $exit_code -eq 0 ]; then
            if [ -f "{{ output_filename }}" ]; then
              echo "âœ… çˆ¬è™«æˆåŠŸå®Œæˆï¼Œå·²ç”Ÿæˆæ•°æ®æ–‡ä»¶"
              echo "file_exists=true" >> $GITHUB_OUTPUT
              echo "file_size=$(stat -c%s {{ output_filename }})" >> $GITHUB_OUTPUT
              echo "file_path={{ output_filename }}" >> $GITHUB_OUTPUT
              
              # å¤åˆ¶åˆ°æ—¥æœŸç›®å½•
              cp {{ output_filename }} {{ data_dir }}/daily/$RUN_DATE/
              echo "æ•°æ®æ–‡ä»¶å·²å¤åˆ¶åˆ°æ—¥æœŸç›®å½•"
            else
              echo "âš ï¸ çˆ¬è™«è¿›ç¨‹æˆåŠŸä½†æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶"
              echo "file_exists=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "âŒ çˆ¬è™«è¿›ç¨‹å¤±è´¥ (é€€å‡ºä»£ç : $exit_code)"
            echo "file_exists=false" >> $GITHUB_OUTPUT
          fi
      
      - name: åˆ›å»ºçˆ¬è™«çŠ¶æ€æ–‡ä»¶
        run: |
          mkdir -p {{ status_dir }}
          if [ "{% raw %}${{ steps.run_scraper.outputs.file_exists }}{% endraw %}" == "true" ]; then
            cat > {{ status_dir }}/crawler_$SITE_ID.json << EOF
            {
              "status": "success",
              "site_id": "$SITE_ID",
              "date": "$RUN_DATE",
              "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "file_path": "{{ data_dir }}/daily/$RUN_DATE/{{ output_filename }}",
              "file_size": "{% raw %}${{ steps.run_scraper.outputs.file_size }}{% endraw %}",
              "run_id": "{% raw %}${{ github.run_id }}{% endraw %}",
              "run_url": "{% raw %}${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}{% endraw %}",
              "message": "çˆ¬è™«è¿è¡ŒæˆåŠŸ"
            }
            EOF
          else
            cat > {{ status_dir }}/crawler_$SITE_ID.json << EOF
            {
              "status": "failed",
              "site_id": "$SITE_ID",
              "date": "$RUN_DATE",
              "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "run_id": "{% raw %}${{ github.run_id }}{% endraw %}",
              "run_url": "{% raw %}${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}{% endraw %}",
              "message": "çˆ¬è™«è¿è¡Œå¤±è´¥æˆ–æœªç”Ÿæˆæ–‡ä»¶"
            }
            EOF
          fi
          echo "å·²åˆ›å»ºçˆ¬è™«çŠ¶æ€æ–‡ä»¶"
      
      - name: æäº¤ç»“æžœåˆ°ä»“åº“
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          
          git add {{ data_dir }}/daily/$RUN_DATE/ || echo "æ²¡æœ‰æ•°æ®ç›®å½•å˜æ›´"
          git add {{ output_filename }} || echo "æ²¡æœ‰ä¸»æ•°æ®æ–‡ä»¶"
          git add {{ status_dir }}/crawler_$SITE_ID.json
          
          # æ‹‰å–è¿œç¨‹æ›´æ”¹ï¼Œé¿å…æŽ¨é€å†²çª
          git pull --rebase origin main || echo "æ‹‰å–è¿œç¨‹ä»“åº“å¤±è´¥ï¼Œå°è¯•ç»§ç»­æäº¤"
          
          if git diff --staged --quiet; then
            echo "æ²¡æœ‰å˜æ›´éœ€è¦æäº¤"
          else
            git commit -m "ðŸ¤– è‡ªåŠ¨æ›´æ–°: {{ site_name }}çˆ¬è™«æ•°æ® ($RUN_DATE)"
            git push
            echo "âœ… æˆåŠŸæäº¤å¹¶æŽ¨é€çˆ¬è™«ç»“æžœ"
          fi
      
      - name: ä¸Šä¼ çˆ¬å–æ•°æ® (ä½œä¸ºå·¥ä»¶)
        if: {% raw %}steps.run_scraper.outputs.file_exists == 'true'{% endraw %}
        uses: actions/upload-artifact@v4
        with:
          name: {{ site_id }}-data-${% raw %}{{ needs.pre-check.outputs.run_date }}{% endraw %}
          path: |
            {{ data_dir }}/daily/${% raw %}{{ needs.pre-check.outputs.run_date }}{% endraw %}/
            {{ output_filename }}
          retention-days: 5

      {% if run_analysis %}
      # è§¦å‘åˆ†æžå·¥ä½œæµ
      - name: è§¦å‘åˆ†æžå·¥ä½œæµ
        if: {% raw %}steps.run_scraper.outputs.file_exists == 'true'{% endraw %}
        uses: benc-uk/workflow-dispatch@v1
        continue-on-error: true
        with:
          workflow: analyzer_{{ site_id }}.yml
          token: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}
          inputs: '{"data_date": "{% raw %}${{ needs.pre-check.outputs.run_date }}{% endraw %}", "data_file": "{{ data_dir }}/daily/{% raw %}${{ needs.pre-check.outputs.run_date }}{% endraw %}/{{ output_filename }}", "site_id": "{{ site_id }}"}'
      {% endif %}

  # é€šçŸ¥ä½œä¸š
  notify:
    name: å‘é€é€šçŸ¥
    needs: [pre-check, crawl]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: æ£€å‡ºä»£ç 
        uses: actions/checkout@v4
      
      - name: è®¾ç½®PythonçŽ¯å¢ƒ
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      
      - name: å®‰è£…ä¾èµ–
        run: |
          pip install -r requirements.txt
      
      - name: ä¸‹è½½çˆ¬å–æ•°æ®
        if: {% raw %}needs.crawl.result == 'success'{% endraw %}
        uses: actions/download-artifact@v4
        with:
          name: {{ site_id }}-data-${% raw %}{{ needs.pre-check.outputs.run_date }}{% endraw %}
          path: temp-crawler/
        continue-on-error: true
        
      - name: è§¦å‘ä»ªè¡¨æ¿æ›´æ–°
        if: {% raw %}always(){% endraw %}
        uses: benc-uk/workflow-dispatch@v1
        continue-on-error: true
        with:
          workflow: update_dashboard.yml
          token: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}
      
      - name: å‡†å¤‡é€šçŸ¥å†…å®¹å¹¶å‘é€
        env:
          DINGTALK_WEBHOOK_URL: {% raw %}${{ secrets.DINGTALK_WEBHOOK }}{% endraw %}
          FEISHU_WEBHOOK_URL: {% raw %}${{ secrets.FEISHU_WEBHOOK }}{% endraw %}
          WECHAT_WORK_WEBHOOK_URL: {% raw %}${{ secrets.WECHAT_WEBHOOK }}{% endraw %}
        run: |
          # æŸ¥æ‰¾çˆ¬å–ç»“æžœæ–‡ä»¶
          CRAWLER_FILE=""
          if [ -d "temp-crawler" ] && [ "$(ls -A temp-crawler)" ]; then
            CRAWLER_FILE=$(find temp-crawler -name "*.json" -o -name "*.csv" -o -name "*.tsv" | head -1)
            echo "æ‰¾åˆ°çˆ¬å–æ–‡ä»¶: $CRAWLER_FILE"
          fi
          
          # å¦‚æžœæ²¡æœ‰çˆ¬å–æ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„çŠ¶æ€æ–‡ä»¶
          if [ -z "$CRAWLER_FILE" ] || [ ! -f "$CRAWLER_FILE" ]; then
            mkdir -p temp-crawler
            cat > temp-crawler/crawler_status.json << EOF
          {
            "crawler_status": {
              "pre_check": "{% raw %}${{ needs.pre-check.result }}{% endraw %}",
              "crawl": "{% raw %}${{ needs.crawl.result }}{% endraw %}",
              "site_id": "{{ site_id }}",
              "date": "{% raw %}${{ needs.pre-check.outputs.run_date }}{% endraw %}",
              "run_id": "{% raw %}${{ github.run_id }}{% endraw %}",
              "run_url": "{% raw %}${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}{% endraw %}"
            }
          }
          EOF
            CRAWLER_FILE="temp-crawler/crawler_status.json"
            echo "åˆ›å»ºçŠ¶æ€æ–‡ä»¶: $CRAWLER_FILE"
          fi
          
          # å‘é€é€šçŸ¥
          echo "ðŸ“¢ å‘é€çˆ¬è™«ä»»åŠ¡é€šçŸ¥..."
          python scripts/notify.py --file "$CRAWLER_FILE" --site "{{ site_name }}" 