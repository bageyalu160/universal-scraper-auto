{
  // Jsonnetå·¥å…·åº“ - ä¸ºå¢å¼ºç‰ˆå·¥ä½œæµæ¨¡æ¿æä¾›é€šç”¨å‡½æ•°
  
  // è·å–ç«™ç‚¹åç§°
  getSiteName(site_config, site_id)::
    if std.objectHas(site_config, 'site_info') && std.objectHas(site_config.site_info, 'name') then
      site_config.site_info.name
    else if std.objectHas(site_config, 'site') && std.objectHas(site_config.site, 'name') then
      site_config.site.name
    else
      site_id,
  
  // è·å–é…ç½®éƒ¨åˆ†ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›é»˜è®¤å€¼
  getConfigSection(config, section_name, default_value={})::  
    if std.objectHas(config, section_name) then
      config[section_name]
    else
      default_value,

  // è·å–é…ç½®å€¼ï¼Œæ”¯æŒåµŒå¥—è·¯å¾„ï¼ˆå¦‚ "scraping.engine"ï¼‰
  getConfigValue(config, path, default_value)::  
    local parts = std.split(path, '.');
    local navigate(obj, idx) =
      if idx >= std.length(parts) then obj
      else if std.objectHas(obj, parts[idx]) then navigate(obj[parts[idx]], idx + 1)
      else default_value;
    navigate(config, 0),

  // è·å–åˆ†æé…ç½®
  getAnalysisConfig(site_config)::
    $.getConfigSection(site_config, 'analysis', {}),
  
  // è·å–è¾“å‡ºæ‰©å±•å
  getOutputExtension(analysis_config)::
    if std.objectHas(analysis_config, 'output_format') then
      if analysis_config.output_format == 'json' then 'json'
      else if analysis_config.output_format == 'tsv' then 'tsv'
      else if analysis_config.output_format == 'csv' then 'csv'
      else 'json'
    else
      'tsv',
  
  // æ ¹æ®å¼•æ“ç±»å‹è·å–çˆ¬è™«ä¾èµ–é¡¹
  getCrawlerDependencies(engine_type, dependencies_config)::  
    if engine_type == 'firecrawl' then
      dependencies_config.crawler.firecrawl
    else if engine_type == 'playwright' then
      dependencies_config.crawler.playwright
    else if engine_type == 'selenium' then
      dependencies_config.crawler.selenium
    else
      dependencies_config.crawler.requests,

  // æ ¹æ®AIæä¾›å•†è·å–åˆ†æå™¨ä¾èµ–é¡¹
  getAnalyzerDependencies(ai_provider, dependencies_config)::  
    dependencies_config.analyzer + 
    if ai_provider == 'openai' then
      ['openai>=1.0.0']
    else if ai_provider == 'gemini' then
      ['google-generativeai>=0.3.1']
    else
      [],

  // è·å–ç¯å¢ƒå˜é‡é…ç½®
  getEnvironmentVariables(analysis_config, global_config):: [
    { name: 'OPENAI_API_KEY', secret: 'OPENAI_API_KEY' },
    { name: 'GEMINI_API_KEY', secret: 'GEMINI_API_KEY' },
    { name: 'ANTHROPIC_API_KEY', secret: 'ANTHROPIC_API_KEY' }
  ],
  
  // è·å–ä½œä¸šè¶…æ—¶è®¾ç½®
  getJobTimeout(job_type, global_config)::  
    local base_timeout = if std.objectHas(global_config, 'runtime') && std.objectHas(global_config.runtime, 'timeout_minutes') 
                       then global_config.runtime.timeout_minutes 
                       else 30;
    
    // æ ¹æ®ä½œä¸šç±»å‹è¿”å›é€‚å½“çš„è¶…æ—¶æ—¶é—´
    if job_type == 'setup' then std.ceil(base_timeout / 3)
    else if job_type == 'crawl' then std.ceil(base_timeout * 2)
    else if job_type == 'analyze' then base_timeout
    else if job_type == 'proxy_pool' then std.ceil(base_timeout / 1.5)
    else if job_type == 'dashboard' then std.ceil(base_timeout / 2)
    else base_timeout,
  
  // è·å–é”™è¯¯å¤„ç†ç­–ç•¥
  getErrorHandlingStrategy(step_type, global_config)::  
    // é»˜è®¤ç­–ç•¥
    local default_strategy = {
      'continue-on-error': false,
      'fail-fast': true
    };
    
    // å¯ä»¥å®¹å¿é”™è¯¯çš„æ­¥éª¤ç±»å‹
    local tolerant_steps = [
      'crawl', 'analyze', 'notification', 'proxy_validation', 'data_processing'
    ];
    
    // å¦‚æœæ˜¯å¯å®¹é”™æ­¥éª¤ï¼Œåˆ™å…è®¸ç»§ç»­æ‰§è¡Œ
    if std.member(tolerant_steps, step_type) then
      default_strategy + { 'continue-on-error': true, 'fail-fast': false }
    else
      default_strategy,
  
  // ç”Ÿæˆç¼“å­˜é…ç½®
  generateCacheConfig(workflow_type, id, engine='')::  
    local prefix = workflow_type + '-deps-' + id;
    local key_prefix = if engine != '' then prefix + '-' + engine else prefix;
    local paths = ['~/.cache/pip'];
    
    if workflow_type == 'crawler' then
      {
        enabled: true,
        key_prefix: key_prefix,
        paths: paths + ['data/' + id + '/cache']
      }
    else if workflow_type == 'analyzer' then
      {
        enabled: true,
        key_prefix: key_prefix,
        paths: paths + ['analysis/' + id + '/cache']
      }
    else if workflow_type == 'proxy_pool' then
      {
        enabled: true,
        key_prefix: key_prefix,
        paths: paths + ['proxy_pool/cache']
      }
    else if workflow_type == 'dashboard' then
      {
        enabled: true,
        key_prefix: key_prefix,
        paths: paths + ['dashboard/cache']
      }
    else
      {
        enabled: true,
        key_prefix: key_prefix,
        paths: paths
      },

  // ç”Ÿæˆå·¥ä½œæµç¯å¢ƒå˜é‡
  generateWorkflowEnv(workflow_type, global_config)::  
    // åŸºç¡€ç¯å¢ƒå˜é‡
    local paths = if std.objectHas(global_config, 'paths') then global_config.paths else {
      config: 'config',
      config_sites: 'config/sites',
      data_daily: 'data/daily',
      status: 'status',
      logs: 'logs'
    };
    
    local base_env = {
      CONFIG_DIR: paths.config,
      SITES_DIR: paths.config_sites,
      DATA_DIR: paths.data_daily,
      STATUS_DIR: paths.status,
      LOGS_DIR: paths.logs,
      PYTHONUNBUFFERED: '1',
      WORKFLOW_TYPE: workflow_type
    };
    
    // æ ¹æ®å·¥ä½œæµç±»å‹æ·»åŠ ç‰¹å®šç¯å¢ƒå˜é‡
    local analysis = if std.objectHas(global_config, 'analysis') then global_config.analysis else {
      provider: 'gemini',
      prompt_dir: 'config/analysis/prompts'
    };
    
    local specific_env = 
      if workflow_type == 'analyzer' then {
        AI_PROVIDER: analysis.provider,
        PROMPT_DIR: analysis.prompt_dir
      }
      else if workflow_type == 'crawler' then {
        PROXY_ENABLED: std.toString(global_config.proxy.enabled)
      }
      else if workflow_type == 'proxy_pool' then {
        PROXY_TIMEOUT: std.toString(30)
      }
      else {};
    
    base_env + specific_env,
  
  // ç”Ÿæˆæ£€å‡ºä»£ç æ­¥éª¤
  generateCheckoutStep(fetch_depth=1)::  
    {
      name: 'æ£€å‡ºä»£ç ',
      uses: 'actions/checkout@v4',
      [if fetch_depth != 1 then 'with']: {
        'fetch-depth': fetch_depth
      }
    },

  // ç”Ÿæˆè®¾ç½®Pythonæ­¥éª¤
  generatePythonSetupStep(python_version, cache_pip=true)::  
    {
      name: 'è®¾ç½®Python',
      uses: 'actions/setup-python@v5',
      with: {
        'python-version': python_version,
        cache: if cache_pip then 'pip' else null
      }
    },

  // ç”Ÿæˆç¼“å­˜æ­¥éª¤
  generateCacheStep(cache_config, hashfiles_pattern)::  
    {
      name: 'ç¼“å­˜ä¾èµ–å’Œæ•°æ®',
      [if cache_config.enabled then 'if']: 'true',
      uses: 'actions/cache@v3',
      with: {
        path: std.join('\n', cache_config.paths),
        key: cache_config.key_prefix + '-${{ hashFiles(\'' + hashfiles_pattern + '\') }}',
        'restore-keys': cache_config.key_prefix + '-'
      }
    },

  // ç”Ÿæˆåˆ›å»ºç›®å½•æ­¥éª¤
  generateDirectorySetupStep(directories)::  
    {
      name: 'åˆ›å»ºå¿…è¦ç›®å½•',
      run: std.join('\n', ['mkdir -p ' + dir for dir in directories])
    },

  // æ„å»ºè§¦å‘æ¡ä»¶
  buildTriggers():: {
    workflow_dispatch: {
      inputs: {
        data_date: {
          description: 'æ•°æ®æ—¥æœŸ (YYYY-MM-DDæ ¼å¼)',
          required: true,
          type: 'string'
        },
        data_file: {
          description: 'è¦åˆ†æçš„æ•°æ®æ–‡ä»¶è·¯å¾„',
          required: true,
          type: 'string'
        },
        site_id: {
          description: 'ç½‘ç«™ID',
          required: true,
          type: 'string'
        },
        model: {
          description: 'AIæ¨¡å‹é€‰æ‹©',
          required: false,
          type: 'choice',
          default: 'default',
          options: ['default', 'gemini-1.5-pro', 'gpt-4-turbo', 'claude-3-sonnet']
        }
      }
    },
    repository_dispatch: {
      types: ['crawler_completed']
    },
    workflow_call: {
      inputs: {
        data_date: {
          description: 'æ•°æ®æ—¥æœŸ',
          required: true,
          type: 'string'
        },
        data_file: {
          description: 'æ•°æ®æ–‡ä»¶è·¯å¾„',
          required: true,
          type: 'string'
        },
        site_id: {
          description: 'ç½‘ç«™ID',
          required: true,
          type: 'string'
        }
      },
      secrets: {
        API_KEY: {
          required: false
        }
      }
    }
  },
  
  // æ„å»ºå…¨å±€ç¯å¢ƒå˜é‡
  buildGlobalEnv(site_id, analysis_dir):: {
    PYTHON_VERSION: '3.10',
    ANALYSIS_DIR: analysis_dir + '/daily',
    SITE_ID: site_id,
    TZ: 'Asia/Shanghai'
  },
  
  // æ„å»ºæƒé™è®¾ç½®
  buildPermissions():: {
    contents: 'write',
    actions: 'write'
  },
  
  // æ„å»ºé»˜è®¤é…ç½®
  buildDefaults():: {
    run: {
      shell: 'bash'
    }
  },
  
  // ç”ŸæˆGitæäº¤æ­¥éª¤
  generateGitCommitStep(paths_to_add, commit_message)::  
    local paths_array = if std.type(paths_to_add) == 'array' then paths_to_add else [paths_to_add];
    {
      name: 'æäº¤æ›´æ”¹',
      run: |||
        # é…ç½®Git
        git config user.name "github-actions[bot]"
        git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

        # æ·»åŠ æ–‡ä»¶
        %(add_commands)s

        # æ‹‰å–è¿œç¨‹æ›´æ”¹ï¼Œé¿å…æ¨é€å†²çª
        git pull --rebase origin main || echo "æ‹‰å–è¿œç¨‹ä»“åº“å¤±è´¥ï¼Œå°è¯•ç»§ç»­æäº¤"

        # æäº¤æ›´æ”¹
        if git diff --staged --quiet; then
          echo "æ²¡æœ‰å˜æ›´éœ€è¦æäº¤"
        else
          git commit -m "%(commit_message)s"
          git push
          echo "âœ… æˆåŠŸæäº¤å¹¶æ¨é€æ›´æ”¹"
        fi
      ||| % {
        add_commands: std.join('\n', ['git add ' + std.strReplace(path, '"', '') for path in paths_array]),
        commit_message: commit_message
      }
    },

  // ç”Ÿæˆå·¥ä½œæµæ‰‹åŠ¨è§¦å‘é…ç½®
  generateWorkflowDispatchTrigger(inputs)::  
    {
      workflow_dispatch: {
        inputs: inputs
      }
    },

  // ç”Ÿæˆå®šæ—¶è§¦å‘é…ç½®
  generateScheduleTrigger(cron_expression)::  
    {
      schedule: [
        {cron: cron_expression}
      ]
    },

  // ç”Ÿæˆå¹¶å‘æ§åˆ¶é…ç½®
  generateConcurrencyConfig(workflow_type, id)::  
    {
      group: workflow_type + '-' + id + '-${{ github.ref }}',
      'cancel-in-progress': true
    },

  // æ„å»ºå¹¶å‘æ§åˆ¶
  buildConcurrency(site_id):: {
    group: 'analyzer-' + site_id + '-${{ github.ref }}',
    'cancel-in-progress': true
  },
  
  // æ„å»ºé¢„æ£€æŸ¥ä½œä¸š
  buildPreCheckJob():: {
    name: 'å‡†å¤‡åˆ†æç¯å¢ƒ',
    'runs-on': 'ubuntu-latest',
    outputs: {
      data_date: '${{ steps.params.outputs.data_date }}',
      data_file: '${{ steps.params.outputs.data_file }}',
      site_id: '${{ steps.params.outputs.site_id }}',
      analysis_dir: '${{ steps.params.outputs.analysis_dir }}',
      cache_key: '${{ steps.params.outputs.cache_key }}'
    },
    steps: [
      {
        name: 'æ£€å‡ºä»£ç ',
        uses: 'actions/checkout@v4'
      },
      {
        name: 'ç¡®å®šåˆ†æå‚æ•°',
        id: 'params',
        run: |||
          # ä»å‚æ•°ä¸­è·å–æ•°æ®æ—¥æœŸå’Œæ–‡ä»¶è·¯å¾„
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            DATA_DATE="${{ github.event.inputs.data_date }}"
            DATA_FILE="${{ github.event.inputs.data_file }}"
            SITE_ID="${{ github.event.inputs.site_id }}"
          elif [ "${{ github.event_name }}" == "repository_dispatch" ]; then
            DATA_DATE="${{ github.event.client_payload.data_date }}"
            DATA_FILE="${{ github.event.client_payload.data_file }}"
            SITE_ID="${{ github.event.client_payload.site_id }}"
          elif [ "${{ github.event_name }}" == "workflow_call" ]; then
            DATA_DATE="${{ inputs.data_date }}"
            DATA_FILE="${{ inputs.data_file }}"
            SITE_ID="${{ inputs.site_id }}"
          else
            # ä»çŠ¶æ€æ–‡ä»¶è·å–æœ€æ–°æ•°æ®
            if [ -f "status/crawler_status.json" ]; then
              DATA_DATE=$(jq -r '.date' status/crawler_status.json)
              DATA_FILE=$(jq -r '.file_path' status/crawler_status.json)
              SITE_ID="${{ env.SITE_ID }}"
            else
              echo "âŒ é”™è¯¯: æ— æ³•ç¡®å®šæ•°æ®æ—¥æœŸå’Œæ–‡ä»¶è·¯å¾„"
              exit 1
            fi
          fi
          
          # ç¡®ä¿æ—¥æœŸç›®å½•å­˜åœ¨
          mkdir -p "${ANALYSIS_DIR}/${DATA_DATE}"
          
          # è®¾ç½®è¾“å‡ºå‚æ•°
          echo "data_date=${DATA_DATE}" >> $GITHUB_OUTPUT
          echo "data_file=${DATA_FILE}" >> $GITHUB_OUTPUT
          echo "site_id=${SITE_ID}" >> $GITHUB_OUTPUT
          echo "analysis_dir=${ANALYSIS_DIR}/${DATA_DATE}" >> $GITHUB_OUTPUT
          
          # ç”Ÿæˆç¼“å­˜é”®
          if [ -f "requirements.txt" ]; then
            HASH=$(sha256sum requirements.txt | cut -d ' ' -f 1 | head -c 8)
            echo "cache_key=deps-analysis-$HASH-v1" >> $GITHUB_OUTPUT
          else
            echo "cache_key=deps-analysis-default-v1" >> $GITHUB_OUTPUT
          fi
          
          echo "ğŸ“Œ è®¾ç½®åˆ†æå‚æ•°: æ—¥æœŸ=${DATA_DATE}, æ–‡ä»¶=${DATA_FILE}, ç«™ç‚¹=${SITE_ID}"
        |||
      },
      {
        name: 'æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨',
        run: |||
          if [ ! -f "${{ steps.params.outputs.data_file }}" ]; then
            echo "âŒ é”™è¯¯: æ•°æ®æ–‡ä»¶ ${{ steps.params.outputs.data_file }} ä¸å­˜åœ¨"
            exit 1
          else
            echo "âœ… æ•°æ®æ–‡ä»¶å­˜åœ¨ï¼Œå‡†å¤‡åˆ†æ"
          fi
        |||
      }
    ]
  },
  
  // æ„å»ºåˆ†æä½œä¸š
  buildAnalyzeJob(site_config, global_config):: {
    local analysis_config = $.getAnalysisConfig(site_config),
    local output_extension = $.getOutputExtension(analysis_config),
    local env_vars = $.getEnvironmentVariables(analysis_config, global_config),
    local send_notification = if std.objectHas(global_config, 'notification') && std.objectHas(global_config.notification, 'enabled') then
      global_config.notification.enabled
    else
      false,
    local analyzer_script = if std.objectHas(site_config, 'analysis') && std.objectHas(site_config.analysis, 'script') then
      site_config.analysis.script
    else
      'scripts/ai_analyzer.py',
    local notification_script = if std.objectHas(global_config, 'notification') && std.objectHas(global_config.notification, 'script') then
      global_config.notification.script
    else
      'scripts/notifier.py',
    
    name: 'æ‰§è¡ŒAIåˆ†æ',
    needs: 'pre-check',
    'runs-on': 'ubuntu-latest',
    'timeout-minutes': 30,
    env: {
      DATA_DATE: '${{ needs.pre-check.outputs.data_date }}',
      DATA_FILE: '${{ needs.pre-check.outputs.data_file }}',
      SITE_ID: '${{ needs.pre-check.outputs.site_id }}',
      ANALYSIS_DIR: '${{ needs.pre-check.outputs.analysis_dir }}'
    },
    steps: [
      {
        name: 'æ£€å‡ºä»£ç ',
        uses: 'actions/checkout@v4'
      },
      {
        name: 'è®¾ç½®Pythonç¯å¢ƒ',
        uses: 'actions/setup-python@v5',
        with: {
          'python-version': '${{ env.PYTHON_VERSION }}',
          cache: 'pip',
          'cache-dependency-path': '**/requirements.txt'
        }
      },
      {
        name: 'å®‰è£…ä¾èµ–',
        run: |||
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            echo "å®‰è£…å¿…è¦çš„ä¾èµ–..."
            pip install requests pyyaml pandas openai google-generativeai
          fi
        |||
      },
      {
        name: 'è¿è¡ŒAIåˆ†æ',
        id: 'run_analysis',
        'continue-on-error': true,
        env: {
          [env_var.name]: '${{ secrets.' + env_var.secret + ' }}'
          for env_var in env_vars
        } + {
          MODEL: '${{ github.event.inputs.model || \'default\' }}',
          HEIMAO_KEYWORDS: '${{ vars.HEIMAO_KEYWORDS || \'\' }}',
          ENABLE_NOTIFICATION: '${{ vars.ENABLE_NOTIFICATION || \'false\' }}',
          NOTIFICATION_TYPE: '${{ vars.NOTIFICATION_TYPE || \'none\' }}',
          NOTIFICATION_WEBHOOK: '${{ vars.NOTIFICATION_WEBHOOK || \'\' }}',
          GITHUB_REPOSITORY: '${{ github.repository }}'
        },
        run: |||
          echo "ğŸ“Š å¼€å§‹åˆ†ææ•°æ®æ–‡ä»¶: $DATA_FILE"
          
          # æŒ‡å®šæ¨¡å‹é…ç½®
          if [ "$MODEL" != "default" ]; then
            MODEL_ARG="--model $MODEL"
            echo "ğŸ§  ä½¿ç”¨æŒ‡å®šæ¨¡å‹: $MODEL"
          else
            MODEL_ARG=""
            echo "ğŸ§  ä½¿ç”¨é»˜è®¤æ¨¡å‹"
          fi
          
          # è¿è¡ŒAIåˆ†æè„šæœ¬
          python %(analyzer_script)s --file "$DATA_FILE" --site "$SITE_ID" --output "$ANALYSIS_DIR/analysis_$DATA_DATE.%(output_extension)s" $MODEL_ARG
          
          # æ£€æŸ¥åˆ†æç»“æœæ–‡ä»¶
          if [ -f "$ANALYSIS_DIR/analysis_$DATA_DATE.%(output_extension)s" ]; then
            echo "âœ… åˆ†ææˆåŠŸå®Œæˆï¼Œå·²ç”Ÿæˆç»“æœæ–‡ä»¶"
            echo "analysis_exists=true" >> $GITHUB_OUTPUT
            echo "analysis_file=$ANALYSIS_DIR/analysis_$DATA_DATE.%(output_extension)s" >> $GITHUB_OUTPUT
          else
            echo "âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°åˆ†æç»“æœæ–‡ä»¶"
            echo "analysis_exists=false" >> $GITHUB_OUTPUT
          fi
        ||| % {
          analyzer_script: analyzer_script,
          output_extension: output_extension
        }
      },
      {
        name: 'åˆ›å»ºåˆ†æçŠ¶æ€æ–‡ä»¶',
        run: |||
          mkdir -p status
          # åˆ›å»ºçŠ¶æ€æ–‡ä»¶
          if [ "${{ steps.run_analysis.outcome }}" == "success" ] && [ "${{ steps.run_analysis.outputs.analysis_exists }}" == "true" ]; then
            cat > status/analyzer_$SITE_ID.json << EOF
            {
              "status": "success",
              "site_id": "$SITE_ID",
              "date": "$DATA_DATE",
              "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "data_file": "$DATA_FILE",
              "analysis_file": "$ANALYSIS_DIR/analysis_$DATA_DATE.%(output_extension)s",
              "run_id": "${{ github.run_id }}",
              "run_url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}",
              "message": "æ•°æ®åˆ†ææˆåŠŸå®Œæˆ"
            }
          EOF
          else
            cat > status/analyzer_$SITE_ID.json << EOF
            {
              "status": "failed",
              "site_id": "$SITE_ID",
              "date": "$DATA_DATE",
              "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "data_file": "$DATA_FILE",
              "run_id": "${{ github.run_id }}",
              "run_url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}",
              "message": "æ•°æ®åˆ†æå¤±è´¥æˆ–æ— ç»“æœ"
            }
          EOF
          fi
          echo "å·²åˆ›å»ºåˆ†æçŠ¶æ€æ–‡ä»¶"
        ||| % {
          output_extension: output_extension
        }
      },
      {
        name: 'æäº¤åˆ†æç»“æœå’ŒçŠ¶æ€',
        run: |||
          echo "æ­£åœ¨æäº¤åˆ†æç»“æœå’ŒçŠ¶æ€..."
          # è®¾ç½®gité…ç½®
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          
          # æ·»åŠ éœ€è¦æäº¤çš„æ–‡ä»¶
          if [ -f "$ANALYSIS_DIR/analysis_$DATA_DATE.%(output_extension)s" ]; then
            git add "$ANALYSIS_DIR/" || echo "æ²¡æœ‰åˆ†æç›®å½•å˜æ›´"
          fi
          git add status/analyzer_$SITE_ID.json
          
          # æ£€æŸ¥æ˜¯å¦æœ‰å˜æ›´éœ€è¦æäº¤
          if git diff --staged --quiet; then
            echo "æ²¡æœ‰å˜æ›´éœ€è¦æäº¤"
          else
            # åˆ›å»ºæäº¤
            git commit -m "ğŸ¤– è‡ªåŠ¨æ›´æ–°: %(site_name)såˆ†æç»“æœ ($DATA_DATE)"
            # æ¨é€åˆ°ä»“åº“
            git push
            echo "âœ… æˆåŠŸæäº¤å¹¶æ¨é€åˆ†æç»“æœå’ŒçŠ¶æ€"
          fi
        ||| % {
          output_extension: output_extension,
          site_name: $.getSiteName(site_config, std.extVar('site_id'))
        }
      },
      {
        name: 'ä¸Šä¼ åˆ†æç»“æœ (ä½œä¸ºå·¥ä»¶)',
        'if': 'steps.run_analysis.outputs.analysis_exists == \'true\'',
        uses: 'actions/upload-artifact@v4',
        with: {
          name: std.extVar('site_id') + '-analysis-${{ needs.pre-check.outputs.data_date }}',
          path: '${{ steps.run_analysis.outputs.analysis_file }}',
          'retention-days': 5
        }
      }
    ] + (
      if send_notification then [
        {
          name: 'å‘é€åˆ†æç»“æœé€šçŸ¥',
          'if': 'steps.run_analysis.outputs.analysis_exists == \'true\'',
          env: {
            ENABLE_NOTIFICATION: '${{ vars.ENABLE_NOTIFICATION || \'false\' }}',
            NOTIFICATION_TYPE: '${{ vars.NOTIFICATION_TYPE || \'none\' }}',
            NOTIFICATION_WEBHOOK: '${{ vars.NOTIFICATION_WEBHOOK || \'\' }}',
            ANALYSIS_FILE: '${{ steps.run_analysis.outputs.analysis_file }}'
          },
          run: |||
            if [ "$ENABLE_NOTIFICATION" = "true" ] && [ -n "$NOTIFICATION_WEBHOOK" ]; then
              echo "ğŸ“¢ å‘é€åˆ†æç»“æœé€šçŸ¥..."
              python %(notification_script)s --file "$ANALYSIS_FILE" --site "$SITE_ID"
            else
              echo "â„¹ï¸ æœªå¯ç”¨é€šçŸ¥æˆ–ç¼ºå°‘é…ç½®ï¼Œè·³è¿‡é€šçŸ¥å‘é€"
            fi
          ||| % {
            notification_script: notification_script
          }
        }
      ] else []
    ) + [
      {
        name: 'è§¦å‘ä»ªè¡¨ç›˜æ›´æ–°å·¥ä½œæµ',
        'if': 'steps.run_analysis.outputs.analysis_exists == \'true\'',
        uses: 'benc-uk/workflow-dispatch@v1',
        with: {
          workflow: 'update_dashboard.yml',
          token: '${{ secrets.GITHUB_TOKEN }}',
          inputs: '{"site_id": "${{ needs.pre-check.outputs.site_id }}"}'
        }
      }
    ]
  },
  
  // ç”Ÿæˆå·¥ä½œæµçŠ¶æ€æŠ¥å‘Šæ­¥éª¤
  generateWorkflowStatusStep(workflow_type, site_id, parent_param='parent_workflow_id'):: {
    name: 'æŠ¥å‘Šå·¥ä½œæµçŠ¶æ€',
    'if': 'always()',
    run: |||
      # åˆ›å»ºçŠ¶æ€ç›®å½•
      mkdir -p status/workflow
      
      # ç”ŸæˆçŠ¶æ€æ–‡ä»¶
      cat > status/workflow/%(workflow_type)s_%(site_id)s.json << EOF
      {
        "workflow_id": "${{ github.run_id }}",
        "parent_workflow_id": "${{ github.event.inputs.%(parent_param)s || '' }}",
        "workflow_type": "%(workflow_type)s",
        "site_id": "%(site_id)s",
        "status": "${{ job.status }}",
        "timestamp": "$(date -u +%%Y-%%m-%%dT%%H:%%M:%%SZ)",
        "data_date": "${{ needs.pre-check.outputs.run_date }}",
        "artifacts": [
          "%(site_id)s-data-${{ needs.pre-check.outputs.run_date }}",
          "%(site_id)s-status-${{ needs.pre-check.outputs.run_date }}"
        ],
        "run_url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
      }
      EOF
      
      # å¦‚æœæ˜¯ç”±ä¸»å·¥ä½œæµè§¦å‘çš„ï¼Œåˆ™æ¨é€çŠ¶æ€
      if [ -n "${{ github.event.inputs.%(parent_param)s }}" ]; then
        # é…ç½®Git
        git config user.name "github-actions[bot]"
        git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
        
        # æ·»åŠ çŠ¶æ€æ–‡ä»¶
        git add status/workflow/%(workflow_type)s_%(site_id)s.json
        
        # æ‹‰å–è¿œç¨‹æ›´æ”¹ï¼Œé¿å…æ¨é€å†²çª
        git pull --rebase origin main || echo "æ‹‰å–è¿œç¨‹ä»“åº“å¤±è´¥ï¼Œå°è¯•ç»§ç»­æäº¤"
        
        # æäº¤æ›´æ”¹
        if ! git diff --staged --quiet; then
          git commit -m "ğŸ“Š å·¥ä½œæµçŠ¶æ€æ›´æ–°: %(workflow_type)s_%(site_id)s (${{ needs.pre-check.outputs.run_date }})"
          git push
        fi
      fi
    ||| % {
      workflow_type: workflow_type,
      site_id: site_id,
      parent_param: parent_param
    }
  },

  // ç”Ÿæˆå­å·¥ä½œæµçŠ¶æ€æ£€æŸ¥æ­¥éª¤
  generateWorkflowStatusCheckStep(workflow_type, id, timeout_seconds=300)::  
    {
      name: 'æ£€æŸ¥' + workflow_type + '_' + id + 'çŠ¶æ€',
      id: 'check_' + workflow_type + '_' + id + '_status',
      run: |||
        # è®¾ç½®è¶…æ—¶æ—¶é—´
        TIMEOUT_SECONDS=%(timeout_seconds)d
        START_TIME=$(date +%%s)
        END_TIME=$((START_TIME + TIMEOUT_SECONDS))
        
        echo "å¼€å§‹æ£€æŸ¥å·¥ä½œæµçŠ¶æ€ï¼Œæœ€å¤§ç­‰å¾…æ—¶é—´: $TIMEOUT_SECONDS ç§’"
        
        # å¾ªç¯æ£€æŸ¥çŠ¶æ€æ–‡ä»¶
        while true; do
          CURRENT_TIME=$(date +%%s)
          
          # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
          if [ $CURRENT_TIME -gt $END_TIME ]; then
            echo "::warning::ç­‰å¾…å·¥ä½œæµçŠ¶æ€è¶…æ—¶"
            echo "status=timeout" >> $GITHUB_OUTPUT
            break
          fi
          
          # æ£€æŸ¥çŠ¶æ€æ–‡ä»¶æ˜¯å¦å­˜åœ¨
          if [ -f "status/workflow/%(workflow_type)s_%(id)s.json" ]; then
            # è¯»å–çŠ¶æ€
            STATUS=$(jq -r '.status' status/workflow/%(workflow_type)s_%(id)s.json)
            echo "å½“å‰çŠ¶æ€: $STATUS"
            
            # å¦‚æœçŠ¶æ€æ˜¯ç»ˆæ€ï¼Œåˆ™é€€å‡ºå¾ªç¯
            if [ "$STATUS" == "success" ] || [ "$STATUS" == "failed" ] || [ "$STATUS" == "skipped" ]; then
              echo "status=$STATUS" >> $GITHUB_OUTPUT
              break
            fi
          fi
          
          # ç­‰å¾…10ç§’åå†æ¬¡æ£€æŸ¥
          echo "å·¥ä½œæµä»åœ¨è¿è¡Œï¼Œç­‰å¾…10ç§’åå†æ¬¡æ£€æŸ¥..."
          sleep 10
        done
      ||| % {workflow_type: workflow_type, id: id, timeout_seconds: timeout_seconds}
    },
    
  // ç”Ÿæˆå¯é‡è¯•æ­¥éª¤
  generateRetryableStep(step_name, command, max_retries=3, retry_delay=5)::  
    {
      name: step_name,
      id: std.strReplace(std.asciiLower(step_name), ' ', '_'),
      run: |||
        # å¯é‡è¯•å‘½ä»¤æ‰§è¡Œ
        MAX_RETRIES=%(max_retries)d
        RETRY_DELAY=%(retry_delay)d
        STEP_SUCCESS=false
        
        for ((i=1; i<=MAX_RETRIES; i++)); do
          echo "å°è¯•æ‰§è¡Œ #$i/$MAX_RETRIES..."
          
          # æ‰§è¡Œå‘½ä»¤
          %(command)s
          EXIT_CODE=$?
          
          # æ£€æŸ¥ç»“æœ
          if [ $EXIT_CODE -eq 0 ]; then
            echo "âœ… å‘½ä»¤æˆåŠŸæ‰§è¡Œ"
            STEP_SUCCESS=true
            echo "success=true" >> $GITHUB_OUTPUT
            break
          else
            echo "âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥ (é”™è¯¯ç : $EXIT_CODE)"
            if [ $i -lt $MAX_RETRIES ]; then
              echo "ç­‰å¾… $RETRY_DELAY ç§’åé‡è¯•..."
              sleep $RETRY_DELAY
            fi
          fi
        done
        
        if [ "$STEP_SUCCESS" != "true" ]; then
          echo "âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå‘½ä»¤æ‰§è¡Œå¤±è´¥"
          echo "success=false" >> $GITHUB_OUTPUT
          exit 1
        fi
      ||| % {command: command, max_retries: max_retries, retry_delay: retry_delay}
    },  
      // ç”Ÿæˆå·¥ä½œæµçŠ¶æ€æŠ¥å‘Šæ­¥éª¤
  generateWorkflowStatusStep(workflow_type, site_id, parent_param='parent_workflow_id'):: {
    name: 'æŠ¥å‘Šå·¥ä½œæµçŠ¶æ€',
    'if': 'always()',
    run: |||
      # åˆ›å»ºçŠ¶æ€ç›®å½•
      mkdir -p status/workflow
      
      # ç”ŸæˆçŠ¶æ€æ–‡ä»¶
      cat > status/workflow/%(workflow_type)s_%(site_id)s.json << EOF
      {
        "workflow_id": "${{ github.run_id }}",
        "parent_workflow_id": "${{ github.event.inputs.%(parent_param)s || '' }}",
        "workflow_type": "%(workflow_type)s",
        "site_id": "%(site_id)s",
        "status": "${{ job.status }}",
        "timestamp": "$(date -u +%%Y-%%m-%%dT%%H:%%M:%%SZ)",
        "data_date": "${{ needs.pre-check.outputs.run_date }}",
        "artifacts": [
          "%(site_id)s-data-${{ needs.pre-check.outputs.run_date }}",
          "%(site_id)s-status-${{ needs.pre-check.outputs.run_date }}"
        ],
        "run_url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
      }
      EOF
      
      # å¦‚æœæ˜¯ç”±ä¸»å·¥ä½œæµè§¦å‘çš„ï¼Œåˆ™æ¨é€çŠ¶æ€
      if [ -n "${{ github.event.inputs.%(parent_param)s }}" ]; then
        # é…ç½®Git
        git config user.name "github-actions[bot]"
        git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
        
        # æ·»åŠ çŠ¶æ€æ–‡ä»¶
        git add status/workflow/%(workflow_type)s_%(site_id)s.json
        
        # æ‹‰å–è¿œç¨‹æ›´æ”¹ï¼Œé¿å…æ¨é€å†²çª
        git pull --rebase origin main || echo "æ‹‰å–è¿œç¨‹ä»“åº“å¤±è´¥ï¼Œå°è¯•ç»§ç»­æäº¤"
        
        # æäº¤æ›´æ”¹
        if ! git diff --staged --quiet; then
          git commit -m "ğŸ“Š å·¥ä½œæµçŠ¶æ€æ›´æ–°: %(workflow_type)s_%(site_id)s (${{ needs.pre-check.outputs.run_date }})"
          git push
        fi
      fi
    ||| % {
      workflow_type: workflow_type,
      site_id: site_id,
      max_wait: max_wait
    }
  },

  // æ„å»ºé€šçŸ¥ä½œä¸š
  buildNotifyJob(site_config, global_config):: {
    local site_id = std.extVar('site_id'),
    
    name: 'å‘é€é€šçŸ¥',
    needs: ['pre-check', 'analyze'],
    'if': 'always()',
    'runs-on': 'ubuntu-latest',
    steps: [
      {
        name: 'æ£€å‡ºä»£ç ',
        'if': 'contains(needs.*.result, \'failure\') || contains(needs.*.result, \'cancelled\')',
        uses: 'actions/checkout@v4'
      },
      {
        name: 'è®¾ç½®Pythonç¯å¢ƒ',
        uses: 'actions/setup-python@v4',
        with: {
          'python-version': '3.9'
        }
      },
      {
        name: 'å®‰è£…ä¾èµ–',
        run: |||
          pip install -r requirements.txt
        |||
      },
      {
        name: 'ä¸‹è½½åˆ†æç»“æœ',
        'if': 'needs.analyze.result == \'success\'',
        uses: 'actions/download-artifact@v4',
        with: {
          name: site_id + '-analysis-${{ needs.pre-check.outputs.data_date }}',
          path: 'temp-analysis/'
        },
        'continue-on-error': true
      },
      {
        name: 'å‡†å¤‡é€šçŸ¥å†…å®¹å¹¶å‘é€',
        env: {
          DINGTALK_WEBHOOK_URL: '${{ secrets.DINGTALK_WEBHOOK }}',
          FEISHU_WEBHOOK_URL: '${{ secrets.FEISHU_WEBHOOK }}',
          WECHAT_WORK_WEBHOOK_URL: '${{ secrets.WECHAT_WEBHOOK }}'
        },
        run: |||
          # æŸ¥æ‰¾åˆ†æç»“æœæ–‡ä»¶
          ANALYSIS_FILE=""
          if [ -d "temp-analysis" ] && [ "$(ls -A temp-analysis)" ]; then
            ANALYSIS_FILE=$(find temp-analysis -name "*.json" | head -1)
            echo "æ‰¾åˆ°åˆ†ææ–‡ä»¶: $ANALYSIS_FILE"
          fi
          
          # å¦‚æœæ²¡æœ‰åˆ†ææ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„çŠ¶æ€æ–‡ä»¶
          if [ -z "$ANALYSIS_FILE" ] || [ ! -f "$ANALYSIS_FILE" ]; then
            mkdir -p temp-analysis
            cat > temp-analysis/workflow_status.json << EOF
          {
            "workflow_status": {
              "pre_check": "${{ needs.pre-check.result }}",
              "analyze": "${{ needs.analyze.result }}",
              "site_id": "%(site_id)s",
              "date": "${{ needs.pre-check.outputs.data_date }}",
              "run_id": "${{ github.run_id }}",
              "run_url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            }
          }
          EOF
            ANALYSIS_FILE="temp-analysis/workflow_status.json"
            echo "åˆ›å»ºçŠ¶æ€æ–‡ä»¶: $ANALYSIS_FILE"
          fi
          
          # å‘é€é€šçŸ¥
          echo "ğŸ“¢ å‘é€é€šçŸ¥..."
          python scripts/notify.py --file "$ANALYSIS_FILE" --site "%(site_name)s"
        ||| % {
          site_id: site_id,
          site_name: $.getSiteName(site_config, site_id)
        }
      }
    ]
  },
  
  // æ„å»ºå®Œæ•´çš„åˆ†æå™¨ä½œä¸šé›†åˆ
  buildAnalyzerJobs(site_config, global_config):: {
    'pre-check': $.buildPreCheckJob(),
    analyze: $.buildAnalyzeJob(site_config, global_config),
    notify: $.buildNotifyJob(site_config, global_config)
  },
  
  // ç”Ÿæˆè®°å½•å¼€å§‹æ—¶é—´æ­¥éª¤
  generateStartTimeStep()::{
    name: 'è®°å½•å¼€å§‹æ—¶é—´',
    id: 'setup_start',
    run: |||
      echo "start_time=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_OUTPUT
    |||
  },
  
  // ç”Ÿæˆå·¥ä½œæµæ‰§è¡ŒæŒ‡æ ‡æ”¶é›†æ­¥éª¤
  generateMetricsCollectionStep(workflow_type, id)::{
    name: 'æ”¶é›†æ‰§è¡ŒæŒ‡æ ‡',
    id: 'metrics_collection',
    if: "always()",
    run: |||
      # åˆ›å»ºæŒ‡æ ‡ç›®å½•
      mkdir -p metrics/workflow
      
      # è®°å½•æ‰§è¡Œæ—¶é—´
      START_TIME="${{ steps.setup_start.outputs.start_time || github.event.repository.pushed_at }}"
      END_TIME="$(date -u +%Y-%m-%dT%H:%M:%SZ)"
      
      # è®¡ç®—æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
      START_SECONDS=$(date -d "$START_TIME" +%s 2>/dev/null || date -j -f "%Y-%m-%dT%H:%M:%SZ" "$START_TIME" +%s 2>/dev/null || echo "0")
      END_SECONDS=$(date -d "$END_TIME" +%s 2>/dev/null || date -j -f "%Y-%m-%dT%H:%M:%SZ" "$END_TIME" +%s 2>/dev/null || echo "0")
      
      if [ "$START_SECONDS" != "0" ] && [ "$END_SECONDS" != "0" ]; then
        DURATION=$((END_SECONDS - START_SECONDS))
      else
        DURATION=0
      fi
      
      # è·å–èµ„æºä½¿ç”¨æƒ…å†µ
      MEMORY_USAGE=$(ps -o rss= -p $$ 2>/dev/null || echo "0")
      MEMORY_USAGE=$((MEMORY_USAGE / 1024)) # è½¬æ¢ä¸ºMB
      
      # åˆ›å»ºæŒ‡æ ‡æ–‡ä»¶
      cat > metrics/workflow/%(workflow_type)s_%(id)s_$(date -u +%%Y%%m%%d).json << EOF
      {
        "workflow_type": "%(workflow_type)s",
        "id": "%(id)s",
        "run_id": "${{ github.run_id }}",
        "start_time": "$START_TIME",
        "end_time": "$END_TIME",
        "duration_seconds": $DURATION,
        "status": "${{ job.status }}",
        "memory_usage_mb": $MEMORY_USAGE,
        "date": "$(date -u +%%Y-%%m-%%d)",
        "timestamp": "$(date -u +%%Y-%%m-%%dT%%H:%%M:%%SZ)"
      }
      EOF
      
      echo "âœ… æ‰§è¡ŒæŒ‡æ ‡å·²ä¿å­˜åˆ° metrics/workflow/%(workflow_type)s_%(id)s_$(date -u +%%Y%%m%%d).json"
      
      # è¾“å‡ºå…³é”®æŒ‡æ ‡
      echo "æ‰§è¡Œæ—¶é—´: $DURATION ç§’"
      echo "å†…å­˜ä½¿ç”¨: $MEMORY_USAGE MB"
      echo "çŠ¶æ€: ${{ job.status }}"
      
      # æäº¤æŒ‡æ ‡æ–‡ä»¶
      if [ "${{ github.event_name }}" != "pull_request" ]; then
        git config user.name "github-actions[bot]"
        git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
        git add metrics/workflow/%(workflow_type)s_%(id)s_$(date -u +%%Y%%m%%d).json
        git commit -m "ğŸ“Š æ·»åŠ å·¥ä½œæµæ‰§è¡ŒæŒ‡æ ‡: %(workflow_type)s_%(id)s" || echo "æ²¡æœ‰æ›´æ”¹éœ€è¦æäº¤"
        git pull --rebase origin main || echo "æ‹‰å–å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ"
        git push || echo "æ¨é€å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ"
      fi
    ||| % {workflow_type: workflow_type, id: id}
  },
  
  // ç”Ÿæˆæ™ºèƒ½è°ƒåº¦æ­¥éª¤
  generateSmartSchedulingStep(workflow_type, id)::{
    name: 'æ™ºèƒ½è°ƒåº¦åˆ†æ',
    id: 'smart_scheduling',
    run: |||
      # åˆ›å»ºæŒ‡æ ‡ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
      mkdir -p metrics/workflow
      
      echo "ğŸ§  å¼€å§‹æ™ºèƒ½è°ƒåº¦åˆ†æ..."
      
      # æ£€æŸ¥å†å²æŒ‡æ ‡æ–‡ä»¶
      METRICS_DIR="metrics/workflow"
      METRICS_PATTERN="%(workflow_type)s_%(id)s_*.json"
      
      # æŸ¥æ‰¾æ‰€æœ‰å†å²æŒ‡æ ‡æ–‡ä»¶
      HISTORY_FILES=$(find $METRICS_DIR -name "$METRICS_PATTERN" 2>/dev/null | sort)
      HISTORY_COUNT=$(echo "$HISTORY_FILES" | wc -l)
      
      if [ "$HISTORY_COUNT" -lt 5 ]; then
        echo "âš ï¸ å†å²æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘5ä¸ªå†å²è®°å½•è¿›è¡Œæ™ºèƒ½è°ƒåº¦åˆ†æ"
        echo "å½“å‰ä»…æœ‰ $HISTORY_COUNT ä¸ªå†å²è®°å½•"
        echo "scheduling_recommendation=default" >> $GITHUB_OUTPUT
        exit 0
      fi
      
      echo "ğŸ“Š æ‰¾åˆ° $HISTORY_COUNT ä¸ªå†å²è®°å½•ï¼Œå¼€å§‹åˆ†æ..."
      
      # åˆ†ææ‰§è¡Œæ—¶é—´æ¨¡å¼
      TOTAL_DURATION=0
      SUCCESS_COUNT=0
      FAILURE_COUNT=0
      MAX_DURATION=0
      MIN_DURATION=999999
      
      # æŒ‰æ—¶é—´æ®µç»Ÿè®¡æˆåŠŸç‡
      MORNING_SUCCESS=0
      MORNING_TOTAL=0
      AFTERNOON_SUCCESS=0
      AFTERNOON_TOTAL=0
      EVENING_SUCCESS=0
      EVENING_TOTAL=0
      NIGHT_SUCCESS=0
      NIGHT_TOTAL=0
      
      # åˆ†ææ¯ä¸ªæŒ‡æ ‡æ–‡ä»¶
      for FILE in $HISTORY_FILES; do
        if [ -f "$FILE" ]; then
          # æå–å…³é”®æŒ‡æ ‡
          DURATION=$(jq -r '.duration_seconds' "$FILE")
          STATUS=$(jq -r '.status' "$FILE")
          START_TIME=$(jq -r '.start_time' "$FILE")
          
          # ç»Ÿè®¡æ€»æ—¶é•¿
          TOTAL_DURATION=$((TOTAL_DURATION + DURATION))
          
          # æ›´æ–°æœ€å¤§/æœ€å°æ—¶é•¿
          if [ $DURATION -gt $MAX_DURATION ]; then
            MAX_DURATION=$DURATION
          fi
          
          if [ $DURATION -lt $MIN_DURATION ]; then
            MIN_DURATION=$DURATION
          fi
          
          # ç»Ÿè®¡æˆåŠŸ/å¤±è´¥æ¬¡æ•°
          if [ "$STATUS" == "success" ]; then
            SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
            
            # æå–å°æ—¶
            HOUR=$(date -d "$START_TIME" "+%H" 2>/dev/null || date -j -f "%Y-%m-%dT%H:%M:%SZ" "$START_TIME" "+%H" 2>/dev/null || echo "0")
            
            # æŒ‰æ—¶é—´æ®µç»Ÿè®¡
            if [ $HOUR -ge 6 ] && [ $HOUR -lt 12 ]; then
              MORNING_SUCCESS=$((MORNING_SUCCESS + 1))
              MORNING_TOTAL=$((MORNING_TOTAL + 1))
            elif [ $HOUR -ge 12 ] && [ $HOUR -lt 18 ]; then
              AFTERNOON_SUCCESS=$((AFTERNOON_SUCCESS + 1))
              AFTERNOON_TOTAL=$((AFTERNOON_TOTAL + 1))
            elif [ $HOUR -ge 18 ] && [ $HOUR -lt 24 ]; then
              EVENING_SUCCESS=$((EVENING_SUCCESS + 1))
              EVENING_TOTAL=$((EVENING_TOTAL + 1))
            else
              NIGHT_SUCCESS=$((NIGHT_SUCCESS + 1))
              NIGHT_TOTAL=$((NIGHT_TOTAL + 1))
            fi
          else
            FAILURE_COUNT=$((FAILURE_COUNT + 1))
            
            # æå–å°æ—¶ï¼ˆå¯¹äºå¤±è´¥çš„æƒ…å†µï¼‰
            HOUR=$(date -d "$START_TIME" "+%H" 2>/dev/null || date -j -f "%Y-%m-%dT%H:%M:%SZ" "$START_TIME" "+%H" 2>/dev/null || echo "0")
            
            # æŒ‰æ—¶é—´æ®µç»Ÿè®¡æ€»æ•°
            if [ $HOUR -ge 6 ] && [ $HOUR -lt 12 ]; then
              MORNING_TOTAL=$((MORNING_TOTAL + 1))
            elif [ $HOUR -ge 12 ] && [ $HOUR -lt 18 ]; then
              AFTERNOON_TOTAL=$((AFTERNOON_TOTAL + 1))
            elif [ $HOUR -ge 18 ] && [ $HOUR -lt 24 ]; then
              EVENING_TOTAL=$((EVENING_TOTAL + 1))
            else
              NIGHT_TOTAL=$((NIGHT_TOTAL + 1))
            fi
          fi
        fi
      done
      
      # è®¡ç®—å¹³å‡æ‰§è¡Œæ—¶é—´
      if [ $HISTORY_COUNT -gt 0 ]; then
        AVG_DURATION=$((TOTAL_DURATION / HISTORY_COUNT))
      else
        AVG_DURATION=0
      fi
      
      # è®¡ç®—æˆåŠŸç‡
      if [ $HISTORY_COUNT -gt 0 ]; then
        SUCCESS_RATE=$((SUCCESS_COUNT * 100 / HISTORY_COUNT))
      else
        SUCCESS_RATE=0
      fi
      
      # è®¡ç®—å„æ—¶æ®µæˆåŠŸç‡
      if [ $MORNING_TOTAL -gt 0 ]; then
        MORNING_RATE=$((MORNING_SUCCESS * 100 / MORNING_TOTAL))
      else
        MORNING_RATE=0
      fi
      
      if [ $AFTERNOON_TOTAL -gt 0 ]; then
        AFTERNOON_RATE=$((AFTERNOON_SUCCESS * 100 / AFTERNOON_TOTAL))
      else
        AFTERNOON_RATE=0
      fi
      
      if [ $EVENING_TOTAL -gt 0 ]; then
        EVENING_RATE=$((EVENING_SUCCESS * 100 / EVENING_TOTAL))
      else
        EVENING_RATE=0
      fi
      
      if [ $NIGHT_TOTAL -gt 0 ]; then
        NIGHT_RATE=$((NIGHT_SUCCESS * 100 / NIGHT_TOTAL))
      else
        NIGHT_RATE=0
      fi
      
      # è¾“å‡ºåˆ†æç»“æœ
      echo "ğŸ“ˆ åˆ†æç»“æœ:"
      echo "- å¹³å‡æ‰§è¡Œæ—¶é—´: $AVG_DURATION ç§’"
      echo "- æœ€é•¿æ‰§è¡Œæ—¶é—´: $MAX_DURATION ç§’"
      echo "- æœ€çŸ­æ‰§è¡Œæ—¶é—´: $MIN_DURATION ç§’"
      echo "- æ€»æˆåŠŸç‡: $SUCCESS_RATE%"
      echo "- ä¸ŠåˆæˆåŠŸç‡(6-12ç‚¹): $MORNING_RATE% (æˆåŠŸ: $MORNING_SUCCESS, æ€»æ•°: $MORNING_TOTAL)"
      echo "- ä¸‹åˆæˆåŠŸç‡(12-18ç‚¹): $AFTERNOON_RATE% (æˆåŠŸ: $AFTERNOON_SUCCESS, æ€»æ•°: $AFTERNOON_TOTAL)"
      echo "- æ™šä¸ŠæˆåŠŸç‡(18-24ç‚¹): $EVENING_RATE% (æˆåŠŸ: $EVENING_SUCCESS, æ€»æ•°: $EVENING_TOTAL)"
      echo "- å‡Œæ™¨æˆåŠŸç‡(0-6ç‚¹): $NIGHT_RATE% (æˆåŠŸ: $NIGHT_SUCCESS, æ€»æ•°: $NIGHT_TOTAL)"
      
      # ç¡®å®šæœ€ä½³æ‰§è¡Œæ—¶æ®µ
      BEST_RATE=0
      BEST_TIME="morning"
      
      if [ $MORNING_RATE -gt $BEST_RATE ] && [ $MORNING_TOTAL -ge 3 ]; then
        BEST_RATE=$MORNING_RATE
        BEST_TIME="morning"
      fi
      
      if [ $AFTERNOON_RATE -gt $BEST_RATE ] && [ $AFTERNOON_TOTAL -ge 3 ]; then
        BEST_RATE=$AFTERNOON_RATE
        BEST_TIME="afternoon"
      fi
      
      if [ $EVENING_RATE -gt $BEST_RATE ] && [ $EVENING_TOTAL -ge 3 ]; then
        BEST_RATE=$EVENING_RATE
        BEST_TIME="evening"
      fi
      
      if [ $NIGHT_RATE -gt $BEST_RATE ] && [ $NIGHT_TOTAL -ge 3 ]; then
        BEST_RATE=$NIGHT_RATE
        BEST_TIME="night"
      fi
      
      # æ ¹æ®æœ€ä½³æ—¶æ®µç”Ÿæˆcronè¡¨è¾¾å¼
      case $BEST_TIME in
        morning)
          CRON_EXPR="0 9 * * *"  # ä¸Šåˆ9ç‚¹
          TIME_DESC="ä¸Šåˆ9ç‚¹"
          ;;
        afternoon)
          CRON_EXPR="0 14 * * *"  # ä¸‹åˆ2ç‚¹
          TIME_DESC="ä¸‹åˆ2ç‚¹"
          ;;
        evening)
          CRON_EXPR="0 20 * * *"  # æ™šä¸Š8ç‚¹
          TIME_DESC="æ™šä¸Š8ç‚¹"
          ;;
        night)
          CRON_EXPR="0 2 * * *"  # å‡Œæ™¨2ç‚¹
          TIME_DESC="å‡Œæ™¨2ç‚¹"
          ;;
        *)
          CRON_EXPR="0 0 * * *"  # é»˜è®¤åˆå¤œ
          TIME_DESC="åˆå¤œ12ç‚¹"
          ;;
      esac
      
      # æ ¹æ®æˆåŠŸç‡è°ƒæ•´æ‰§è¡Œé¢‘ç‡
      if [ $SUCCESS_RATE -lt 50 ]; then
        # æˆåŠŸç‡ä½äº50%ï¼Œå‡å°‘æ‰§è¡Œé¢‘ç‡
        FREQ_RECOMMENDATION="å‡å°‘æ‰§è¡Œé¢‘ç‡ï¼Œå»ºè®®æ¯å‘¨æ‰§è¡Œä¸€æ¬¡"
        CRON_EXPR="0 $(echo $CRON_EXPR | cut -d' ' -f2-) * * 0"  # æ¯å‘¨æ—¥
      elif [ $SUCCESS_RATE -gt 90 ]; then
        # æˆåŠŸç‡é«˜äº90%ï¼Œå¯ä»¥å¢åŠ æ‰§è¡Œé¢‘ç‡
        FREQ_RECOMMENDATION="æˆåŠŸç‡é«˜ï¼Œå¯ä»¥å¢åŠ æ‰§è¡Œé¢‘ç‡ï¼Œå»ºè®®æ¯å¤©æ‰§è¡Œ"
      else
        # æˆåŠŸç‡é€‚ä¸­ï¼Œç»´æŒå½“å‰é¢‘ç‡
        FREQ_RECOMMENDATION="æˆåŠŸç‡é€‚ä¸­ï¼Œç»´æŒå½“å‰æ‰§è¡Œé¢‘ç‡"
      fi
      
      echo "ğŸ”® è°ƒåº¦å»ºè®®:"
      echo "- æœ€ä½³æ‰§è¡Œæ—¶æ®µ: $BEST_TIME ($TIME_DESC)"
      echo "- å»ºè®®çš„cronè¡¨è¾¾å¼: $CRON_EXPR"
      echo "- é¢‘ç‡å»ºè®®: $FREQ_RECOMMENDATION"
      
      # ä¿å­˜è°ƒåº¦å»ºè®®åˆ°è¾“å‡º
      echo "scheduling_recommendation=$BEST_TIME" >> $GITHUB_OUTPUT
      echo "cron_expression=$CRON_EXPR" >> $GITHUB_OUTPUT
      
      # åˆ›å»ºè°ƒåº¦é…ç½®æ–‡ä»¶
      cat > metrics/workflow/%(workflow_type)s_%(id)s_schedule.json << EOF
      {
        "workflow_type": "%(workflow_type)s",
        "id": "%(id)s",
        "best_time": "$BEST_TIME",
        "cron_expression": "$CRON_EXPR",
        "success_rate": $SUCCESS_RATE,
        "avg_duration": $AVG_DURATION,
        "morning_rate": $MORNING_RATE,
        "afternoon_rate": $AFTERNOON_RATE,
        "evening_rate": $EVENING_RATE,
        "night_rate": $NIGHT_RATE,
        "recommendation": "$FREQ_RECOMMENDATION",
        "last_updated": "$(date -u +%%Y-%%m-%%dT%%H:%%M:%%SZ)"
      }
      EOF
      
      echo "âœ… è°ƒåº¦é…ç½®å·²ä¿å­˜åˆ° metrics/workflow/%(workflow_type)s_%(id)s_schedule.json"
      
      # æäº¤è°ƒåº¦é…ç½®
      if [ "${{ github.event_name }}" != "pull_request" ]; then
        git config user.name "github-actions[bot]"
        git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
        git add metrics/workflow/%(workflow_type)s_%(id)s_schedule.json
        git commit -m "â±ï¸ æ›´æ–°æ™ºèƒ½è°ƒåº¦é…ç½®: %(workflow_type)s_%(id)s" || echo "æ²¡æœ‰æ›´æ”¹éœ€è¦æäº¤"
        git pull --rebase origin main || echo "æ‹‰å–å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ"
        git push || echo "æ¨é€å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ"
      fi
    ||| % {workflow_type: workflow_type, id: id}
  },
  
  // ç”Ÿæˆåº”ç”¨æ™ºèƒ½è°ƒåº¦çš„æ­¥éª¤
  generateApplyScheduleStep(workflow_type, id)::{
    name: 'åº”ç”¨æ™ºèƒ½è°ƒåº¦',
    id: 'apply_schedule',
    needs: ['smart_scheduling'],
    run: |||
      # æ£€æŸ¥æ˜¯å¦æœ‰è°ƒåº¦å»ºè®®
      SCHEDULE_RECOMMENDATION="${{ needs.smart_scheduling.outputs.scheduling_recommendation }}"
      CRON_EXPRESSION="${{ needs.smart_scheduling.outputs.cron_expression }}"
      
      if [ -z "$SCHEDULE_RECOMMENDATION" ] || [ "$SCHEDULE_RECOMMENDATION" == "default" ]; then
        echo "âš ï¸ æ²¡æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®è¿›è¡Œæ™ºèƒ½è°ƒåº¦ï¼Œä½¿ç”¨é»˜è®¤è°ƒåº¦"
        exit 0
      fi
      
      echo "ğŸ”„ åº”ç”¨æ™ºèƒ½è°ƒåº¦: $SCHEDULE_RECOMMENDATION ($CRON_EXPRESSION)"
      
      # æ›´æ–°å·¥ä½œæµæ–‡ä»¶ä¸­çš„è°ƒåº¦è¡¨è¾¾å¼
      # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„å·¥ä½œæµæ–‡ä»¶ç»“æ„è¿›è¡Œè°ƒæ•´
      WORKFLOW_FILE=".github/workflows/%(workflow_type)s_%(id)s.yml"
      
      if [ -f "$WORKFLOW_FILE" ]; then
        # å¤‡ä»½åŸæ–‡ä»¶
        cp "$WORKFLOW_FILE" "$WORKFLOW_FILE.bak"
        
        # æ›´æ–°cronè¡¨è¾¾å¼
        # è¿™é‡Œä½¿ç”¨sedå‘½ä»¤æ›¿æ¢cronè¡¨è¾¾å¼ï¼Œéœ€è¦æ ¹æ®å®é™…æ–‡ä»¶æ ¼å¼è°ƒæ•´
        sed -i.tmp "s/cron: '[^']*'/cron: '$CRON_EXPRESSION'/" "$WORKFLOW_FILE" || \
        sed -i "s/cron: '[^']*'/cron: '$CRON_EXPRESSION'/" "$WORKFLOW_FILE"
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        rm -f "$WORKFLOW_FILE.tmp"
        
        echo "âœ… å·²æ›´æ–°å·¥ä½œæµè°ƒåº¦é…ç½®"
        
        # æäº¤æ›´æ”¹
        if [ "${{ github.event_name }}" != "pull_request" ]; then
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add "$WORKFLOW_FILE"
          git commit -m "â±ï¸ è‡ªåŠ¨æ›´æ–°å·¥ä½œæµè°ƒåº¦: %(workflow_type)s_%(id)s" || echo "æ²¡æœ‰æ›´æ”¹éœ€è¦æäº¤"
          git pull --rebase origin main || echo "æ‹‰å–å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ"
          git push || echo "æ¨é€å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ"
        fi
      else
        echo "âš ï¸ å·¥ä½œæµæ–‡ä»¶ä¸å­˜åœ¨: $WORKFLOW_FILE"
      fi
    ||| % {workflow_type: workflow_type, id: id}
  }
} 