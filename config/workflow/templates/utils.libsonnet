{
  // Jsonnetå·¥å…·åº“ - ä¸ºå¢žå¼ºç‰ˆå·¥ä½œæµæ¨¡æ¿æä¾›é€šç”¨å‡½æ•°
  
  // èŽ·å–ç«™ç‚¹åç§°
  getSiteName(site_config, site_id)::
    if std.objectHas(site_config, 'site_info') && std.objectHas(site_config.site_info, 'name') then
      site_config.site_info.name
    else if std.objectHas(site_config, 'site') && std.objectHas(site_config.site, 'name') then
      site_config.site.name
    else
      site_id,
  
  // èŽ·å–åˆ†æžé…ç½®
  getAnalysisConfig(site_config)::
    if std.objectHas(site_config, 'analysis') then
      site_config.analysis
    else
      {},
  
  // èŽ·å–è¾“å‡ºæ‰©å±•å
  getOutputExtension(analysis_config)::
    if std.objectHas(analysis_config, 'output_format') then
      if analysis_config.output_format == 'json' then 'json'
      else if analysis_config.output_format == 'tsv' then 'tsv'
      else if analysis_config.output_format == 'csv' then 'csv'
      else 'json'
    else
      'tsv',
  
  // èŽ·å–çŽ¯å¢ƒå˜é‡é…ç½®
  getEnvironmentVariables(analysis_config, global_config):: [
    { name: 'OPENAI_API_KEY', secret: 'OPENAI_API_KEY' },
    { name: 'GEMINI_API_KEY', secret: 'GEMINI_API_KEY' },
    { name: 'ANTHROPIC_API_KEY', secret: 'ANTHROPIC_API_KEY' }
  ],
  
  // æž„å»ºè§¦å‘æ¡ä»¶
  buildTriggers():: {
    workflow_dispatch: {
      inputs: {
        data_date: {
          description: 'æ•°æ®æ—¥æœŸ (YYYY-MM-DDæ ¼å¼)',
          required: true,
          type: 'string'
        },
        data_file: {
          description: 'è¦åˆ†æžçš„æ•°æ®æ–‡ä»¶è·¯å¾„',
          required: true,
          type: 'string'
        },
        site_id: {
          description: 'ç½‘ç«™ID',
          required: true,
          type: 'string'
        },
        model: {
          description: 'AIæ¨¡åž‹é€‰æ‹©',
          required: false,
          type: 'choice',
          default: 'default',
          options: ['default', 'gemini-1.5-pro', 'gpt-4-turbo', 'claude-3-sonnet']
        }
      }
    },
    repository_dispatch: {
      types: ['crawler_completed']
    },
    workflow_call: {
      inputs: {
        data_date: {
          description: 'æ•°æ®æ—¥æœŸ',
          required: true,
          type: 'string'
        },
        data_file: {
          description: 'æ•°æ®æ–‡ä»¶è·¯å¾„',
          required: true,
          type: 'string'
        },
        site_id: {
          description: 'ç½‘ç«™ID',
          required: true,
          type: 'string'
        }
      },
      secrets: {
        API_KEY: {
          required: false
        }
      }
    }
  },
  
  // æž„å»ºå…¨å±€çŽ¯å¢ƒå˜é‡
  buildGlobalEnv(site_id, analysis_dir):: {
    PYTHON_VERSION: '3.10',
    ANALYSIS_DIR: analysis_dir + '/daily',
    SITE_ID: site_id,
    TZ: 'Asia/Shanghai'
  },
  
  // æž„å»ºæƒé™è®¾ç½®
  buildPermissions():: {
    contents: 'write',
    actions: 'write'
  },
  
  // æž„å»ºé»˜è®¤é…ç½®
  buildDefaults():: {
    run: {
      shell: 'bash'
    }
  },
  
  // æž„å»ºå¹¶å‘æŽ§åˆ¶
  buildConcurrency(site_id):: {
    group: 'analyzer-' + site_id + '-${{ github.ref }}',
    'cancel-in-progress': true
  },
  
  // æž„å»ºé¢„æ£€æŸ¥ä½œä¸š
  buildPreCheckJob():: {
    name: 'å‡†å¤‡åˆ†æžçŽ¯å¢ƒ',
    'runs-on': 'ubuntu-latest',
    outputs: {
      data_date: '${{ steps.params.outputs.data_date }}',
      data_file: '${{ steps.params.outputs.data_file }}',
      site_id: '${{ steps.params.outputs.site_id }}',
      analysis_dir: '${{ steps.params.outputs.analysis_dir }}',
      cache_key: '${{ steps.params.outputs.cache_key }}'
    },
    steps: [
      {
        name: 'æ£€å‡ºä»£ç ',
        uses: 'actions/checkout@v4'
      },
      {
        name: 'ç¡®å®šåˆ†æžå‚æ•°',
        id: 'params',
        run: |||
          # ä»Žå‚æ•°ä¸­èŽ·å–æ•°æ®æ—¥æœŸå’Œæ–‡ä»¶è·¯å¾„
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            DATA_DATE="${{ github.event.inputs.data_date }}"
            DATA_FILE="${{ github.event.inputs.data_file }}"
            SITE_ID="${{ github.event.inputs.site_id }}"
          elif [ "${{ github.event_name }}" == "repository_dispatch" ]; then
            DATA_DATE="${{ github.event.client_payload.data_date }}"
            DATA_FILE="${{ github.event.client_payload.data_file }}"
            SITE_ID="${{ github.event.client_payload.site_id }}"
          elif [ "${{ github.event_name }}" == "workflow_call" ]; then
            DATA_DATE="${{ inputs.data_date }}"
            DATA_FILE="${{ inputs.data_file }}"
            SITE_ID="${{ inputs.site_id }}"
          else
            # ä»ŽçŠ¶æ€æ–‡ä»¶èŽ·å–æœ€æ–°æ•°æ®
            if [ -f "status/crawler_status.json" ]; then
              DATA_DATE=$(jq -r '.date' status/crawler_status.json)
              DATA_FILE=$(jq -r '.file_path' status/crawler_status.json)
              SITE_ID="${{ env.SITE_ID }}"
            else
              echo "âŒ é”™è¯¯: æ— æ³•ç¡®å®šæ•°æ®æ—¥æœŸå’Œæ–‡ä»¶è·¯å¾„"
              exit 1
            fi
          fi
          
          # ç¡®ä¿æ—¥æœŸç›®å½•å­˜åœ¨
          mkdir -p "${ANALYSIS_DIR}/${DATA_DATE}"
          
          # è®¾ç½®è¾“å‡ºå‚æ•°
          echo "data_date=${DATA_DATE}" >> $GITHUB_OUTPUT
          echo "data_file=${DATA_FILE}" >> $GITHUB_OUTPUT
          echo "site_id=${SITE_ID}" >> $GITHUB_OUTPUT
          echo "analysis_dir=${ANALYSIS_DIR}/${DATA_DATE}" >> $GITHUB_OUTPUT
          
          # ç”Ÿæˆç¼“å­˜é”®
          if [ -f "requirements.txt" ]; then
            HASH=$(sha256sum requirements.txt | cut -d ' ' -f 1 | head -c 8)
            echo "cache_key=deps-analysis-$HASH-v1" >> $GITHUB_OUTPUT
          else
            echo "cache_key=deps-analysis-default-v1" >> $GITHUB_OUTPUT
          fi
          
          echo "ðŸ“Œ è®¾ç½®åˆ†æžå‚æ•°: æ—¥æœŸ=${DATA_DATE}, æ–‡ä»¶=${DATA_FILE}, ç«™ç‚¹=${SITE_ID}"
        |||
      },
      {
        name: 'æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨',
        run: |||
          if [ ! -f "${{ steps.params.outputs.data_file }}" ]; then
            echo "âŒ é”™è¯¯: æ•°æ®æ–‡ä»¶ ${{ steps.params.outputs.data_file }} ä¸å­˜åœ¨"
            exit 1
          else
            echo "âœ… æ•°æ®æ–‡ä»¶å­˜åœ¨ï¼Œå‡†å¤‡åˆ†æž"
          fi
        |||
      }
    ]
  },
  
  // æž„å»ºåˆ†æžä½œä¸š
  buildAnalyzeJob(site_config, global_config):: {
    local analysis_config = $.getAnalysisConfig(site_config),
    local output_extension = $.getOutputExtension(analysis_config),
    local env_vars = $.getEnvironmentVariables(analysis_config, global_config),
    local send_notification = if std.objectHas(global_config, 'notification') && std.objectHas(global_config.notification, 'enabled') then
      global_config.notification.enabled
    else
      false,
    local analyzer_script = if std.objectHas(site_config, 'analysis') && std.objectHas(site_config.analysis, 'script') then
      site_config.analysis.script
    else
      'scripts/ai_analyzer.py',
    local notification_script = if std.objectHas(global_config, 'notification') && std.objectHas(global_config.notification, 'script') then
      global_config.notification.script
    else
      'scripts/notifier.py',
    
    name: 'æ‰§è¡ŒAIåˆ†æž',
    needs: 'pre-check',
    'runs-on': 'ubuntu-latest',
    'timeout-minutes': 30,
    env: {
      DATA_DATE: '${{ needs.pre-check.outputs.data_date }}',
      DATA_FILE: '${{ needs.pre-check.outputs.data_file }}',
      SITE_ID: '${{ needs.pre-check.outputs.site_id }}',
      ANALYSIS_DIR: '${{ needs.pre-check.outputs.analysis_dir }}'
    },
    steps: [
      {
        name: 'æ£€å‡ºä»£ç ',
        uses: 'actions/checkout@v4'
      },
      {
        name: 'è®¾ç½®PythonçŽ¯å¢ƒ',
        uses: 'actions/setup-python@v5',
        with: {
          'python-version': '${{ env.PYTHON_VERSION }}',
          cache: 'pip',
          'cache-dependency-path': '**/requirements.txt'
        }
      },
      {
        name: 'å®‰è£…ä¾èµ–',
        run: |||
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            echo "å®‰è£…å¿…è¦çš„ä¾èµ–..."
            pip install requests pyyaml pandas openai google-generativeai
          fi
        |||
      },
      {
        name: 'è¿è¡ŒAIåˆ†æž',
        id: 'run_analysis',
        'continue-on-error': true,
        env: {
          [env_var.name]: '${{ secrets.' + env_var.secret + ' }}'
          for env_var in env_vars
        } + {
          MODEL: '${{ github.event.inputs.model || \'default\' }}',
          HEIMAO_KEYWORDS: '${{ vars.HEIMAO_KEYWORDS || \'\' }}',
          ENABLE_NOTIFICATION: '${{ vars.ENABLE_NOTIFICATION || \'false\' }}',
          NOTIFICATION_TYPE: '${{ vars.NOTIFICATION_TYPE || \'none\' }}',
          NOTIFICATION_WEBHOOK: '${{ vars.NOTIFICATION_WEBHOOK || \'\' }}',
          GITHUB_REPOSITORY: '${{ github.repository }}'
        },
        run: |||
          echo "ðŸ“Š å¼€å§‹åˆ†æžæ•°æ®æ–‡ä»¶: $DATA_FILE"
          
          # æŒ‡å®šæ¨¡åž‹é…ç½®
          if [ "$MODEL" != "default" ]; then
            MODEL_ARG="--model $MODEL"
            echo "ðŸ§  ä½¿ç”¨æŒ‡å®šæ¨¡åž‹: $MODEL"
          else
            MODEL_ARG=""
            echo "ðŸ§  ä½¿ç”¨é»˜è®¤æ¨¡åž‹"
          fi
          
          # è¿è¡ŒAIåˆ†æžè„šæœ¬
          python %(analyzer_script)s --file "$DATA_FILE" --site "$SITE_ID" --output "$ANALYSIS_DIR/analysis_$DATA_DATE.%(output_extension)s" $MODEL_ARG
          
          # æ£€æŸ¥åˆ†æžç»“æžœæ–‡ä»¶
          if [ -f "$ANALYSIS_DIR/analysis_$DATA_DATE.%(output_extension)s" ]; then
            echo "âœ… åˆ†æžæˆåŠŸå®Œæˆï¼Œå·²ç”Ÿæˆç»“æžœæ–‡ä»¶"
            echo "analysis_exists=true" >> $GITHUB_OUTPUT
            echo "analysis_file=$ANALYSIS_DIR/analysis_$DATA_DATE.%(output_extension)s" >> $GITHUB_OUTPUT
          else
            echo "âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°åˆ†æžç»“æžœæ–‡ä»¶"
            echo "analysis_exists=false" >> $GITHUB_OUTPUT
          fi
        ||| % {
          analyzer_script: analyzer_script,
          output_extension: output_extension
        }
      },
      {
        name: 'åˆ›å»ºåˆ†æžçŠ¶æ€æ–‡ä»¶',
        run: |||
          mkdir -p status
          # åˆ›å»ºçŠ¶æ€æ–‡ä»¶
          if [ "${{ steps.run_analysis.outcome }}" == "success" ] && [ "${{ steps.run_analysis.outputs.analysis_exists }}" == "true" ]; then
            cat > status/analyzer_$SITE_ID.json << EOF
            {
              "status": "success",
              "site_id": "$SITE_ID",
              "date": "$DATA_DATE",
              "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "data_file": "$DATA_FILE",
              "analysis_file": "$ANALYSIS_DIR/analysis_$DATA_DATE.%(output_extension)s",
              "run_id": "${{ github.run_id }}",
              "run_url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}",
              "message": "æ•°æ®åˆ†æžæˆåŠŸå®Œæˆ"
            }
          EOF
          else
            cat > status/analyzer_$SITE_ID.json << EOF
            {
              "status": "failed",
              "site_id": "$SITE_ID",
              "date": "$DATA_DATE",
              "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "data_file": "$DATA_FILE",
              "run_id": "${{ github.run_id }}",
              "run_url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}",
              "message": "æ•°æ®åˆ†æžå¤±è´¥æˆ–æ— ç»“æžœ"
            }
          EOF
          fi
          echo "å·²åˆ›å»ºåˆ†æžçŠ¶æ€æ–‡ä»¶"
        ||| % {
          output_extension: output_extension
        }
      },
      {
        name: 'æäº¤åˆ†æžç»“æžœå’ŒçŠ¶æ€',
        run: |||
          echo "æ­£åœ¨æäº¤åˆ†æžç»“æžœå’ŒçŠ¶æ€..."
          # è®¾ç½®gité…ç½®
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          
          # æ·»åŠ éœ€è¦æäº¤çš„æ–‡ä»¶
          if [ -f "$ANALYSIS_DIR/analysis_$DATA_DATE.%(output_extension)s" ]; then
            git add "$ANALYSIS_DIR/" || echo "æ²¡æœ‰åˆ†æžç›®å½•å˜æ›´"
          fi
          git add status/analyzer_$SITE_ID.json
          
          # æ£€æŸ¥æ˜¯å¦æœ‰å˜æ›´éœ€è¦æäº¤
          if git diff --staged --quiet; then
            echo "æ²¡æœ‰å˜æ›´éœ€è¦æäº¤"
          else
            # åˆ›å»ºæäº¤
            git commit -m "ðŸ¤– è‡ªåŠ¨æ›´æ–°: %(site_name)såˆ†æžç»“æžœ ($DATA_DATE)"
            # æŽ¨é€åˆ°ä»“åº“
            git push
            echo "âœ… æˆåŠŸæäº¤å¹¶æŽ¨é€åˆ†æžç»“æžœå’ŒçŠ¶æ€"
          fi
        ||| % {
          output_extension: output_extension,
          site_name: $.getSiteName(site_config, std.extVar('site_id'))
        }
      },
      {
        name: 'ä¸Šä¼ åˆ†æžç»“æžœ (ä½œä¸ºå·¥ä»¶)',
        'if': 'steps.run_analysis.outputs.analysis_exists == \'true\'',
        uses: 'actions/upload-artifact@v4',
        with: {
          name: std.extVar('site_id') + '-analysis-${{ needs.pre-check.outputs.data_date }}',
          path: '${{ steps.run_analysis.outputs.analysis_file }}',
          'retention-days': 5
        }
      }
    ] + (
      if send_notification then [
        {
          name: 'å‘é€åˆ†æžç»“æžœé€šçŸ¥',
          'if': 'steps.run_analysis.outputs.analysis_exists == \'true\'',
          env: {
            ENABLE_NOTIFICATION: '${{ vars.ENABLE_NOTIFICATION || \'false\' }}',
            NOTIFICATION_TYPE: '${{ vars.NOTIFICATION_TYPE || \'none\' }}',
            NOTIFICATION_WEBHOOK: '${{ vars.NOTIFICATION_WEBHOOK || \'\' }}',
            ANALYSIS_FILE: '${{ steps.run_analysis.outputs.analysis_file }}'
          },
          run: |||
            if [ "$ENABLE_NOTIFICATION" = "true" ] && [ -n "$NOTIFICATION_WEBHOOK" ]; then
              echo "ðŸ“¢ å‘é€åˆ†æžç»“æžœé€šçŸ¥..."
              python %(notification_script)s --file "$ANALYSIS_FILE" --site "$SITE_ID"
            else
              echo "â„¹ï¸ æœªå¯ç”¨é€šçŸ¥æˆ–ç¼ºå°‘é…ç½®ï¼Œè·³è¿‡é€šçŸ¥å‘é€"
            fi
          ||| % {
            notification_script: notification_script
          }
        }
      ] else []
    ) + [
      {
        name: 'è§¦å‘ä»ªè¡¨ç›˜æ›´æ–°å·¥ä½œæµ',
        'if': 'steps.run_analysis.outputs.analysis_exists == \'true\'',
        uses: 'benc-uk/workflow-dispatch@v1',
        with: {
          workflow: 'update_dashboard.yml',
          token: '${{ secrets.GITHUB_TOKEN }}',
          inputs: '{"site_id": "${{ needs.pre-check.outputs.site_id }}"}'
        }
      }
    ]
  },
  
  // æž„å»ºé€šçŸ¥ä½œä¸š
  buildNotifyJob(site_config, global_config):: {
    local site_id = std.extVar('site_id'),
    
    name: 'å‘é€é€šçŸ¥',
    needs: ['pre-check', 'analyze'],
    'if': 'always()',
    'runs-on': 'ubuntu-latest',
    steps: [
      {
        name: 'æ£€å‡ºä»£ç ',
        'if': 'contains(needs.*.result, \'failure\') || contains(needs.*.result, \'cancelled\')',
        uses: 'actions/checkout@v4'
      },
      {
        name: 'è®¾ç½®PythonçŽ¯å¢ƒ',
        uses: 'actions/setup-python@v4',
        with: {
          'python-version': '3.9'
        }
      },
      {
        name: 'å®‰è£…ä¾èµ–',
        run: |||
          pip install -r requirements.txt
        |||
      },
      {
        name: 'ä¸‹è½½åˆ†æžç»“æžœ',
        'if': 'needs.analyze.result == \'success\'',
        uses: 'actions/download-artifact@v4',
        with: {
          name: site_id + '-analysis-${{ needs.pre-check.outputs.data_date }}',
          path: 'temp-analysis/'
        },
        'continue-on-error': true
      },
      {
        name: 'å‡†å¤‡é€šçŸ¥å†…å®¹å¹¶å‘é€',
        env: {
          DINGTALK_WEBHOOK_URL: '${{ secrets.DINGTALK_WEBHOOK }}',
          FEISHU_WEBHOOK_URL: '${{ secrets.FEISHU_WEBHOOK }}',
          WECHAT_WORK_WEBHOOK_URL: '${{ secrets.WECHAT_WEBHOOK }}'
        },
        run: |||
          # æŸ¥æ‰¾åˆ†æžç»“æžœæ–‡ä»¶
          ANALYSIS_FILE=""
          if [ -d "temp-analysis" ] && [ "$(ls -A temp-analysis)" ]; then
            ANALYSIS_FILE=$(find temp-analysis -name "*.json" | head -1)
            echo "æ‰¾åˆ°åˆ†æžæ–‡ä»¶: $ANALYSIS_FILE"
          fi
          
          # å¦‚æžœæ²¡æœ‰åˆ†æžæ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„çŠ¶æ€æ–‡ä»¶
          if [ -z "$ANALYSIS_FILE" ] || [ ! -f "$ANALYSIS_FILE" ]; then
            mkdir -p temp-analysis
            cat > temp-analysis/workflow_status.json << EOF
          {
            "workflow_status": {
              "pre_check": "${{ needs.pre-check.result }}",
              "analyze": "${{ needs.analyze.result }}",
              "site_id": "%(site_id)s",
              "date": "${{ needs.pre-check.outputs.data_date }}",
              "run_id": "${{ github.run_id }}",
              "run_url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            }
          }
          EOF
            ANALYSIS_FILE="temp-analysis/workflow_status.json"
            echo "åˆ›å»ºçŠ¶æ€æ–‡ä»¶: $ANALYSIS_FILE"
          fi
          
          # å‘é€é€šçŸ¥
          echo "ðŸ“¢ å‘é€é€šçŸ¥..."
          python scripts/notify.py --file "$ANALYSIS_FILE" --site "%(site_name)s"
        ||| % {
          site_id: site_id,
          site_name: $.getSiteName(site_config, site_id)
        }
      }
    ]
  },
  
  // æž„å»ºå®Œæ•´çš„åˆ†æžå™¨ä½œä¸šé›†åˆ
  buildAnalyzerJobs(site_config, global_config):: {
    'pre-check': $.buildPreCheckJob(),
    analyze: $.buildAnalyzeJob(site_config, global_config),
    notify: $.buildNotifyJob(site_config, global_config)
  }
} 