{
  // Jsonnetå·¥å…·åº“ - ä¸ºå¢žå¼ºç‰ˆå·¥ä½œæµæ¨¡æ¿æä¾›é€šç”¨å‡½æ•°
  
  // èŽ·å–ç«™ç‚¹åç§°
  getSiteName(site_config, site_id)::
    if std.objectHas(site_config, 'site_info') && std.objectHas(site_config.site_info, 'name') then
      site_config.site_info.name
    else if std.objectHas(site_config, 'site') && std.objectHas(site_config.site, 'name') then
      site_config.site.name
    else
      site_id,
  
  // èŽ·å–é…ç½®éƒ¨åˆ†ï¼Œå¦‚æžœä¸å­˜åœ¨åˆ™è¿”å›žé»˜è®¤å€¼
  getConfigSection(config, section_name, default_value={})::  
    if std.objectHas(config, section_name) then
      config[section_name]
    else
      default_value,

  // èŽ·å–é…ç½®å€¼ï¼Œæ”¯æŒåµŒå¥—è·¯å¾„ï¼ˆå¦‚ "scraping.engine"ï¼‰
  getConfigValue(config, path, default_value)::  
    local parts = std.split(path, '.');
    local navigate(obj, idx) =
      if idx >= std.length(parts) then obj
      else if std.objectHas(obj, parts[idx]) then navigate(obj[parts[idx]], idx + 1)
      else default_value;
    navigate(config, 0),

  // èŽ·å–åˆ†æžé…ç½®
  getAnalysisConfig(site_config)::
    $.getConfigSection(site_config, 'analysis', {}),
  
  // èŽ·å–è¾“å‡ºæ‰©å±•å
  getOutputExtension(analysis_config)::
    if std.objectHas(analysis_config, 'output_format') then
      if analysis_config.output_format == 'json' then 'json'
      else if analysis_config.output_format == 'tsv' then 'tsv'
      else if analysis_config.output_format == 'csv' then 'csv'
      else 'json'
    else
      'tsv',
  
  // æ ¹æ®å¼•æ“Žç±»åž‹èŽ·å–çˆ¬è™«ä¾èµ–é¡¹
  getCrawlerDependencies(engine_type, dependencies_config)::  
    if engine_type == 'firecrawl' then
      dependencies_config.crawler.firecrawl
    else if engine_type == 'playwright' then
      dependencies_config.crawler.playwright
    else if engine_type == 'selenium' then
      dependencies_config.crawler.selenium
    else
      dependencies_config.crawler.requests,

  // æ ¹æ®AIæä¾›å•†èŽ·å–åˆ†æžå™¨ä¾èµ–é¡¹
  getAnalyzerDependencies(ai_provider, dependencies_config)::  
    dependencies_config.analyzer + 
    if ai_provider == 'openai' then
      ['openai>=1.0.0']
    else if ai_provider == 'gemini' then
      ['google-generativeai>=0.3.1']
    else
      [],

  // èŽ·å–çŽ¯å¢ƒå˜é‡é…ç½®
  getEnvironmentVariables(analysis_config, global_config):: [
    { name: 'OPENAI_API_KEY', secret: 'OPENAI_API_KEY' },
    { name: 'GEMINI_API_KEY', secret: 'GEMINI_API_KEY' },
    { name: 'ANTHROPIC_API_KEY', secret: 'ANTHROPIC_API_KEY' }
  ],
  
  // èŽ·å–ä½œä¸šè¶…æ—¶è®¾ç½®
  getJobTimeout(job_type, global_config)::  
    local base_timeout = if std.objectHas(global_config, 'runtime') && std.objectHas(global_config.runtime, 'timeout_minutes') 
                       then global_config.runtime.timeout_minutes 
                       else 30;
    
    // æ ¹æ®ä½œä¸šç±»åž‹è¿”å›žé€‚å½“çš„è¶…æ—¶æ—¶é—´
    if job_type == 'setup' then std.ceil(base_timeout / 3)
    else if job_type == 'crawl' then std.ceil(base_timeout * 2)
    else if job_type == 'analyze' then base_timeout
    else if job_type == 'proxy_pool' then std.ceil(base_timeout / 1.5)
    else if job_type == 'dashboard' then std.ceil(base_timeout / 2)
    else base_timeout,
  
  // èŽ·å–é”™è¯¯å¤„ç†ç­–ç•¥
  getErrorHandlingStrategy(step_type, global_config)::  
    // é»˜è®¤ç­–ç•¥
    local default_strategy = {
      'continue-on-error': false,
      'fail-fast': true
    };
    
    // å¯ä»¥å®¹å¿é”™è¯¯çš„æ­¥éª¤ç±»åž‹
    local tolerant_steps = [
      'crawl', 'analyze', 'notification', 'proxy_validation', 'data_processing'
    ];
    
    // å¦‚æžœæ˜¯å¯å®¹é”™æ­¥éª¤ï¼Œåˆ™å…è®¸ç»§ç»­æ‰§è¡Œ
    if std.member(tolerant_steps, step_type) then
      default_strategy + { 'continue-on-error': true, 'fail-fast': false }
    else
      default_strategy,
  
  // ç”Ÿæˆç¼“å­˜é…ç½®
  generateCacheConfig(workflow_type, id, engine='')::  
    local prefix = workflow_type + '-deps-' + id;
    local key_prefix = if engine != '' then prefix + '-' + engine else prefix;
    local paths = ['~/.cache/pip'];
    
    if workflow_type == 'crawler' then
      {
        enabled: true,
        key_prefix: key_prefix,
        paths: paths + ['data/' + id + '/cache']
      }
    else if workflow_type == 'analyzer' then
      {
        enabled: true,
        key_prefix: key_prefix,
        paths: paths + ['analysis/' + id + '/cache']
      }
    else if workflow_type == 'proxy_pool' then
      {
        enabled: true,
        key_prefix: key_prefix,
        paths: paths + ['proxy_pool/cache']
      }
    else if workflow_type == 'dashboard' then
      {
        enabled: true,
        key_prefix: key_prefix,
        paths: paths + ['dashboard/cache']
      }
    else
      {
        enabled: true,
        key_prefix: key_prefix,
        paths: paths
      },

  // ç”Ÿæˆå·¥ä½œæµçŽ¯å¢ƒå˜é‡
  generateWorkflowEnv(workflow_type, global_config)::  
    // åŸºç¡€çŽ¯å¢ƒå˜é‡
    local paths = if std.objectHas(global_config, 'paths') then global_config.paths else {
      config: 'config',
      config_sites: 'config/sites',
      data_daily: 'data/daily',
      status: 'status',
      logs: 'logs'
    };
    
    local base_env = {
      CONFIG_DIR: paths.config,
      SITES_DIR: paths.config_sites,
      DATA_DIR: paths.data_daily,
      STATUS_DIR: paths.status,
      LOGS_DIR: paths.logs,
      PYTHONUNBUFFERED: '1',
      WORKFLOW_TYPE: workflow_type
    };
    
    // æ ¹æ®å·¥ä½œæµç±»åž‹æ·»åŠ ç‰¹å®šçŽ¯å¢ƒå˜é‡
    local analysis = if std.objectHas(global_config, 'analysis') then global_config.analysis else {
      provider: 'gemini',
      prompt_dir: 'config/analysis/prompts'
    };
    
    local specific_env = 
      if workflow_type == 'analyzer' then {
        AI_PROVIDER: analysis.provider,
        PROMPT_DIR: analysis.prompt_dir
      }
      else if workflow_type == 'crawler' then {
        PROXY_ENABLED: std.toString(global_config.proxy.enabled)
      }
      else if workflow_type == 'proxy_pool' then {
        PROXY_TIMEOUT: std.toString(30)
      }
      else {};
    
    base_env + specific_env,
  
  // ç”Ÿæˆæ£€å‡ºä»£ç æ­¥éª¤
  generateCheckoutStep(fetch_depth=1)::  
    {
      name: 'æ£€å‡ºä»£ç ',
      uses: 'actions/checkout@v4',
      [if fetch_depth != 1 then 'with']: {
        'fetch-depth': fetch_depth
      }
    },

  // ç”Ÿæˆè®¾ç½®Pythonæ­¥éª¤
  generatePythonSetupStep(python_version, cache_pip=true)::  
    {
      name: 'è®¾ç½®Python',
      uses: 'actions/setup-python@v5',
      with: {
        'python-version': python_version,
        cache: if cache_pip then 'pip' else null
      }
    },

  // ç”Ÿæˆç¼“å­˜æ­¥éª¤
  generateCacheStep(cache_config, hashfiles_pattern)::  
    {
      name: 'ç¼“å­˜ä¾èµ–å’Œæ•°æ®',
      [if cache_config.enabled then 'if']: 'true',
      uses: 'actions/cache@v3',
      with: {
        path: std.join('\n', cache_config.paths),
        key: cache_config.key_prefix + '-${{ hashFiles(\'' + hashfiles_pattern + '\') }}',
        'restore-keys': cache_config.key_prefix + '-'
      }
    },

  // ç”Ÿæˆåˆ›å»ºç›®å½•æ­¥éª¤
  generateDirectorySetupStep(directories)::  
    {
      name: 'åˆ›å»ºå¿…è¦ç›®å½•',
      run: std.join('\n', ['mkdir -p ' + dir for dir in directories])
    },

  // æž„å»ºè§¦å‘æ¡ä»¶
  buildTriggers():: {
    workflow_dispatch: {
      inputs: {
        data_date: {
          description: 'æ•°æ®æ—¥æœŸ (YYYY-MM-DDæ ¼å¼)',
          required: true,
          type: 'string'
        },
        data_file: {
          description: 'è¦åˆ†æžçš„æ•°æ®æ–‡ä»¶è·¯å¾„',
          required: true,
          type: 'string'
        },
        site_id: {
          description: 'ç½‘ç«™ID',
          required: true,
          type: 'string'
        },
        model: {
          description: 'AIæ¨¡åž‹é€‰æ‹©',
          required: false,
          type: 'choice',
          default: 'default',
          options: ['default', 'gemini-1.5-pro', 'gpt-4-turbo', 'claude-3-sonnet']
        }
      }
    },
    repository_dispatch: {
      types: ['crawler_completed']
    },
    workflow_call: {
      inputs: {
        data_date: {
          description: 'æ•°æ®æ—¥æœŸ',
          required: true,
          type: 'string'
        },
        data_file: {
          description: 'æ•°æ®æ–‡ä»¶è·¯å¾„',
          required: true,
          type: 'string'
        },
        site_id: {
          description: 'ç½‘ç«™ID',
          required: true,
          type: 'string'
        }
      },
      secrets: {
        API_KEY: {
          required: false
        }
      }
    }
  },
  
  // æž„å»ºå…¨å±€çŽ¯å¢ƒå˜é‡
  buildGlobalEnv(site_id, analysis_dir):: {
    PYTHON_VERSION: '3.10',
    ANALYSIS_DIR: analysis_dir + '/daily',
    SITE_ID: site_id,
    TZ: 'Asia/Shanghai'
  },
  
  // æž„å»ºæƒé™è®¾ç½®
  buildPermissions():: {
    contents: 'write',
    actions: 'write'
  },
  
  // æž„å»ºé»˜è®¤é…ç½®
  buildDefaults():: {
    run: {
      shell: 'bash'
    }
  },
  
  // ç”ŸæˆGitæäº¤æ­¥éª¤
  generateGitCommitStep(paths_to_add, commit_message)::  
    local paths_array = if std.isArray(paths_to_add) then paths_to_add else [paths_to_add];
    {
      name: 'æäº¤æ›´æ”¹',
      run: |||
        # é…ç½®Git
        git config user.name "github-actions[bot]"
        git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
        
        # æ·»åŠ æ–‡ä»¶
        %(add_commands)s
        
        # æäº¤æ›´æ”¹
        if git diff --staged --quiet; then
          echo "æ²¡æœ‰å˜æ›´éœ€è¦æäº¤"
        else
          git commit -m "%(commit_message)s"
          git push
          echo "âœ… æˆåŠŸæäº¤æ›´æ”¹"
        fi
      ||| % {
        add_commands: std.join('\n', ['git add ' + std.strReplace(path, '"', '') for path in paths_array]),
        commit_message: commit_message
      }
    },

  // ç”Ÿæˆå·¥ä½œæµæ‰‹åŠ¨è§¦å‘é…ç½®
  generateWorkflowDispatchTrigger(inputs)::  
    {
      workflow_dispatch: {
        inputs: inputs
      }
    },

  // ç”Ÿæˆå®šæ—¶è§¦å‘é…ç½®
  generateScheduleTrigger(cron_expression)::  
    {
      schedule: [
        {cron: cron_expression}
      ]
    },

  // ç”Ÿæˆå¹¶å‘æŽ§åˆ¶é…ç½®
  generateConcurrencyConfig(workflow_type, id)::  
    {
      group: workflow_type + '-' + id + '-${{ github.ref }}',
      'cancel-in-progress': true
    },

  // æž„å»ºå¹¶å‘æŽ§åˆ¶
  buildConcurrency(site_id):: {
    group: 'analyzer-' + site_id + '-${{ github.ref }}',
    'cancel-in-progress': true
  },
  
  // æž„å»ºé¢„æ£€æŸ¥ä½œä¸š
  buildPreCheckJob():: {
    name: 'å‡†å¤‡åˆ†æžçŽ¯å¢ƒ',
    'runs-on': 'ubuntu-latest',
    outputs: {
      data_date: '${{ steps.params.outputs.data_date }}',
      data_file: '${{ steps.params.outputs.data_file }}',
      site_id: '${{ steps.params.outputs.site_id }}',
      analysis_dir: '${{ steps.params.outputs.analysis_dir }}',
      cache_key: '${{ steps.params.outputs.cache_key }}'
    },
    steps: [
      {
        name: 'æ£€å‡ºä»£ç ',
        uses: 'actions/checkout@v4'
      },
      {
        name: 'ç¡®å®šåˆ†æžå‚æ•°',
        id: 'params',
        run: |||
          # ä»Žå‚æ•°ä¸­èŽ·å–æ•°æ®æ—¥æœŸå’Œæ–‡ä»¶è·¯å¾„
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            DATA_DATE="${{ github.event.inputs.data_date }}"
            DATA_FILE="${{ github.event.inputs.data_file }}"
            SITE_ID="${{ github.event.inputs.site_id }}"
          elif [ "${{ github.event_name }}" == "repository_dispatch" ]; then
            DATA_DATE="${{ github.event.client_payload.data_date }}"
            DATA_FILE="${{ github.event.client_payload.data_file }}"
            SITE_ID="${{ github.event.client_payload.site_id }}"
          elif [ "${{ github.event_name }}" == "workflow_call" ]; then
            DATA_DATE="${{ inputs.data_date }}"
            DATA_FILE="${{ inputs.data_file }}"
            SITE_ID="${{ inputs.site_id }}"
          else
            # ä»ŽçŠ¶æ€æ–‡ä»¶èŽ·å–æœ€æ–°æ•°æ®
            if [ -f "status/crawler_status.json" ]; then
              DATA_DATE=$(jq -r '.date' status/crawler_status.json)
              DATA_FILE=$(jq -r '.file_path' status/crawler_status.json)
              SITE_ID="${{ env.SITE_ID }}"
            else
              echo "âŒ é”™è¯¯: æ— æ³•ç¡®å®šæ•°æ®æ—¥æœŸå’Œæ–‡ä»¶è·¯å¾„"
              exit 1
            fi
          fi
          
          # ç¡®ä¿æ—¥æœŸç›®å½•å­˜åœ¨
          mkdir -p "${ANALYSIS_DIR}/${DATA_DATE}"
          
          # è®¾ç½®è¾“å‡ºå‚æ•°
          echo "data_date=${DATA_DATE}" >> $GITHUB_OUTPUT
          echo "data_file=${DATA_FILE}" >> $GITHUB_OUTPUT
          echo "site_id=${SITE_ID}" >> $GITHUB_OUTPUT
          echo "analysis_dir=${ANALYSIS_DIR}/${DATA_DATE}" >> $GITHUB_OUTPUT
          
          # ç”Ÿæˆç¼“å­˜é”®
          if [ -f "requirements.txt" ]; then
            HASH=$(sha256sum requirements.txt | cut -d ' ' -f 1 | head -c 8)
            echo "cache_key=deps-analysis-$HASH-v1" >> $GITHUB_OUTPUT
          else
            echo "cache_key=deps-analysis-default-v1" >> $GITHUB_OUTPUT
          fi
          
          echo "ðŸ“Œ è®¾ç½®åˆ†æžå‚æ•°: æ—¥æœŸ=${DATA_DATE}, æ–‡ä»¶=${DATA_FILE}, ç«™ç‚¹=${SITE_ID}"
        |||
      },
      {
        name: 'æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨',
        run: |||
          if [ ! -f "${{ steps.params.outputs.data_file }}" ]; then
            echo "âŒ é”™è¯¯: æ•°æ®æ–‡ä»¶ ${{ steps.params.outputs.data_file }} ä¸å­˜åœ¨"
            exit 1
          else
            echo "âœ… æ•°æ®æ–‡ä»¶å­˜åœ¨ï¼Œå‡†å¤‡åˆ†æž"
          fi
        |||
      }
    ]
  },
  
  // æž„å»ºåˆ†æžä½œä¸š
  buildAnalyzeJob(site_config, global_config):: {
    local analysis_config = $.getAnalysisConfig(site_config),
    local output_extension = $.getOutputExtension(analysis_config),
    local env_vars = $.getEnvironmentVariables(analysis_config, global_config),
    local send_notification = if std.objectHas(global_config, 'notification') && std.objectHas(global_config.notification, 'enabled') then
      global_config.notification.enabled
    else
      false,
    local analyzer_script = if std.objectHas(site_config, 'analysis') && std.objectHas(site_config.analysis, 'script') then
      site_config.analysis.script
    else
      'scripts/ai_analyzer.py',
    local notification_script = if std.objectHas(global_config, 'notification') && std.objectHas(global_config.notification, 'script') then
      global_config.notification.script
    else
      'scripts/notifier.py',
    
    name: 'æ‰§è¡ŒAIåˆ†æž',
    needs: 'pre-check',
    'runs-on': 'ubuntu-latest',
    'timeout-minutes': 30,
    env: {
      DATA_DATE: '${{ needs.pre-check.outputs.data_date }}',
      DATA_FILE: '${{ needs.pre-check.outputs.data_file }}',
      SITE_ID: '${{ needs.pre-check.outputs.site_id }}',
      ANALYSIS_DIR: '${{ needs.pre-check.outputs.analysis_dir }}'
    },
    steps: [
      {
        name: 'æ£€å‡ºä»£ç ',
        uses: 'actions/checkout@v4'
      },
      {
        name: 'è®¾ç½®PythonçŽ¯å¢ƒ',
        uses: 'actions/setup-python@v5',
        with: {
          'python-version': '${{ env.PYTHON_VERSION }}',
          cache: 'pip',
          'cache-dependency-path': '**/requirements.txt'
        }
      },
      {
        name: 'å®‰è£…ä¾èµ–',
        run: |||
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            echo "å®‰è£…å¿…è¦çš„ä¾èµ–..."
            pip install requests pyyaml pandas openai google-generativeai
          fi
        |||
      },
      {
        name: 'è¿è¡ŒAIåˆ†æž',
        id: 'run_analysis',
        'continue-on-error': true,
        env: {
          [env_var.name]: '${{ secrets.' + env_var.secret + ' }}'
          for env_var in env_vars
        } + {
          MODEL: '${{ github.event.inputs.model || \'default\' }}',
          HEIMAO_KEYWORDS: '${{ vars.HEIMAO_KEYWORDS || \'\' }}',
          ENABLE_NOTIFICATION: '${{ vars.ENABLE_NOTIFICATION || \'false\' }}',
          NOTIFICATION_TYPE: '${{ vars.NOTIFICATION_TYPE || \'none\' }}',
          NOTIFICATION_WEBHOOK: '${{ vars.NOTIFICATION_WEBHOOK || \'\' }}',
          GITHUB_REPOSITORY: '${{ github.repository }}'
        },
        run: |||
          echo "ðŸ“Š å¼€å§‹åˆ†æžæ•°æ®æ–‡ä»¶: $DATA_FILE"
          
          # æŒ‡å®šæ¨¡åž‹é…ç½®
          if [ "$MODEL" != "default" ]; then
            MODEL_ARG="--model $MODEL"
            echo "ðŸ§  ä½¿ç”¨æŒ‡å®šæ¨¡åž‹: $MODEL"
          else
            MODEL_ARG=""
            echo "ðŸ§  ä½¿ç”¨é»˜è®¤æ¨¡åž‹"
          fi
          
          # è¿è¡ŒAIåˆ†æžè„šæœ¬
          python %(analyzer_script)s --file "$DATA_FILE" --site "$SITE_ID" --output "$ANALYSIS_DIR/analysis_$DATA_DATE.%(output_extension)s" $MODEL_ARG
          
          # æ£€æŸ¥åˆ†æžç»“æžœæ–‡ä»¶
          if [ -f "$ANALYSIS_DIR/analysis_$DATA_DATE.%(output_extension)s" ]; then
            echo "âœ… åˆ†æžæˆåŠŸå®Œæˆï¼Œå·²ç”Ÿæˆç»“æžœæ–‡ä»¶"
            echo "analysis_exists=true" >> $GITHUB_OUTPUT
            echo "analysis_file=$ANALYSIS_DIR/analysis_$DATA_DATE.%(output_extension)s" >> $GITHUB_OUTPUT
          else
            echo "âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°åˆ†æžç»“æžœæ–‡ä»¶"
            echo "analysis_exists=false" >> $GITHUB_OUTPUT
          fi
        ||| % {
          analyzer_script: analyzer_script,
          output_extension: output_extension
        }
      },
      {
        name: 'åˆ›å»ºåˆ†æžçŠ¶æ€æ–‡ä»¶',
        run: |||
          mkdir -p status
          # åˆ›å»ºçŠ¶æ€æ–‡ä»¶
          if [ "${{ steps.run_analysis.outcome }}" == "success" ] && [ "${{ steps.run_analysis.outputs.analysis_exists }}" == "true" ]; then
            cat > status/analyzer_$SITE_ID.json << EOF
            {
              "status": "success",
              "site_id": "$SITE_ID",
              "date": "$DATA_DATE",
              "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "data_file": "$DATA_FILE",
              "analysis_file": "$ANALYSIS_DIR/analysis_$DATA_DATE.%(output_extension)s",
              "run_id": "${{ github.run_id }}",
              "run_url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}",
              "message": "æ•°æ®åˆ†æžæˆåŠŸå®Œæˆ"
            }
          EOF
          else
            cat > status/analyzer_$SITE_ID.json << EOF
            {
              "status": "failed",
              "site_id": "$SITE_ID",
              "date": "$DATA_DATE",
              "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "data_file": "$DATA_FILE",
              "run_id": "${{ github.run_id }}",
              "run_url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}",
              "message": "æ•°æ®åˆ†æžå¤±è´¥æˆ–æ— ç»“æžœ"
            }
          EOF
          fi
          echo "å·²åˆ›å»ºåˆ†æžçŠ¶æ€æ–‡ä»¶"
        ||| % {
          output_extension: output_extension
        }
      },
      {
        name: 'æäº¤åˆ†æžç»“æžœå’ŒçŠ¶æ€',
        run: |||
          echo "æ­£åœ¨æäº¤åˆ†æžç»“æžœå’ŒçŠ¶æ€..."
          # è®¾ç½®gité…ç½®
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          
          # æ·»åŠ éœ€è¦æäº¤çš„æ–‡ä»¶
          if [ -f "$ANALYSIS_DIR/analysis_$DATA_DATE.%(output_extension)s" ]; then
            git add "$ANALYSIS_DIR/" || echo "æ²¡æœ‰åˆ†æžç›®å½•å˜æ›´"
          fi
          git add status/analyzer_$SITE_ID.json
          
          # æ£€æŸ¥æ˜¯å¦æœ‰å˜æ›´éœ€è¦æäº¤
          if git diff --staged --quiet; then
            echo "æ²¡æœ‰å˜æ›´éœ€è¦æäº¤"
          else
            # åˆ›å»ºæäº¤
            git commit -m "ðŸ¤– è‡ªåŠ¨æ›´æ–°: %(site_name)såˆ†æžç»“æžœ ($DATA_DATE)"
            # æŽ¨é€åˆ°ä»“åº“
            git push
            echo "âœ… æˆåŠŸæäº¤å¹¶æŽ¨é€åˆ†æžç»“æžœå’ŒçŠ¶æ€"
          fi
        ||| % {
          output_extension: output_extension,
          site_name: $.getSiteName(site_config, std.extVar('site_id'))
        }
      },
      {
        name: 'ä¸Šä¼ åˆ†æžç»“æžœ (ä½œä¸ºå·¥ä»¶)',
        'if': 'steps.run_analysis.outputs.analysis_exists == \'true\'',
        uses: 'actions/upload-artifact@v4',
        with: {
          name: std.extVar('site_id') + '-analysis-${{ needs.pre-check.outputs.data_date }}',
          path: '${{ steps.run_analysis.outputs.analysis_file }}',
          'retention-days': 5
        }
      }
    ] + (
      if send_notification then [
        {
          name: 'å‘é€åˆ†æžç»“æžœé€šçŸ¥',
          'if': 'steps.run_analysis.outputs.analysis_exists == \'true\'',
          env: {
            ENABLE_NOTIFICATION: '${{ vars.ENABLE_NOTIFICATION || \'false\' }}',
            NOTIFICATION_TYPE: '${{ vars.NOTIFICATION_TYPE || \'none\' }}',
            NOTIFICATION_WEBHOOK: '${{ vars.NOTIFICATION_WEBHOOK || \'\' }}',
            ANALYSIS_FILE: '${{ steps.run_analysis.outputs.analysis_file }}'
          },
          run: |||
            if [ "$ENABLE_NOTIFICATION" = "true" ] && [ -n "$NOTIFICATION_WEBHOOK" ]; then
              echo "ðŸ“¢ å‘é€åˆ†æžç»“æžœé€šçŸ¥..."
              python %(notification_script)s --file "$ANALYSIS_FILE" --site "$SITE_ID"
            else
              echo "â„¹ï¸ æœªå¯ç”¨é€šçŸ¥æˆ–ç¼ºå°‘é…ç½®ï¼Œè·³è¿‡é€šçŸ¥å‘é€"
            fi
          ||| % {
            notification_script: notification_script
          }
        }
      ] else []
    ) + [
      {
        name: 'è§¦å‘ä»ªè¡¨ç›˜æ›´æ–°å·¥ä½œæµ',
        'if': 'steps.run_analysis.outputs.analysis_exists == \'true\'',
        uses: 'benc-uk/workflow-dispatch@v1',
        with: {
          workflow: 'update_dashboard.yml',
          token: '${{ secrets.GITHUB_TOKEN }}',
          inputs: '{"site_id": "${{ needs.pre-check.outputs.site_id }}"}'
        }
      }
    ]
  },
  
  // æž„å»ºé€šçŸ¥ä½œä¸š
  buildNotifyJob(site_config, global_config):: {
    local site_id = std.extVar('site_id'),
    
    name: 'å‘é€é€šçŸ¥',
    needs: ['pre-check', 'analyze'],
    'if': 'always()',
    'runs-on': 'ubuntu-latest',
    steps: [
      {
        name: 'æ£€å‡ºä»£ç ',
        'if': 'contains(needs.*.result, \'failure\') || contains(needs.*.result, \'cancelled\')',
        uses: 'actions/checkout@v4'
      },
      {
        name: 'è®¾ç½®PythonçŽ¯å¢ƒ',
        uses: 'actions/setup-python@v4',
        with: {
          'python-version': '3.9'
        }
      },
      {
        name: 'å®‰è£…ä¾èµ–',
        run: |||
          pip install -r requirements.txt
        |||
      },
      {
        name: 'ä¸‹è½½åˆ†æžç»“æžœ',
        'if': 'needs.analyze.result == \'success\'',
        uses: 'actions/download-artifact@v4',
        with: {
          name: site_id + '-analysis-${{ needs.pre-check.outputs.data_date }}',
          path: 'temp-analysis/'
        },
        'continue-on-error': true
      },
      {
        name: 'å‡†å¤‡é€šçŸ¥å†…å®¹å¹¶å‘é€',
        env: {
          DINGTALK_WEBHOOK_URL: '${{ secrets.DINGTALK_WEBHOOK }}',
          FEISHU_WEBHOOK_URL: '${{ secrets.FEISHU_WEBHOOK }}',
          WECHAT_WORK_WEBHOOK_URL: '${{ secrets.WECHAT_WEBHOOK }}'
        },
        run: |||
          # æŸ¥æ‰¾åˆ†æžç»“æžœæ–‡ä»¶
          ANALYSIS_FILE=""
          if [ -d "temp-analysis" ] && [ "$(ls -A temp-analysis)" ]; then
            ANALYSIS_FILE=$(find temp-analysis -name "*.json" | head -1)
            echo "æ‰¾åˆ°åˆ†æžæ–‡ä»¶: $ANALYSIS_FILE"
          fi
          
          # å¦‚æžœæ²¡æœ‰åˆ†æžæ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„çŠ¶æ€æ–‡ä»¶
          if [ -z "$ANALYSIS_FILE" ] || [ ! -f "$ANALYSIS_FILE" ]; then
            mkdir -p temp-analysis
            cat > temp-analysis/workflow_status.json << EOF
          {
            "workflow_status": {
              "pre_check": "${{ needs.pre-check.result }}",
              "analyze": "${{ needs.analyze.result }}",
              "site_id": "%(site_id)s",
              "date": "${{ needs.pre-check.outputs.data_date }}",
              "run_id": "${{ github.run_id }}",
              "run_url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            }
          }
          EOF
            ANALYSIS_FILE="temp-analysis/workflow_status.json"
            echo "åˆ›å»ºçŠ¶æ€æ–‡ä»¶: $ANALYSIS_FILE"
          fi
          
          # å‘é€é€šçŸ¥
          echo "ðŸ“¢ å‘é€é€šçŸ¥..."
          python scripts/notify.py --file "$ANALYSIS_FILE" --site "%(site_name)s"
        ||| % {
          site_id: site_id,
          site_name: $.getSiteName(site_config, site_id)
        }
      }
    ]
  },
  
  // æž„å»ºå®Œæ•´çš„åˆ†æžå™¨ä½œä¸šé›†åˆ
  buildAnalyzerJobs(site_config, global_config):: {
    'pre-check': $.buildPreCheckJob(),
    analyze: $.buildAnalyzeJob(site_config, global_config),
    notify: $.buildNotifyJob(site_config, global_config)
  }
} 