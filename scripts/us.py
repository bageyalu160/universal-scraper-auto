#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Universal Scraper (us) - ç»Ÿä¸€å‘½ä»¤è¡Œå·¥å…·
æä¾›æ ‡å‡†åŒ–çš„å‘½ä»¤æ¥å£æ¥ç®¡ç†å·¥ä½œæµç”Ÿæˆå’Œæ‰§è¡Œ

æ­¤æ–‡ä»¶æ•´åˆäº†ä»¥ä¸‹åŠŸèƒ½ï¼š
- å·¥ä½œæµç”Ÿæˆ (åŸ generate_workflow.py, generate_workflows*.py)
- çˆ¬è™«æ‰§è¡Œ (åŸ scraper.py)
- åˆ†ææ‰§è¡Œ (åŸ analyzer.py)
- å·¥ä½œæµæ‰§è¡Œ (åŸ run_workflow.py)
- é€šçŸ¥å‘é€ (åŸ notifier.py)

ä½¿ç”¨æ–¹æ³•ï¼š
  python scripts/us.py [å‘½ä»¤] [é€‰é¡¹]

ä¸»è¦å‘½ä»¤ï¼š
  generate, gen   - ç”Ÿæˆå·¥ä½œæµé…ç½®
  execute, run     - æ‰§è¡Œå®Œæ•´å·¥ä½œæµ
  scrape, crawl    - ä»…æ‰§è¡Œçˆ¬è™«
  analyze          - ä»…æ‰§è¡Œåˆ†æ
  notify           - ä»…å‘é€é€šçŸ¥

ç¤ºä¾‹ï¼š
  python scripts/us.py generate --site pm001 --type all
  python scripts/us.py run --site heimao
  python scripts/us.py scrape --site pm001
"""

import os
import sys
import argparse
import logging
import time
import yaml
import json
import importlib
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# å¯¼å…¥å·¥ä½œæµç»„ä»¶
from scripts.workflow_generator.engine_factory import WorkflowEngineFactory

# å¯¼å…¥å¢å¼ºç‰ˆJsonnetç”Ÿæˆå™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
try:
    from scripts.workflow_generator.enhanced_jsonnet_generator import EnhancedJsonnetGenerator
    ENHANCED_JSONNET_AVAILABLE = True
except ImportError:
    ENHANCED_JSONNET_AVAILABLE = False

# å¯¼å…¥å·¥ä½œæµéªŒè¯å™¨
try:
    from scripts.workflow_generator.validators import WorkflowValidator
    VALIDATOR_AVAILABLE = True
except ImportError:
    VALIDATOR_AVAILABLE = False


def setup_logger(verbose=False):
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    level = logging.DEBUG if verbose else logging.INFO
    
    logger = logging.getLogger('universal-scraper')
    logger.setLevel(level)
    
    # æ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨
    if logger.handlers:
        logger.handlers.clear()
    
    # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(level)
    
    # åˆ›å»ºæ ¼å¼åŒ–å™¨
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    
    # æ·»åŠ å¤„ç†å™¨åˆ°æ—¥å¿—è®°å½•å™¨
    logger.addHandler(console_handler)
    
    return logger


def handle_generate(args, logger):
    """ç”Ÿæˆå·¥ä½œæµå‘½ä»¤å¤„ç†å‡½æ•°"""
    logger.info(f"å¼€å§‹ç”Ÿæˆå·¥ä½œæµï¼Œç«™ç‚¹: {args.site or 'æ‰€æœ‰'}, ç±»å‹: {args.type}")
    
    # è®°å½•è¯¦ç»†å‚æ•°
    if logger.level == logging.DEBUG:
        logger.debug(f"è¯¦ç»†å‚æ•°: config={args.config}, sites_dir={args.sites_dir}, output_dir={args.output_dir}, enhanced={args.enhanced}")
        if hasattr(args, 'cache') and args.cache:
            logger.debug(f"ç¼“å­˜è®¾ç½®: {args.cache}")
        if hasattr(args, 'timeout') and args.timeout:
            logger.debug(f"è¶…æ—¶è®¾ç½®: {args.timeout} åˆ†é’Ÿ")
        if hasattr(args, 'error_strategy') and args.error_strategy:
            logger.debug(f"é”™è¯¯å¤„ç†ç­–ç•¥: {args.error_strategy}")
    
    # åˆ›å»ºå·¥ä½œæµå¼•æ“å·¥å‚
    factory = WorkflowEngineFactory(
        settings_path=args.config,
        sites_dir=args.sites_dir,
        output_dir=args.output_dir,
        logger=logger
    )
    logger.debug("å·¥ä½œæµå¼•æ“å·¥å‚åˆ›å»ºæˆåŠŸ")
    
    # ä½¿ç”¨å¢å¼ºç‰ˆå¼•æ“è¿˜æ˜¯æ ‡å‡†å¼•æ“
    if args.enhanced and ENHANCED_JSONNET_AVAILABLE:
        logger.debug("ä½¿ç”¨å¢å¼ºç‰ˆ Jsonnet å¼•æ“")
        generator = EnhancedJsonnetGenerator(
            settings_path=args.config,
            sites_dir=args.sites_dir,
            output_dir=args.output_dir,
            logger=logger
        )
    else:
        logger.debug("ä½¿ç”¨æ ‡å‡† Jsonnet å¼•æ“")
        generator = factory.get_generator('jsonnet', validate_output=True)
    
    # åº”ç”¨é«˜çº§é…ç½®
    if hasattr(args, 'cache') and args.cache:
        cache_enabled = args.cache == 'enable'
        logger.info(f"ç¼“å­˜è®¾ç½®: {'å¯ç”¨' if cache_enabled else 'ç¦ç”¨'}")
        generator.set_cache_enabled(cache_enabled)
    
    if hasattr(args, 'timeout') and args.timeout:
        logger.info(f"è¶…æ—¶è®¾ç½®: {args.timeout} åˆ†é’Ÿ")
        generator.set_timeout(args.timeout)
    
    if hasattr(args, 'error_strategy') and args.error_strategy:
        logger.info(f"é”™è¯¯å¤„ç†ç­–ç•¥: {args.error_strategy}")
        generator.set_error_strategy(args.error_strategy)
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    success = False
    
    # æ ¹æ®ç±»å‹å’Œç«™ç‚¹å‚æ•°ç”Ÿæˆå·¥ä½œæµ
    if args.type == 'all':
        if args.site:
            logger.info(f"ä¸ºç«™ç‚¹ {args.site} ç”Ÿæˆçˆ¬è™«å’Œåˆ†æå·¥ä½œæµ")
            crawler_success = generator.generate_crawler_workflow(args.site)
            
            if args.enhanced and ENHANCED_JSONNET_AVAILABLE:
                analyzer_success = generator.generate_enhanced_analyzer_workflow(args.site)
            else:
                analyzer_success = generator.generate_analyzer_workflow(args.site)
                
            success = crawler_success and analyzer_success
        else:
            logger.info("ç”Ÿæˆæ‰€æœ‰å·¥ä½œæµ")
            success = generator.generate_all_workflows()
    
    elif args.type == 'common':
        logger.info("ç”Ÿæˆé€šç”¨å·¥ä½œæµï¼ˆä¸»è°ƒåº¦ã€ä»ªè¡¨ç›˜ã€ä»£ç†æ± ï¼‰")
        success = generator.generate_common_workflows()
    
    elif args.type == 'crawler':
        if not args.site:
            logger.error("ç”Ÿæˆçˆ¬è™«å·¥ä½œæµéœ€è¦æŒ‡å®šç«™ç‚¹ID")
            return False
        logger.info(f"ä¸ºç«™ç‚¹ {args.site} ç”Ÿæˆçˆ¬è™«å·¥ä½œæµ")
        success = generator.generate_crawler_workflow(args.site)
    
    elif args.type == 'analyzer':
        if not args.site:
            logger.error("ç”Ÿæˆåˆ†æå·¥ä½œæµéœ€è¦æŒ‡å®šç«™ç‚¹ID")
            return False
        logger.info(f"ä¸ºç«™ç‚¹ {args.site} ç”Ÿæˆåˆ†æå·¥ä½œæµ")
        
        if args.enhanced and ENHANCED_JSONNET_AVAILABLE:
            success = generator.generate_enhanced_analyzer_workflow(args.site)
        else:
            success = generator.generate_analyzer_workflow(args.site)
    
    # è®¡ç®—è€—æ—¶
    elapsed_time = time.time() - start_time
    
    if success:
        logger.info(f"âœ… å·¥ä½œæµç”ŸæˆæˆåŠŸï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
    else:
        logger.error(f"âŒ å·¥ä½œæµç”Ÿæˆå¤±è´¥ï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
    
    return success


def handle_execute(args, logger):
    """æ‰§è¡Œå·¥ä½œæµå‘½ä»¤å¤„ç†å‡½æ•°"""
    if not args.site:
        logger.error("æ‰§è¡Œå·¥ä½œæµéœ€è¦æŒ‡å®šç«™ç‚¹ID")
        return False
    
    logger.info(f"å¼€å§‹æ‰§è¡Œå®Œæ•´å·¥ä½œæµï¼Œç«™ç‚¹: {args.site}")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = args.output_dir
    if not output_dir:
        today = datetime.now().strftime('%Y-%m-%d')
        output_dir = os.path.join('data', 'daily', today)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    try:
        # åŠ è½½é…ç½®
        site_config = load_config(args.site, args.config)
        global_config = load_global_config(args.settings)
        
        # è·å–ç«™ç‚¹åç§°
        site_name = site_config.get('site', {}).get('name', args.site)
        
        # 1. è¿è¡Œçˆ¬è™«é˜¶æ®µ
        logger.info(f"=== å¼€å§‹çˆ¬è™«é˜¶æ®µ: {args.site} ===")
        scrape_args = argparse.Namespace(
            site=args.site,
            output_dir=output_dir,
            config=args.config,
            verbose=args.verbose
        )
        success, crawler_result = handle_scrape(scrape_args, logger)
        
        if not success:
            logger.error("çˆ¬è™«é˜¶æ®µå¤±è´¥ï¼Œç»ˆæ­¢å·¥ä½œæµ")
            return False
        
        # 2. è¿è¡Œåˆ†æé˜¶æ®µ
        data_file = crawler_result.get('output_path')
        logger.info(f"çˆ¬è™«æˆåŠŸï¼Œè·å–äº† {crawler_result.get('count', 0)} æ¡æ•°æ®")
        logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {data_file}")
        
        # è®¾ç½®åˆ†æè¾“å‡ºç›®å½•
        analysis_dir = os.path.join('analysis', 'daily', datetime.now().strftime('%Y-%m-%d'))
        os.makedirs(analysis_dir, exist_ok=True)
        
        # åˆ›å»ºåˆ†æå‚æ•°
        analyze_args = argparse.Namespace(
            site=args.site,
            data=data_file,
            output_dir=analysis_dir,
            config=args.config,
            settings=args.settings,
            verbose=args.verbose
        )
        
        logger.info(f"=== å¼€å§‹åˆ†æé˜¶æ®µ: {args.site} ===")
        analysis_success, analysis_result = handle_analyze(analyze_args, logger)
        
        # 3. è¿è¡Œé€šçŸ¥é˜¶æ®µï¼ˆå¦‚æœåˆ†ææˆåŠŸï¼‰
        if analysis_success:
            # è·å–åˆ†æç»“æœæ–‡ä»¶è·¯å¾„
            analysis_file = os.path.join(analysis_dir, f"{args.site}_analysis.json")
            summary_file = os.path.join(analysis_dir, f"{args.site}_summary.md")
            
            logger.info(f"=== å¼€å§‹é€šçŸ¥é˜¶æ®µ: {args.site} ===")
            try:
                notification_result = run_notifier(
                    args.site, 
                    site_name, 
                    data_file, 
                    analysis_file, 
                    summary_file, 
                    global_config
                )
                
                if notification_result and notification_result.get('status') == 'success':
                    logger.info("é€šçŸ¥å‘é€æˆåŠŸ")
                else:
                    logger.warning(f"é€šçŸ¥å‘é€å¤±è´¥: {notification_result.get('message', 'æœªçŸ¥é”™è¯¯')}")
            except Exception as e:
                logger.warning(f"é€šçŸ¥é˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}")
        
        # è®¡ç®—æ€»è€—æ—¶
        elapsed_time = time.time() - start_time
        logger.info(f"=== å·¥ä½œæµå®Œæˆ: {args.site}, æ€»è€—æ—¶: {elapsed_time:.2f} ç§’ ===")
        
        return analysis_success
    except Exception as e:
        elapsed_time = time.time() - start_time if 'start_time' in locals() else 0
        logger.exception(f"âŒ å·¥ä½œæµè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}, è€—æ—¶: {elapsed_time:.2f}ç§’")
        return False


def handle_scrape(args, logger):
    """çˆ¬è™«å‘½ä»¤å¤„ç†å‡½æ•°"""
    if not args.site:
        logger.error("æ‰§è¡Œçˆ¬è™«éœ€è¦æŒ‡å®šç«™ç‚¹ID")
        return False
    
    logger.info(f"å¼€å§‹æ‰§è¡Œçˆ¬è™«ï¼Œç«™ç‚¹: {args.site}")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = args.output_dir
    if not output_dir:
        today = datetime.now().strftime('%Y-%m-%d')
        output_dir = os.path.join('data', 'daily', today)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # åŠ è½½é…ç½®
    config = load_config(args.site, args.config)
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # è¿è¡Œçˆ¬è™«
    result = run_scraper(args.site, config, output_dir)
    
    # è®¡ç®—è€—æ—¶
    elapsed_time = time.time() - start_time
    
    if result and result.get('status') == 'success':
        logger.info(f"âœ… çˆ¬è™«æ‰§è¡ŒæˆåŠŸï¼Œè·å–äº† {result.get('count')} æ¡æ•°æ®ï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
        logger.info(f"ğŸ“„ æ•°æ®å·²ä¿å­˜åˆ°: {result.get('output_path')}")
        return True, result
    else:
        logger.error(f"âŒ çˆ¬è™«æ‰§è¡Œå¤±è´¥ï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
        return False, result


def handle_analyze(args, logger):
    """åˆ†æå‘½ä»¤å¤„ç†å‡½æ•°"""
    if not args.site:
        logger.error("æ‰§è¡Œåˆ†æéœ€è¦æŒ‡å®šç«™ç‚¹ID")
        return False, {"status": "error", "message": "ç¼ºå°‘ç«™ç‚¹ID"}
    
    if not args.data:
        logger.error("æ‰§è¡Œåˆ†æéœ€è¦æŒ‡å®šæ•°æ®æ–‡ä»¶")
        return False, {"status": "error", "message": "ç¼ºå°‘æ•°æ®æ–‡ä»¶"}
    
    logger.info(f"å¼€å§‹æ‰§è¡Œåˆ†æï¼Œç«™ç‚¹: {args.site}ï¼Œæ•°æ®æ–‡ä»¶: {args.data}")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = args.output_dir
    if not output_dir:
        today = datetime.now().strftime('%Y-%m-%d')
        output_dir = os.path.join('analysis', 'daily', today)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        # åŠ è½½ç«™ç‚¹é…ç½®
        site_config = load_config(args.site, args.config)
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # è¿è¡Œåˆ†æå™¨
        result = run_analyzer(args.site, args.data, site_config, output_dir)
        
        # è®¡ç®—è€—æ—¶
        elapsed_time = time.time() - start_time
        
        if result and result.get('status') == 'success':
            logger.info(f"âœ… åˆ†ææ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
            logger.info(f"ğŸ“„ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {result.get('output_path')}")
            return True, result
        else:
            logger.error(f"âŒ åˆ†ææ‰§è¡Œå¤±è´¥ï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
            return False, result
    except Exception as e:
        elapsed_time = time.time() - start_time if 'start_time' in locals() else 0
        logger.exception(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}, è€—æ—¶: {elapsed_time:.2f}ç§’")
        return False, {"status": "error", "message": str(e)}


def load_config(site_id, config_file=None):
    """åŠ è½½ç«™ç‚¹é…ç½®æ–‡ä»¶"""
    if config_file:
        config_path = Path(config_file)
    else:
        config_path = Path('config') / 'sites' / f'{site_id}.yaml'
    
    if not config_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config


def load_global_config(settings_file=None):
    """åŠ è½½å…¨å±€é…ç½®æ–‡ä»¶"""
    if settings_file:
        config_path = Path(settings_file)
    else:
        config_path = Path('config') / 'settings.yaml'
    
    if not config_path.exists():
        raise FileNotFoundError(f"å…¨å±€é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config


def run_scraper(site_id, config, output_dir=None, **kwargs):
    """æ ¹æ®é…ç½®è¿è¡Œçˆ¬è™«ï¼Œæ”¯æŒè‡ªåŠ¨é€ä¼ é¢å¤–å‚æ•°"""
    scraping = config.get('scraping', {})
    engine = scraping.get('engine', 'custom')
    
    if engine == 'custom':
        # è·å–è‡ªå®šä¹‰æ¨¡å—å’Œå‡½æ•°
        module_path = scraping.get('custom_module')
        function_name = scraping.get('custom_function')
        
        if not module_path or not function_name:
            raise ValueError("é…ç½®ç¼ºå°‘custom_moduleæˆ–custom_function")
        
        try:
            # å¯¼å…¥æ¨¡å—
            module = importlib.import_module(module_path)
            scrape_function = getattr(module, function_name)
            
            # è¿è¡Œçˆ¬è™«ï¼Œè‡ªåŠ¨é€ä¼ kwargs
            result = scrape_function(config, output_dir, **kwargs)
            
            # å¤åˆ¶ç»“æœæ–‡ä»¶åˆ°å½“å‰ç›®å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰
            output_config = config.get('output', {})
            output_filename = output_config.get('filename', f'{site_id}_data.json')
            if output_dir:
                source_path = Path(output_dir) / output_filename
                if source_path.exists():
                    import shutil
                    shutil.copy(source_path, output_filename)
            
            return result
        except ImportError as e:
            raise ImportError(f"æ— æ³•å¯¼å…¥æ¨¡å—: {module_path}") from e
        except AttributeError as e:
            raise AttributeError(f"æ¨¡å— {module_path} ä¸­æ²¡æœ‰å‡½æ•° {function_name}") from e
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„çˆ¬è™«å¼•æ“: {engine}")


def run_analyzer(site_id, data_file, config, output_dir):
    """è¿è¡Œåˆ†æå™¨"""
    # ä¸ºPM001ç«™ç‚¹ä½¿ç”¨ä¸“é—¨çš„åˆ†æå™¨
    if site_id == 'pm001':
        from src.analyzers.pm001_analyzer import analyze_pm001_data
        return analyze_pm001_data(data_file, config, output_dir)
    elif site_id == 'heimao':
        from src.analyzers.heimao_analyzer import analyze_heimao_data
        return analyze_heimao_data(data_file, config, output_dir)
    else:
        # å¯¹äºå…¶ä»–ç«™ç‚¹ï¼Œå°è¯•ä½¿ç”¨é€šç”¨åˆ†æå™¨
        try:
            from src.analyzers.generic_analyzer import analyze_data
            return analyze_data(site_id, data_file, config, output_dir)
        except ImportError:
            raise NotImplementedError(f"æ²¡æœ‰ä¸ºç«™ç‚¹ {site_id} æ‰¾åˆ°ä¸“é—¨çš„åˆ†æå™¨ï¼Œä¸”é€šç”¨åˆ†æå™¨ä¸å¯ç”¨")


def run_notifier(site_id, site_name, data_file, analysis_file, summary_file, config):
    """è¿è¡Œé€šçŸ¥æ¨¡å—"""
    # ä½¿ç”¨ç®€å•é€šçŸ¥å™¨
    try:
        from src.notifiers.simple_notifier import send_notification
        return send_notification(site_name, data_file, analysis_file, summary_file, config)
    except ImportError:
        raise ImportError("é€šçŸ¥æ¨¡å—ä¸å¯ç”¨")


def handle_notify(args, logger):
    """é€šçŸ¥å‘½ä»¤å¤„ç†å‡½æ•°"""
    if not args.site:
        logger.error("å‘é€é€šçŸ¥éœ€è¦æŒ‡å®šç«™ç‚¹ID")
        return False
    
    if not args.data or not args.analysis or not args.summary:
        logger.error("å‘é€é€šçŸ¥éœ€è¦æŒ‡å®šæ•°æ®æ–‡ä»¶ã€åˆ†æç»“æœæ–‡ä»¶å’Œæ‘˜è¦æ–‡ä»¶")
        return False
    
    logger.info(f"å¼€å§‹å‘é€é€šçŸ¥ï¼Œç«™ç‚¹: {args.site}")
    
    try:
        # åŠ è½½ç«™ç‚¹é…ç½®å’Œå…¨å±€é…ç½®
        site_config = load_config(args.site, args.config)
        global_config = load_global_config(args.settings)
        
        # è·å–ç«™ç‚¹åç§°
        site_name = site_config.get('site', {}).get('name', args.site)
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # è¿è¡Œé€šçŸ¥å™¨
        result = run_notifier(
            args.site,
            site_name,
            args.data,
            args.analysis,
            args.summary,
            global_config
        )
        
        # è®¡ç®—è€—æ—¶
        elapsed_time = time.time() - start_time
        
        if result and result.get('status') == 'success':
            logger.info(f"âœ… é€šçŸ¥å‘é€æˆåŠŸï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
            return True
        else:
            logger.error(f"âŒ é€šçŸ¥å‘é€å¤±è´¥ï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
            return False
    except Exception as e:
        elapsed_time = time.time() - start_time if 'start_time' in locals() else 0
        logger.exception(f"âŒ é€šçŸ¥å‘é€è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}, è€—æ—¶: {elapsed_time:.2f}ç§’")
        return False


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºä¸»è§£æå™¨
    parser = argparse.ArgumentParser(
        description='Universal Scraper (us) - ç»Ÿä¸€å‘½ä»¤è¡Œå·¥å…·',
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    # æ·»åŠ å…¨å±€é€‰é¡¹
    parser.add_argument('-v', '--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—')
    parser.add_argument('-c', '--config', help='æŒ‡å®šè®¾ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-d', '--sites-dir', help='æŒ‡å®šç«™ç‚¹é…ç½®ç›®å½•')
    parser.add_argument('-o', '--output-dir', help='æŒ‡å®šè¾“å‡ºç›®å½•')
    
    # åˆ›å»ºå­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å‘½ä»¤')
    
    # generateå‘½ä»¤ - ç”Ÿæˆå·¥ä½œæµ
    generate_parser = subparsers.add_parser('generate', help='ç”Ÿæˆå·¥ä½œæµ', aliases=['gen'])
    generate_parser.add_argument('-s', '--site', help='ç«™ç‚¹ID')
    generate_parser.add_argument('-t', '--type', default='all', 
                               choices=['all', 'common', 'crawler', 'analyzer'], 
                               help='å·¥ä½œæµç±»å‹ (é»˜è®¤: all)')
    generate_parser.add_argument('-e', '--enhanced', action='store_true', 
                               help='ä½¿ç”¨å¢å¼ºç‰ˆJsonnetå¼•æ“')
    # ç¼“å­˜æ§åˆ¶
    generate_parser.add_argument('--cache', choices=['enable', 'disable'], 
                               help='å¯ç”¨æˆ–ç¦ç”¨ä¾èµ–é¡¹ç¼“å­˜')
    # è¶…æ—¶è®¾ç½®
    generate_parser.add_argument('--timeout', type=int, 
                               help='è®¾ç½®å·¥ä½œæµè¶…æ—¶æ—¶é—´(åˆ†é’Ÿ)')
    # é”™è¯¯å¤„ç†ç­–ç•¥
    generate_parser.add_argument('--error-strategy', choices=['strict', 'tolerant'], 
                               help='é”™è¯¯å¤„ç†ç­–ç•¥: strict(ä¸¥æ ¼) æˆ– tolerant(å®½æ¾)')
    
    # executeå‘½ä»¤ - è¿è¡Œå®Œæ•´å·¥ä½œæµ
    execute_parser = subparsers.add_parser('execute', help='è¿è¡Œå®Œæ•´å·¥ä½œæµ', aliases=['run'])
    execute_parser.add_argument('-s', '--site', required=True, help='ç«™ç‚¹ID')
    
    # scrapeå‘½ä»¤ - åªè¿è¡Œçˆ¬è™«
    scrape_parser = subparsers.add_parser('scrape', help='åªè¿è¡Œçˆ¬è™«', aliases=['crawl'])
    scrape_parser.add_argument('-s', '--site', required=True, help='ç«™ç‚¹ID')
    
    # analyzeå‘½ä»¤ - åªè¿è¡Œåˆ†æ
    analyze_parser = subparsers.add_parser('analyze', help='åªè¿è¡Œåˆ†æ')
    analyze_parser.add_argument('-s', '--site', required=True, help='ç«™ç‚¹ID')
    analyze_parser.add_argument('-d', '--data', required=True, help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    
    # notifyå‘½ä»¤ - åªå‘é€é€šçŸ¥
    notify_parser = subparsers.add_parser('notify', help='åªå‘é€é€šçŸ¥')
    notify_parser.add_argument('-s', '--site', required=True, help='ç«™ç‚¹ID')
    notify_parser.add_argument('-d', '--data', required=True, help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    notify_parser.add_argument('-a', '--analysis', required=True, help='åˆ†æç»“æœæ–‡ä»¶è·¯å¾„')
    notify_parser.add_argument('-m', '--summary', required=True, help='æ‘˜è¦æ–‡ä»¶è·¯å¾„')
    
    # è§£æå‚æ•°
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger(args.verbose)
    
    # å¦‚æœæ²¡æœ‰æä¾›å‘½ä»¤ï¼Œæ˜¾ç¤ºå¸®åŠ©
    if not args.command:
        parser.print_help()
        return 0
    
    try:
        # æ‰§è¡Œç›¸åº”çš„å‘½ä»¤
        if args.command in ['generate', 'gen']:
            success = handle_generate(args, logger)
        elif args.command in ['execute', 'run']:
            success = handle_execute(args, logger)
        elif args.command in ['scrape', 'crawl']:
            success, _ = handle_scrape(args, logger)
        elif args.command == 'analyze':
            success, _ = handle_analyze(args, logger)
        elif args.command == 'notify':
            success = handle_notify(args, logger)
        else:
            parser.print_help()
            return 0
        
        return 0 if success else 1
    
    except KeyboardInterrupt:
        logger.info("\næ“ä½œå·²å–æ¶ˆ")
        return 1
    except Exception as e:
        logger.exception(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
