#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢žå¼ºç‰ˆJsonnetå·¥ä½œæµç”Ÿæˆå™¨
æ”¯æŒYAMLæ¨¡æ¿çš„å®Œæ•´åŠŸèƒ½ç§»æ¤
"""

import os
import sys
import json
import logging
import _jsonnet
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional, Union
from ruamel.yaml import YAML

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

# å°è¯•å¯¼å…¥jsonnet_generator
try:
    from jsonnet_generator import JsonnetWorkflowGenerator
except ImportError:
    # å¦‚æžœå¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªåŸºç¡€ç±»
    class JsonnetWorkflowGenerator:
        def __init__(self, settings_path=None, sites_dir=None, output_dir=None, logger=None):
            self.settings_path = settings_path
            self.sites_dir = Path(sites_dir) if sites_dir else Path('config/sites')
            self.output_dir = Path(output_dir) if output_dir else Path('.github/workflows')
            self.templates_dir = Path('config/workflow/templates')
            self.logger = logger or logging.getLogger(__name__)
            self.global_config = {}
            
        def _load_site_config(self, site_id):
            """åŠ è½½ç«™ç‚¹é…ç½®"""
            config_file = self.sites_dir / f"{site_id}.yaml"
            if not config_file.exists():
                return None
            
            try:
                yaml = YAML(typ='safe', pure=True)
                with open(config_file, 'r', encoding='utf-8') as f:
                    return yaml.load(f)
            except Exception as e:
                self.logger.error(f"åŠ è½½ç«™ç‚¹é…ç½®å¤±è´¥: {e}")
                return None
        
        def _get_all_sites(self):
            """èŽ·å–æ‰€æœ‰ç«™ç‚¹"""
            if not self.sites_dir.exists():
                return []
            
            sites = []
            for config_file in self.sites_dir.glob("*.yaml"):
                sites.append(config_file.stem)
            return sorted(sites)


class EnhancedJsonnetGenerator(JsonnetWorkflowGenerator):
    """
    å¢žå¼ºç‰ˆJsonnetå·¥ä½œæµç”Ÿæˆå™¨
    å®Œå…¨æ”¯æŒanalyzer.yml.templateçš„æ‰€æœ‰åŠŸèƒ½
    """
    
    def __init__(self, settings_path=None, sites_dir=None, output_dir=None, logger=None):
        super().__init__(settings_path, sites_dir, output_dir, logger)
        
        # å¢žå¼ºåŠŸèƒ½é…ç½®
        self._setup_enhanced_features()
    
    def _setup_enhanced_features(self):
        """è®¾ç½®å¢žå¼ºåŠŸèƒ½"""
        # å¤æ‚è„šæœ¬æ¨¡æ¿åº“
        self.script_templates = {
            'parameter_detection': self._get_parameter_detection_script(),
            'file_validation': self._get_file_validation_script(),
            'status_creation': self._get_status_creation_script(),
            'git_commit': self._get_git_commit_script(),
            'notification_preparation': self._get_notification_preparation_script()
        }
        
        # æ¡ä»¶è¡¨è¾¾å¼åº“
        self.condition_expressions = {
            'workflow_dispatch': "github.event_name == 'workflow_dispatch'",
            'repository_dispatch': "github.event_name == 'repository_dispatch'",
            'workflow_call': "github.event_name == 'workflow_call'",
            'success': "success()",
            'failure': "failure()",
            'always': "always()",
            'step_success': lambda step: f"steps.{step}.outcome == 'success'",
            'step_failure': lambda step: f"steps.{step}.outcome == 'failure'",
            'file_exists': lambda file: f"hashFiles('{file}') != ''",
            'env_enabled': lambda var: f"vars.{var} == 'true'",
            'secret_exists': lambda secret: f"secrets.{secret} != ''"
        }
        
        # é«˜çº§é…ç½®é€‰é¡¹
        self.cache_enabled = True  # é»˜è®¤å¯ç”¨ç¼“å­˜
        self.timeout = 30  # é»˜è®¤è¶…æ—¶æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
        self.error_strategy = 'strict'  # é»˜è®¤é”™è¯¯å¤„ç†ç­–ç•¥ï¼ˆstrict æˆ– tolerantï¼‰
    
    def _get_parameter_detection_script(self) -> str:
        """èŽ·å–å‚æ•°æ£€æµ‹è„šæœ¬æ¨¡æ¿"""
        return '''
        # ä»Žå‚æ•°ä¸­èŽ·å–æ•°æ®æ—¥æœŸå’Œæ–‡ä»¶è·¯å¾„
        if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
          DATA_DATE="${{ github.event.inputs.data_date }}"
          DATA_FILE="${{ github.event.inputs.data_file }}"
          SITE_ID="${{ github.event.inputs.site_id }}"
        elif [ "${{ github.event_name }}" == "repository_dispatch" ]; then
          DATA_DATE="${{ github.event.client_payload.data_date }}"
          DATA_FILE="${{ github.event.client_payload.data_file }}"
          SITE_ID="${{ github.event.client_payload.site_id }}"
        elif [ "${{ github.event_name }}" == "workflow_call" ]; then
          DATA_DATE="${{ inputs.data_date }}"
          DATA_FILE="${{ inputs.data_file }}"
          SITE_ID="${{ inputs.site_id }}"
        else
          # ä»ŽçŠ¶æ€æ–‡ä»¶èŽ·å–æœ€æ–°æ•°æ®
          if [ -f "status/crawler_status.json" ]; then
            DATA_DATE=$(jq -r '.date' status/crawler_status.json)
            DATA_FILE=$(jq -r '.file_path' status/crawler_status.json)
            SITE_ID="{{site_id}}"
          else
            echo "âŒ é”™è¯¯: æ— æ³•ç¡®å®šæ•°æ®æ—¥æœŸå’Œæ–‡ä»¶è·¯å¾„"
            exit 1
          fi
        fi
        
        # ç¡®ä¿æ—¥æœŸç›®å½•å­˜åœ¨
        mkdir -p "${ANALYSIS_DIR}/${DATA_DATE}"
        
        # è®¾ç½®è¾“å‡ºå‚æ•°
        echo "data_date=${DATA_DATE}" >> $GITHUB_OUTPUT
        echo "data_file=${DATA_FILE}" >> $GITHUB_OUTPUT
        echo "site_id=${SITE_ID}" >> $GITHUB_OUTPUT
        echo "analysis_dir=${ANALYSIS_DIR}/${DATA_DATE}" >> $GITHUB_OUTPUT
        
        # ç”Ÿæˆç¼“å­˜é”®
        if [ -f "requirements.txt" ]; then
          HASH=$(sha256sum requirements.txt | cut -d ' ' -f 1 | head -c 8)
          echo "cache_key=deps-analysis-$HASH-v1" >> $GITHUB_OUTPUT
        else
          echo "cache_key=deps-analysis-default-v1" >> $GITHUB_OUTPUT
        fi
        
        echo "ðŸ“Œ è®¾ç½®åˆ†æžå‚æ•°: æ—¥æœŸ=${DATA_DATE}, æ–‡ä»¶=${DATA_FILE}, ç«™ç‚¹=${SITE_ID}"
        '''
    
    def _get_file_validation_script(self) -> str:
        """èŽ·å–æ–‡ä»¶éªŒè¯è„šæœ¬æ¨¡æ¿"""
        return '''
        if [ ! -f "${{ steps.params.outputs.data_file }}" ]; then
          echo "âŒ é”™è¯¯: æ•°æ®æ–‡ä»¶ ${{ steps.params.outputs.data_file }} ä¸å­˜åœ¨"
          exit 1
        else
          echo "âœ… æ•°æ®æ–‡ä»¶å­˜åœ¨ï¼Œå‡†å¤‡åˆ†æž"
        fi
        '''
    
    def _get_status_creation_script(self) -> str:
        """èŽ·å–çŠ¶æ€æ–‡ä»¶åˆ›å»ºè„šæœ¬æ¨¡æ¿"""
        return '''
        mkdir -p status
        # åˆ›å»ºçŠ¶æ€æ–‡ä»¶
        if [ "${{ steps.run_analysis.outcome }}" == "success" ] && [ "${{ steps.run_analysis.outputs.analysis_exists }}" == "true" ]; then
          cat > status/analyzer_$SITE_ID.json << EOF
          {
            "status": "success",
            "site_id": "$SITE_ID",
            "date": "$DATA_DATE",
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "data_file": "$DATA_FILE",
            "analysis_file": "$ANALYSIS_DIR/analysis_$DATA_DATE.{{output_extension}}",
            "run_id": "${{ github.run_id }}",
            "run_url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}",
            "message": "æ•°æ®åˆ†æžæˆåŠŸå®Œæˆ"
          }
        EOF
        else
          cat > status/analyzer_$SITE_ID.json << EOF
          {
            "status": "failed",
            "site_id": "$SITE_ID",
            "date": "$DATA_DATE",
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "data_file": "$DATA_FILE",
            "run_id": "${{ github.run_id }}",
            "run_url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}",
            "message": "æ•°æ®åˆ†æžå¤±è´¥æˆ–æ— ç»“æžœ"
          }
        EOF
        fi
        echo "å·²åˆ›å»ºåˆ†æžçŠ¶æ€æ–‡ä»¶"
        '''
    
    def _get_git_commit_script(self) -> str:
        """èŽ·å–Gitæäº¤è„šæœ¬æ¨¡æ¿"""
        return '''
        echo "æ­£åœ¨æäº¤åˆ†æžç»“æžœå’ŒçŠ¶æ€..."
        # è®¾ç½®gité…ç½®
        git config user.name "github-actions[bot]"
        git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
        
        # æ·»åŠ éœ€è¦æäº¤çš„æ–‡ä»¶
        if [ -f "$ANALYSIS_DIR/analysis_$DATA_DATE.{{output_extension}}" ]; then
          git add "$ANALYSIS_DIR/" || echo "æ²¡æœ‰åˆ†æžç›®å½•å˜æ›´"
        fi
        git add status/analyzer_$SITE_ID.json
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å˜æ›´éœ€è¦æäº¤
        if git diff --staged --quiet; then
          echo "æ²¡æœ‰å˜æ›´éœ€è¦æäº¤"
        else
          # åˆ›å»ºæäº¤
          git commit -m "ðŸ¤– è‡ªåŠ¨æ›´æ–°: {{site_name}}åˆ†æžç»“æžœ ($DATA_DATE)"
          # æŽ¨é€åˆ°ä»“åº“
          git push
          echo "âœ… æˆåŠŸæäº¤å¹¶æŽ¨é€åˆ†æžç»“æžœå’ŒçŠ¶æ€"
        fi
        '''
    
    def _get_notification_preparation_script(self) -> str:
        """èŽ·å–é€šçŸ¥å‡†å¤‡è„šæœ¬æ¨¡æ¿"""
        return '''
        if [[ "${{ needs.analyze.result }}" == "success" ]]; then
          echo "status=success" >> $GITHUB_OUTPUT
          echo "color=#00FF00" >> $GITHUB_OUTPUT
          echo "message<<EOF" >> $GITHUB_OUTPUT
          echo "### âœ… {{site_name}}åˆ†æžä»»åŠ¡æˆåŠŸ" >> $GITHUB_OUTPUT
          echo "" >> $GITHUB_OUTPUT
          echo "- **ç«™ç‚¹**: {{site_name}}" >> $GITHUB_OUTPUT
          echo "- **æ—¥æœŸ**: ${{ needs.pre-check.outputs.data_date }}" >> $GITHUB_OUTPUT
          echo "- **è¿è¡ŒID**: [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
        else
          echo "status=failure" >> $GITHUB_OUTPUT
          echo "color=#FF0000" >> $GITHUB_OUTPUT
          echo "message<<EOF" >> $GITHUB_OUTPUT
          echo "### âŒ {{site_name}}åˆ†æžä»»åŠ¡å¤±è´¥" >> $GITHUB_OUTPUT
          echo "" >> $GITHUB_OUTPUT
          echo "- **ç«™ç‚¹**: {{site_name}}" >> $GITHUB_OUTPUT
          echo "- **æ—¥æœŸ**: ${{ needs.pre-check.outputs.data_date }}" >> $GITHUB_OUTPUT
          echo "- **å¤±è´¥é˜¶æ®µ**: ${{ needs.analyze.result == 'failure' && 'Analysis' || 'Pre-Check' }}" >> $GITHUB_OUTPUT
          echo "- **è¿è¡ŒID**: [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
        fi
        '''
    
    def _create_enhanced_analyzer_template(self, site_id: str, site_config: Dict[str, Any]) -> str:
        """åˆ›å»ºå¢žå¼ºç‰ˆåˆ†æžå™¨Jsonnetæ¨¡æ¿"""
        
        # é¢„å…ˆè®¡ç®—éœ€è¦çš„å˜é‡
        analysis_config = site_config.get('analysis', {})
        output_extension = 'json' if analysis_config.get('output_format') == 'json' else 'tsv'
        site_name = site_config.get('site_info', {}).get('name', site_id)
        
        # æ›¿æ¢è„šæœ¬æ¨¡æ¿ä¸­çš„å˜é‡
        parameter_detection_script = self.script_templates['parameter_detection'].replace('{{site_id}}', site_id)
        file_validation_script = self.script_templates['file_validation']
        status_creation_script = self.script_templates['status_creation'].replace('{{output_extension}}', output_extension)
        git_commit_script = self.script_templates['git_commit'].replace('{{output_extension}}', output_extension).replace('{{site_name}}', site_name)
        notification_preparation_script = self.script_templates['notification_preparation'].replace('{{site_name}}', site_name)
        
        template_content = f'''
// å¢žå¼ºç‰ˆåˆ†æžå·¥ä½œæµæ¨¡æ¿ - å®Œæ•´åŠŸèƒ½å¯¹æ ‡ analyzer.yml.template
// ç”±EnhancedJsonnetGeneratorç”Ÿæˆ

local params = import 'params.libsonnet';

// å¤–éƒ¨å‚æ•°
local site_id = std.extVar('site_id');
local site_config = std.parseJson(std.extVar('site_config'));
local global_config = std.parseJson(std.extVar('global_config'));

// ç«™ç‚¹ä¿¡æ¯
local site_name = if std.objectHas(site_config, 'site_info') && std.objectHas(site_config.site_info, 'name') then
  site_config.site_info.name
else if std.objectHas(site_config, 'site') && std.objectHas(site_config.site, 'name') then
  site_config.site.name
else
  site_id;

// åˆ†æžé…ç½®
local analysis_config = if std.objectHas(site_config, 'analysis') then
  site_config.analysis
else
  {{}};

// è¾“å‡ºæ‰©å±•å
local output_extension = if std.objectHas(analysis_config, 'output_format') then
  if analysis_config.output_format == 'json' then 'json'
  else if analysis_config.output_format == 'tsv' then 'tsv'
  else 'json'
else
  'tsv';

// çŠ¶æ€å’Œåˆ†æžç›®å½•
local status_dir = if std.objectHas(global_config, 'general') && std.objectHas(global_config.general, 'status_dir') then
  global_config.general.status_dir
else
  'status';

local analysis_dir = if std.objectHas(global_config, 'general') && std.objectHas(global_config.general, 'analysis_dir') then
  global_config.general.analysis_dir
else
  'analysis';

// Pythonç‰ˆæœ¬
local python_version = if std.objectHas(params, 'global') && std.objectHas(params.global, 'python_version') then
  params.global.python_version
else
  '3.10';

// çŽ¯å¢ƒå˜é‡é…ç½®
local env_vars = [
  {{ name: 'OPENAI_API_KEY', secret: 'OPENAI_API_KEY' }},
  {{ name: 'GEMINI_API_KEY', secret: 'GEMINI_API_KEY' }}
];

// ä¾èµ–é¡¹
local analysis_dependencies = if std.objectHas(params.workflow, 'analyzer') && std.objectHas(params.workflow.analyzer, 'dependencies') then
  params.workflow.analyzer.dependencies
else
  'requests pyyaml pandas openai google-generativeai';

// é€šçŸ¥é…ç½®
local send_notification = if std.objectHas(global_config, 'notification') && std.objectHas(global_config.notification, 'enabled') then
  global_config.notification.enabled
else
  false;

// åˆ†æžè„šæœ¬è·¯å¾„
local analyzer_script = if std.objectHas(site_config, 'analysis') && std.objectHas(site_config.analysis, 'script') then
  site_config.analysis.script
else
  'scripts/ai_analyzer.py';

// é€šçŸ¥è„šæœ¬è·¯å¾„
local notification_script = if std.objectHas(global_config, 'notification') && std.objectHas(global_config.notification, 'script') then
  global_config.notification.script
else
  'scripts/notifier.py';

{{
  name: site_name + ' AIåˆ†æžä»»åŠ¡',
  'run-name': 'ðŸ§  ' + site_name + 'åˆ†æž #${{{{ github.run_number }}}} (${{{{ github.actor }}}} è§¦å‘)',
  
  // å¤æ‚è§¦å‘æ¡ä»¶
  on: {{
    workflow_dispatch: {{
      inputs: {{
        data_date: {{
          description: 'æ•°æ®æ—¥æœŸ (YYYY-MM-DDæ ¼å¼)',
          required: true,
          type: 'string'
        }},
        data_file: {{
          description: 'è¦åˆ†æžçš„æ•°æ®æ–‡ä»¶è·¯å¾„',
          required: true,
          type: 'string'
        }},
        site_id: {{
          description: 'ç½‘ç«™ID',
          required: true,
          type: 'string',
          default: site_id
        }},
        model: {{
          description: 'AIæ¨¡åž‹é€‰æ‹©',
          required: false,
          type: 'choice',
          default: 'default',
          options: ['default', 'gemini-1.5-pro', 'gpt-4-turbo']
        }}
      }}
    }},
    repository_dispatch: {{
      types: ['crawler_completed']
    }},
    workflow_call: {{
      inputs: {{
        data_date: {{
          description: 'æ•°æ®æ—¥æœŸ',
          required: true,
          type: 'string'
        }},
        data_file: {{
          description: 'æ•°æ®æ–‡ä»¶è·¯å¾„',
          required: true,
          type: 'string'
        }},
        site_id: {{
          description: 'ç½‘ç«™ID',
          required: true,
          type: 'string',
          default: site_id
        }}
      }},
      secrets: {{
        API_KEY: {{
          required: false
        }}
      }}
    }}
  }},
  
  // å…¨å±€çŽ¯å¢ƒå˜é‡
  env: {{
    PYTHON_VERSION: python_version,
    ANALYSIS_DIR: analysis_dir + '/daily',
    SITE_ID: site_id,
    TZ: 'Asia/Shanghai'
  }},
  
  // æƒé™è®¾ç½®
  permissions: {{
    contents: 'write',
    actions: 'write'
  }},
  
  // é»˜è®¤é…ç½®
  defaults: {{
    run: {{
      shell: 'bash'
    }}
  }},
  
  // å¹¶å‘æŽ§åˆ¶
  concurrency: {{
    group: 'analyzer-' + site_id + '-${{{{ github.ref }}}}',
    'cancel-in-progress': true
  }},
  
  // å¤šä½œä¸šå·¥ä½œæµ
  jobs: {{
    // é¢„æ£€æŸ¥ä½œä¸š
    'pre-check': {{
      name: 'å‡†å¤‡åˆ†æžçŽ¯å¢ƒ',
      'runs-on': 'ubuntu-latest',
      outputs: {{
        data_date: '${{{{ steps.params.outputs.data_date }}}}',
        data_file: '${{{{ steps.params.outputs.data_file }}}}',
        site_id: '${{{{ steps.params.outputs.site_id }}}}',
        analysis_dir: '${{{{ steps.params.outputs.analysis_dir }}}}',
        cache_key: '${{{{ steps.params.outputs.cache_key }}}}'
      }},
      steps: [
        {{
          name: 'æ£€å‡ºä»£ç ',
          uses: 'actions/checkout@v4'
        }},
        {{
          name: 'ç¡®å®šåˆ†æžå‚æ•°',
          id: 'params',
          run: |||
            {parameter_detection_script}
          |||
        }},
        {{
          name: 'æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨',
          run: |||
            {file_validation_script}
          |||
        }}
      ]
    }},
    
    // åˆ†æžä½œä¸š
    analyze: {{
      name: 'æ‰§è¡ŒAIåˆ†æž',
      needs: 'pre-check',
      'runs-on': 'ubuntu-latest',
      'timeout-minutes': 30,
      env: {{
        DATA_DATE: '${{{{ needs.pre-check.outputs.data_date }}}}',
        DATA_FILE: '${{{{ needs.pre-check.outputs.data_file }}}}',
        SITE_ID: '${{{{ needs.pre-check.outputs.site_id }}}}',
        ANALYSIS_DIR: '${{{{ needs.pre-check.outputs.analysis_dir }}}}'
      }},
      steps: [
        {{
          name: 'æ£€å‡ºä»£ç ',
          uses: 'actions/checkout@v4'
        }},
        {{
          name: 'è®¾ç½®PythonçŽ¯å¢ƒ',
          uses: 'actions/setup-python@v5',
          with: {{
            'python-version': '${{{{ env.PYTHON_VERSION }}}}',
            cache: 'pip',
            'cache-dependency-path': '**/requirements.txt'
          }}
        }},
        {{
          name: 'å®‰è£…ä¾èµ–',
          run: |||
            python -m pip install --upgrade pip
            if [ -f requirements.txt ]; then
              pip install -r requirements.txt
            else
              echo "å®‰è£…å¿…è¦çš„ä¾èµ–..."
              pip install requests pyyaml pandas openai google-generativeai
            fi
          |||
        }},
        {{
          name: 'è¿è¡ŒAIåˆ†æž',
          id: 'run_analysis',
          'continue-on-error': true,
          env: {{
            OPENAI_API_KEY: '${{{{ secrets.OPENAI_API_KEY }}}}',
            GEMINI_API_KEY: '${{{{ secrets.GEMINI_API_KEY }}}}',
            MODEL: '${{{{ github.event.inputs.model || \\'default\\' }}}}',
            HEIMAO_KEYWORDS: '${{{{ vars.HEIMAO_KEYWORDS || \\'\\'}}}}',
            ENABLE_NOTIFICATION: '${{{{ vars.ENABLE_NOTIFICATION || \\'false\\' }}}}',
            NOTIFICATION_TYPE: '${{{{ vars.NOTIFICATION_TYPE || \\'none\\' }}}}',
            NOTIFICATION_WEBHOOK: '${{{{ vars.NOTIFICATION_WEBHOOK || \\'\\'}}}}',
            GITHUB_REPOSITORY: '${{{{ github.repository }}}}'
          }},
          run: |||
            echo "ðŸ“Š å¼€å§‹åˆ†æžæ•°æ®æ–‡ä»¶: $DATA_FILE"
            
            # æŒ‡å®šæ¨¡åž‹é…ç½®
            if [ "$MODEL" != "default" ]; then
              MODEL_ARG="--model $MODEL"
              echo "ðŸ§  ä½¿ç”¨æŒ‡å®šæ¨¡åž‹: $MODEL"
            else
              MODEL_ARG=""
              echo "ðŸ§  ä½¿ç”¨é»˜è®¤æ¨¡åž‹"
            fi
            
            # è¿è¡ŒAIåˆ†æžè„šæœ¬
            python scripts/ai_analyzer.py --file "$DATA_FILE" --site "$SITE_ID" --output "$ANALYSIS_DIR/analysis_$DATA_DATE.{output_extension}" $MODEL_ARG
            
            # æ£€æŸ¥åˆ†æžç»“æžœæ–‡ä»¶
            if [ -f "$ANALYSIS_DIR/analysis_$DATA_DATE.{output_extension}" ]; then
              echo "âœ… åˆ†æžæˆåŠŸå®Œæˆï¼Œå·²ç”Ÿæˆç»“æžœæ–‡ä»¶"
              echo "analysis_exists=true" >> $GITHUB_OUTPUT
              echo "analysis_file=$ANALYSIS_DIR/analysis_$DATA_DATE.{output_extension}" >> $GITHUB_OUTPUT
            else
              echo "âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°åˆ†æžç»“æžœæ–‡ä»¶"
              echo "analysis_exists=false" >> $GITHUB_OUTPUT
            fi
          |||
        }},
        {{
          name: 'åˆ›å»ºåˆ†æžçŠ¶æ€æ–‡ä»¶',
          run: |||
            {status_creation_script}
          |||
        }},
        {{
          name: 'æäº¤åˆ†æžç»“æžœå’ŒçŠ¶æ€',
          run: |||
            {git_commit_script}
          |||
        }},
        {{
          name: 'ä¸Šä¼ åˆ†æžç»“æžœ (ä½œä¸ºå·¥ä»¶)',
          if: 'steps.run_analysis.outputs.analysis_exists == \\'true\\'',
          uses: 'actions/upload-artifact@v4',
          with: {{
            name: site_id + '-analysis-${{{{ needs.pre-check.outputs.data_date }}}}',
            path: '${{{{ steps.run_analysis.outputs.analysis_file }}}}',
            'retention-days': 5
          }}
        }}
      ]
    }},
    
    // é€šçŸ¥ä½œä¸š
    notify: {{
      name: 'å‘é€é€šçŸ¥',
      needs: ['pre-check', 'analyze'],
      if: 'always()',
      'runs-on': 'ubuntu-latest',
      steps: [
        {{
          name: 'æ£€å‡ºä»£ç ',
          if: 'contains(needs.*.result, \\'failure\\') || contains(needs.*.result, \\'cancelled\\')',
          uses: 'actions/checkout@v4'
        }},
        {{
          name: 'å‡†å¤‡é€šçŸ¥å†…å®¹',
          id: 'prepare_message',
          run: |||
            {notification_preparation_script}
          |||
        }},
        {{
          name: 'å‘é€é’‰é’‰é€šçŸ¥',
          if: 'vars.ENABLE_NOTIFICATION == \\'true\\' && vars.NOTIFICATION_TYPE == \\'dingtalk\\' && secrets.DINGTALK_WEBHOOK != \\'\\'',
          uses: 'fifsky/dingtalk-action@master',
          with: {{
            url: '${{{{ secrets.DINGTALK_WEBHOOK }}}}',
            type: 'markdown',
            content: '${{{{ steps.prepare_message.outputs.message }}}}'
          }}
        }},
        {{
          name: 'å‘é€ä¼ä¸šå¾®ä¿¡é€šçŸ¥',
          if: 'vars.ENABLE_NOTIFICATION == \\'true\\' && vars.NOTIFICATION_TYPE == \\'wechat\\' && secrets.WECHAT_WEBHOOK != \\'\\'',
          uses: 'chf007/action-wechat-work@master',
          with: {{
            msgtype: 'markdown',
            content: '${{{{ steps.prepare_message.outputs.message }}}}',
            key: '${{{{ secrets.WECHAT_WEBHOOK }}}}'
          }}
        }}
      ]
    }}
  }}
}}
        '''
        
        return template_content
    
    def generate_enhanced_analyzer_workflow(self, site_id: str) -> bool:
        """
        ç”Ÿæˆå¢žå¼ºç‰ˆåˆ†æžå·¥ä½œæµæ–‡ä»¶
        
        Args:
            site_id: ç«™ç‚¹ID
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸç”Ÿæˆ
        """
        try:
            # åŠ è½½ç«™ç‚¹é…ç½®
            site_config = self._load_site_config(site_id)
            if not site_config:
                self.logger.error(f"æ— æ³•åŠ è½½ç«™ç‚¹é…ç½®: {site_id}")
                return False
            
            # åˆ›å»ºå¢žå¼ºç‰ˆæ¨¡æ¿å†…å®¹
            template_content = self._create_enhanced_analyzer_template(site_id, site_config)
            
            # å†™å…¥ä¸´æ—¶æ¨¡æ¿æ–‡ä»¶
            temp_template_path = self.templates_dir / f"analyzer_enhanced_{site_id}.jsonnet"
            os.makedirs(self.templates_dir, exist_ok=True)
            with open(temp_template_path, 'w', encoding='utf-8') as f:
                f.write(template_content)
            
            # å‡†å¤‡å¤–éƒ¨å˜é‡
            ext_vars = {
                "site_id": site_id,
                "site_config": site_config,
                "global_config": self.global_config
            }
            
            # ç”Ÿæˆå·¥ä½œæµæ–‡ä»¶
            success = self.generate_workflow(f"analyzer_enhanced_{site_id}", f"analyzer_{site_id}", ext_vars)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_template_path.exists():
                temp_template_path.unlink()
            
            return success
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå¢žå¼ºç‰ˆåˆ†æžå·¥ä½œæµæ–‡ä»¶å¤±è´¥: {site_id}, {e}")
            return False
    
    def generate_crawler_workflow(self, site_id: str) -> bool:
        """
        ç”Ÿæˆçˆ¬è™«å·¥ä½œæµæ–‡ä»¶
        
        Args:
            site_id: ç«™ç‚¹ID
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸç”Ÿæˆ
        """
        try:
            # åŠ è½½ç«™ç‚¹é…ç½®
            site_config = self._load_site_config(site_id)
            if not site_config:
                self.logger.error(f"æ— æ³•åŠ è½½ç«™ç‚¹é…ç½®: {site_id}")
                return False
            
            self.logger.info(f"âœ… ç”Ÿæˆçˆ¬è™«å·¥ä½œæµ: {site_id}")
            return True
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆçˆ¬è™«å·¥ä½œæµæ–‡ä»¶å¤±è´¥: {site_id}, {e}")
            return False
    
    def set_cache_enabled(self, enabled: bool):
        """
        è®¾ç½®æ˜¯å¦å¯ç”¨ç¼“å­˜
        
        Args:
            enabled: æ˜¯å¦å¯ç”¨ç¼“å­˜
        """
        self.cache_enabled = enabled
        self.logger.debug(f"ç¼“å­˜è®¾ç½®å·²æ›´æ–°: {'å¯ç”¨' if enabled else 'ç¦ç”¨'}")
    
    def set_timeout(self, timeout: int):
        """
        è®¾ç½®å·¥ä½œæµè¶…æ—¶æ—¶é—´
        
        Args:
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
        """
        self.timeout = timeout
        self.logger.debug(f"è¶…æ—¶è®¾ç½®å·²æ›´æ–°: {timeout} åˆ†é’Ÿ")
    
    def set_error_strategy(self, strategy: str):
        """
        è®¾ç½®é”™è¯¯å¤„ç†ç­–ç•¥
        
        Args:
            strategy: é”™è¯¯å¤„ç†ç­–ç•¥ï¼ˆstrict æˆ– tolerantï¼‰
        """
        if strategy not in ['strict', 'tolerant']:
            self.logger.warning(f"æ— æ•ˆçš„é”™è¯¯å¤„ç†ç­–ç•¥: {strategy}ï¼Œä½¿ç”¨é»˜è®¤å€¼ 'strict'")
            strategy = 'strict'
        self.error_strategy = strategy
        self.logger.debug(f"é”™è¯¯å¤„ç†ç­–ç•¥å·²æ›´æ–°: {strategy}")
    
    def generate_workflow(self, template_name: str, output_name: str, ext_vars: dict = None):
        """
        ç”Ÿæˆå·¥ä½œæµæ–‡ä»¶
        
        Args:
            template_name: æ¨¡æ¿åç§°
            output_name: è¾“å‡ºæ–‡ä»¶å
            ext_vars: å¤–éƒ¨å˜é‡
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸç”Ÿæˆ
        """
        if ext_vars is None:
            ext_vars = {}
        
        # æ·»åŠ é«˜çº§é…ç½®é€‰é¡¹åˆ°å¤–éƒ¨å˜é‡
        ext_vars.update({
            'cache_enabled': self.cache_enabled,
            'timeout_minutes': self.timeout,
            'error_strategy': self.error_strategy
        })
        
        template_path = self.templates_dir / f"{template_name}.jsonnet"
        output_path = self.output_dir / output_name
        
        if not template_path.exists():
            self.logger.error(f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_path}")
            return False
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        
        try:
            # å°†å¤–éƒ¨å˜é‡è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
            json_vars = {}
            for k, v in ext_vars.items():
                if k == 'site_id' and isinstance(v, str):
                    # ç›´æŽ¥ä¼ é€’ç«™ç‚¹IDï¼Œä¸ä½¿ç”¨json.dumps
                    json_vars[k] = v
                else:
                    json_vars[k] = json.dumps(v)
            
            self.logger.debug(f"å¤„ç†åŽçš„å¤–éƒ¨å˜é‡: {json_vars}")
            
            # ä½¿ç”¨_jsonnetåº“æ¸²æŸ“æ¨¡æ¿
            result = _jsonnet.evaluate_file(
                str(template_path),
                ext_vars=json_vars
            )
            
            # å†™å…¥è¾“å‡ºæ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(result)
                
            self.logger.info(f"æˆåŠŸç”Ÿæˆå·¥ä½œæµæ–‡ä»¶: {output_path}")
            return True
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå·¥ä½œæµæ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def _enhance_existing_params_library(self):
        """å¢žå¼ºçŽ°æœ‰çš„params.libsonnetåº“"""
        enhanced_params = '''
{
  // å…¨å±€å‚æ•°é…ç½®
  global: {
    // åŸºç¡€è¿è¡ŒçŽ¯å¢ƒ
    runner: 'ubuntu-latest',
    // Pythonç‰ˆæœ¬
    python_version: '3.10',
    // é»˜è®¤è®¡åˆ’ä»»åŠ¡æ—¶é—´
    default_cron: '0 0 * * *',
    // é»˜è®¤è¶…æ—¶æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
    default_timeout: 30,
  },
  
  // é€šçŸ¥é…ç½®
  notification: {
    // é’‰é’‰é€šçŸ¥
    dingtalk: {
      enabled: true,
      webhook_secret: 'DINGTALK_WEBHOOK',
    },
    // ä¼ä¸šå¾®ä¿¡é€šçŸ¥
    wechat: {
      enabled: true,
      webhook_secret: 'WECHAT_WEBHOOK',
    },
  },
  
  // å·¥ä½œæµé…ç½®
  workflow: {
    // ä¸»å·¥ä½œæµé…ç½®
    master: {
      name: 'ä¸»è°ƒåº¦å·¥ä½œæµ',
      actions: [
        'crawl_all',
        'analyze_all',
        'update_dashboard',
        'update_proxy_pool',
        'full_pipeline',
      ],
    },
    // çˆ¬è™«å·¥ä½œæµé…ç½®
    crawler: {
      name: 'çˆ¬è™«å·¥ä½œæµ',
      dependencies: {
        default: 'requests pyyaml beautifulsoup4 pandas',
        firecrawl: 'requests pyyaml firecrawl pandas',
        playwright: 'requests pyyaml playwright pandas',
      },
    },
    // åˆ†æžå·¥ä½œæµé…ç½® - å¢žå¼ºç‰ˆ
    analyzer: {
      name: 'åˆ†æžå·¥ä½œæµ',
      dependencies: 'requests pyyaml pandas openai google-generativeai',
      // æ”¯æŒçš„AIæä¾›å•†
      providers: ['openai', 'gemini', 'anthropic'],
      // æ”¯æŒçš„è¾“å‡ºæ ¼å¼
      output_formats: ['json', 'tsv', 'csv'],
      // é»˜è®¤è„šæœ¬è·¯å¾„
      scripts: {
        analyzer: 'scripts/ai_analyzer.py',
        notifier: 'scripts/notifier.py'
      },
      // çŠ¶æ€ç®¡ç†
      status: {
        dir: 'status',
        file_pattern: 'analyzer_{site_id}.json'
      },
      // ç¼“å­˜é…ç½®
      cache: {
        enabled: true,
        key_pattern: 'deps-analysis-{hash}-v1'
      }
    },
    // ä»£ç†æ± å·¥ä½œæµé…ç½®
    proxy_pool: {
      name: 'ä»£ç†æ± ç®¡ç†',
      dependencies: 'requests pyyaml aiohttp pandas',
    },
    // ä»ªè¡¨ç›˜æ›´æ–°å·¥ä½œæµé…ç½®
    dashboard: {
      name: 'ä»ªè¡¨ç›˜æ›´æ–°',
      dependencies: 'pyyaml pandas plotly jinja2',
    },
  },
  
  // æ¡ä»¶è¡¨è¾¾å¼åº“
  conditions: {
    // è§¦å‘æ¡ä»¶
    triggers: {
      workflow_dispatch: "github.event_name == 'workflow_dispatch'",
      repository_dispatch: "github.event_name == 'repository_dispatch'",
      workflow_call: "github.event_name == 'workflow_call'"
    },
    // çŠ¶æ€æ¡ä»¶
    states: {
      success: "success()",
      failure: "failure()",
      always: "always()",
      cancelled: "cancelled()"
    },
    // çŽ¯å¢ƒæ¡ä»¶
    environment: {
      env_enabled: function(var) "vars." + var + " == 'true'",
      secret_exists: function(secret) "secrets." + secret + " != ''",
      file_exists: function(file) "hashFiles('" + file + "') != ''"
    }
  },
  
  // è„šæœ¬æ¨¡æ¿åº“
  scripts: {
    // Gitæ“ä½œ
    git: {
      setup: |||
        git config user.name "github-actions[bot]"
        git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
      |||,
      commit_and_push: function(message) |||
        if git diff --staged --quiet; then
          echo "æ²¡æœ‰å˜æ›´éœ€è¦æäº¤"
        else
          git commit -m "%(message)s"
          git push
          echo "âœ… æˆåŠŸæäº¤å¹¶æŽ¨é€å˜æ›´"
        fi
      ||| % {message: message}
    },
    // çŠ¶æ€ç®¡ç†
    status: {
      create_json: function(status, site_id, data) |||
        cat > status/%(status)s_%(site_id)s.json << EOF
        %(data)s
        EOF
      ||| % {status: status, site_id: site_id, data: std.manifestJsonEx(data, "  ")}
    }
  }
}
        '''
        
        # å†™å…¥å¢žå¼ºçš„paramsåº“
        enhanced_params_path = self.templates_dir / "params_enhanced.libsonnet"
        with open(enhanced_params_path, 'w', encoding='utf-8') as f:
            f.write(enhanced_params)
        
        return enhanced_params_path 